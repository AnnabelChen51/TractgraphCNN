Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, epochs=200, tensorboard=True, net_architecture='TractGraphormer', batch_size=32, rate=1e-05, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20, fl=64, nh=1)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	1e-05
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	2

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/204]	Loss 0.7371 (0.7371)	
training:	Epoch: [1][2/204]	Loss 0.7143 (0.7257)	
training:	Epoch: [1][3/204]	Loss 0.7776 (0.7430)	
training:	Epoch: [1][4/204]	Loss 0.6874 (0.7291)	
training:	Epoch: [1][5/204]	Loss 0.6887 (0.7210)	
training:	Epoch: [1][6/204]	Loss 0.6817 (0.7145)	
training:	Epoch: [1][7/204]	Loss 0.6972 (0.7120)	
training:	Epoch: [1][8/204]	Loss 0.6711 (0.7069)	
training:	Epoch: [1][9/204]	Loss 0.6766 (0.7035)	
training:	Epoch: [1][10/204]	Loss 0.7067 (0.7038)	
training:	Epoch: [1][11/204]	Loss 0.6802 (0.7017)	
training:	Epoch: [1][12/204]	Loss 0.6807 (0.6999)	
training:	Epoch: [1][13/204]	Loss 0.6870 (0.6989)	
training:	Epoch: [1][14/204]	Loss 0.6947 (0.6986)	
training:	Epoch: [1][15/204]	Loss 0.7505 (0.7021)	
training:	Epoch: [1][16/204]	Loss 0.7182 (0.7031)	
training:	Epoch: [1][17/204]	Loss 0.7201 (0.7041)	
training:	Epoch: [1][18/204]	Loss 0.6856 (0.7031)	
training:	Epoch: [1][19/204]	Loss 0.7440 (0.7052)	
training:	Epoch: [1][20/204]	Loss 0.6919 (0.7046)	
training:	Epoch: [1][21/204]	Loss 0.7094 (0.7048)	
training:	Epoch: [1][22/204]	Loss 0.6852 (0.7039)	
training:	Epoch: [1][23/204]	Loss 0.7031 (0.7039)	
training:	Epoch: [1][24/204]	Loss 0.6941 (0.7035)	
training:	Epoch: [1][25/204]	Loss 0.6856 (0.7027)	
training:	Epoch: [1][26/204]	Loss 0.7175 (0.7033)	
training:	Epoch: [1][27/204]	Loss 0.6988 (0.7031)	
training:	Epoch: [1][28/204]	Loss 0.7216 (0.7038)	
training:	Epoch: [1][29/204]	Loss 0.6898 (0.7033)	
training:	Epoch: [1][30/204]	Loss 0.7098 (0.7035)	
training:	Epoch: [1][31/204]	Loss 0.6952 (0.7033)	
training:	Epoch: [1][32/204]	Loss 0.7217 (0.7038)	
training:	Epoch: [1][33/204]	Loss 0.7340 (0.7048)	
training:	Epoch: [1][34/204]	Loss 0.7507 (0.7061)	
training:	Epoch: [1][35/204]	Loss 0.7066 (0.7061)	
training:	Epoch: [1][36/204]	Loss 0.6886 (0.7056)	
training:	Epoch: [1][37/204]	Loss 0.7140 (0.7059)	
training:	Epoch: [1][38/204]	Loss 0.7126 (0.7060)	
training:	Epoch: [1][39/204]	Loss 0.6802 (0.7054)	
training:	Epoch: [1][40/204]	Loss 0.6879 (0.7049)	
training:	Epoch: [1][41/204]	Loss 0.7157 (0.7052)	
training:	Epoch: [1][42/204]	Loss 0.6876 (0.7048)	
training:	Epoch: [1][43/204]	Loss 0.7202 (0.7051)	
training:	Epoch: [1][44/204]	Loss 0.6754 (0.7045)	
training:	Epoch: [1][45/204]	Loss 0.6985 (0.7043)	
training:	Epoch: [1][46/204]	Loss 0.7236 (0.7048)	
training:	Epoch: [1][47/204]	Loss 0.6638 (0.7039)	
training:	Epoch: [1][48/204]	Loss 0.7046 (0.7039)	
training:	Epoch: [1][49/204]	Loss 0.7057 (0.7039)	
training:	Epoch: [1][50/204]	Loss 0.6710 (0.7033)	
training:	Epoch: [1][51/204]	Loss 0.7423 (0.7040)	
training:	Epoch: [1][52/204]	Loss 0.6976 (0.7039)	
training:	Epoch: [1][53/204]	Loss 0.6754 (0.7034)	
training:	Epoch: [1][54/204]	Loss 0.7010 (0.7033)	
training:	Epoch: [1][55/204]	Loss 0.7224 (0.7037)	
training:	Epoch: [1][56/204]	Loss 0.6575 (0.7029)	
training:	Epoch: [1][57/204]	Loss 0.7298 (0.7033)	
training:	Epoch: [1][58/204]	Loss 0.6882 (0.7031)	
training:	Epoch: [1][59/204]	Loss 0.7031 (0.7031)	
training:	Epoch: [1][60/204]	Loss 0.7041 (0.7031)	
training:	Epoch: [1][61/204]	Loss 0.6700 (0.7025)	
training:	Epoch: [1][62/204]	Loss 0.7155 (0.7028)	
training:	Epoch: [1][63/204]	Loss 0.6880 (0.7025)	
training:	Epoch: [1][64/204]	Loss 0.6806 (0.7022)	
training:	Epoch: [1][65/204]	Loss 0.6543 (0.7014)	
training:	Epoch: [1][66/204]	Loss 0.6773 (0.7011)	
training:	Epoch: [1][67/204]	Loss 0.6534 (0.7004)	
training:	Epoch: [1][68/204]	Loss 0.7329 (0.7008)	
training:	Epoch: [1][69/204]	Loss 0.6794 (0.7005)	
training:	Epoch: [1][70/204]	Loss 0.6749 (0.7002)	
training:	Epoch: [1][71/204]	Loss 0.6797 (0.6999)	
training:	Epoch: [1][72/204]	Loss 0.6989 (0.6999)	
training:	Epoch: [1][73/204]	Loss 0.6784 (0.6996)	
training:	Epoch: [1][74/204]	Loss 0.6763 (0.6993)	
training:	Epoch: [1][75/204]	Loss 0.7059 (0.6993)	
training:	Epoch: [1][76/204]	Loss 0.7176 (0.6996)	
training:	Epoch: [1][77/204]	Loss 0.6829 (0.6994)	
training:	Epoch: [1][78/204]	Loss 0.6976 (0.6993)	
training:	Epoch: [1][79/204]	Loss 0.6878 (0.6992)	
training:	Epoch: [1][80/204]	Loss 0.7154 (0.6994)	
training:	Epoch: [1][81/204]	Loss 0.6576 (0.6989)	
training:	Epoch: [1][82/204]	Loss 0.7002 (0.6989)	
training:	Epoch: [1][83/204]	Loss 0.6252 (0.6980)	
training:	Epoch: [1][84/204]	Loss 0.7211 (0.6983)	
training:	Epoch: [1][85/204]	Loss 0.6791 (0.6981)	
training:	Epoch: [1][86/204]	Loss 0.7151 (0.6983)	
training:	Epoch: [1][87/204]	Loss 0.7109 (0.6984)	
training:	Epoch: [1][88/204]	Loss 0.6967 (0.6984)	
training:	Epoch: [1][89/204]	Loss 0.6905 (0.6983)	
training:	Epoch: [1][90/204]	Loss 0.7428 (0.6988)	
training:	Epoch: [1][91/204]	Loss 0.6447 (0.6982)	
training:	Epoch: [1][92/204]	Loss 0.7106 (0.6983)	
training:	Epoch: [1][93/204]	Loss 0.6398 (0.6977)	
training:	Epoch: [1][94/204]	Loss 0.6809 (0.6975)	
training:	Epoch: [1][95/204]	Loss 0.6786 (0.6973)	
training:	Epoch: [1][96/204]	Loss 0.6765 (0.6971)	
training:	Epoch: [1][97/204]	Loss 0.7002 (0.6971)	
training:	Epoch: [1][98/204]	Loss 0.6963 (0.6971)	
training:	Epoch: [1][99/204]	Loss 0.6826 (0.6970)	
training:	Epoch: [1][100/204]	Loss 0.6695 (0.6967)	
training:	Epoch: [1][101/204]	Loss 0.6901 (0.6966)	
training:	Epoch: [1][102/204]	Loss 0.6746 (0.6964)	
training:	Epoch: [1][103/204]	Loss 0.7144 (0.6966)	
training:	Epoch: [1][104/204]	Loss 0.6722 (0.6964)	
training:	Epoch: [1][105/204]	Loss 0.6624 (0.6960)	
training:	Epoch: [1][106/204]	Loss 0.6915 (0.6960)	
training:	Epoch: [1][107/204]	Loss 0.7184 (0.6962)	
training:	Epoch: [1][108/204]	Loss 0.6864 (0.6961)	
training:	Epoch: [1][109/204]	Loss 0.6753 (0.6959)	
training:	Epoch: [1][110/204]	Loss 0.6677 (0.6957)	
training:	Epoch: [1][111/204]	Loss 0.7728 (0.6964)	
training:	Epoch: [1][112/204]	Loss 0.6481 (0.6959)	
training:	Epoch: [1][113/204]	Loss 0.6889 (0.6959)	
training:	Epoch: [1][114/204]	Loss 0.6425 (0.6954)	
training:	Epoch: [1][115/204]	Loss 0.7182 (0.6956)	
training:	Epoch: [1][116/204]	Loss 0.6835 (0.6955)	
training:	Epoch: [1][117/204]	Loss 0.6513 (0.6951)	
training:	Epoch: [1][118/204]	Loss 0.7014 (0.6952)	
training:	Epoch: [1][119/204]	Loss 0.6384 (0.6947)	
training:	Epoch: [1][120/204]	Loss 0.6555 (0.6944)	
training:	Epoch: [1][121/204]	Loss 0.6805 (0.6943)	
training:	Epoch: [1][122/204]	Loss 0.6501 (0.6939)	
training:	Epoch: [1][123/204]	Loss 0.7018 (0.6940)	
training:	Epoch: [1][124/204]	Loss 0.7011 (0.6940)	
training:	Epoch: [1][125/204]	Loss 0.7124 (0.6942)	
training:	Epoch: [1][126/204]	Loss 0.6750 (0.6940)	
training:	Epoch: [1][127/204]	Loss 0.7218 (0.6942)	
training:	Epoch: [1][128/204]	Loss 0.7189 (0.6944)	
training:	Epoch: [1][129/204]	Loss 0.6616 (0.6942)	
training:	Epoch: [1][130/204]	Loss 0.6873 (0.6941)	
training:	Epoch: [1][131/204]	Loss 0.7228 (0.6943)	
training:	Epoch: [1][132/204]	Loss 0.6889 (0.6943)	
training:	Epoch: [1][133/204]	Loss 0.6645 (0.6941)	
training:	Epoch: [1][134/204]	Loss 0.7086 (0.6942)	
training:	Epoch: [1][135/204]	Loss 0.6615 (0.6939)	
training:	Epoch: [1][136/204]	Loss 0.6822 (0.6938)	
training:	Epoch: [1][137/204]	Loss 0.6607 (0.6936)	
training:	Epoch: [1][138/204]	Loss 0.6648 (0.6934)	
training:	Epoch: [1][139/204]	Loss 0.7253 (0.6936)	
training:	Epoch: [1][140/204]	Loss 0.7000 (0.6937)	
training:	Epoch: [1][141/204]	Loss 0.6307 (0.6932)	
training:	Epoch: [1][142/204]	Loss 0.6842 (0.6932)	
training:	Epoch: [1][143/204]	Loss 0.6509 (0.6929)	
training:	Epoch: [1][144/204]	Loss 0.6675 (0.6927)	
training:	Epoch: [1][145/204]	Loss 0.6900 (0.6927)	
training:	Epoch: [1][146/204]	Loss 0.6761 (0.6926)	
training:	Epoch: [1][147/204]	Loss 0.7106 (0.6927)	
training:	Epoch: [1][148/204]	Loss 0.6904 (0.6927)	
training:	Epoch: [1][149/204]	Loss 0.6285 (0.6922)	
training:	Epoch: [1][150/204]	Loss 0.6744 (0.6921)	
training:	Epoch: [1][151/204]	Loss 0.6930 (0.6921)	
training:	Epoch: [1][152/204]	Loss 0.6862 (0.6921)	
training:	Epoch: [1][153/204]	Loss 0.7068 (0.6922)	
training:	Epoch: [1][154/204]	Loss 0.7072 (0.6923)	
training:	Epoch: [1][155/204]	Loss 0.6756 (0.6922)	
training:	Epoch: [1][156/204]	Loss 0.6876 (0.6921)	
training:	Epoch: [1][157/204]	Loss 0.6778 (0.6920)	
training:	Epoch: [1][158/204]	Loss 0.6985 (0.6921)	
training:	Epoch: [1][159/204]	Loss 0.7151 (0.6922)	
training:	Epoch: [1][160/204]	Loss 0.6862 (0.6922)	
training:	Epoch: [1][161/204]	Loss 0.6526 (0.6920)	
training:	Epoch: [1][162/204]	Loss 0.7065 (0.6920)	
training:	Epoch: [1][163/204]	Loss 0.6537 (0.6918)	
training:	Epoch: [1][164/204]	Loss 0.6487 (0.6915)	
training:	Epoch: [1][165/204]	Loss 0.6598 (0.6913)	
training:	Epoch: [1][166/204]	Loss 0.7024 (0.6914)	
training:	Epoch: [1][167/204]	Loss 0.6827 (0.6914)	
training:	Epoch: [1][168/204]	Loss 0.6426 (0.6911)	
training:	Epoch: [1][169/204]	Loss 0.6833 (0.6910)	
training:	Epoch: [1][170/204]	Loss 0.6766 (0.6909)	
training:	Epoch: [1][171/204]	Loss 0.6938 (0.6910)	
training:	Epoch: [1][172/204]	Loss 0.6919 (0.6910)	
training:	Epoch: [1][173/204]	Loss 0.6380 (0.6907)	
training:	Epoch: [1][174/204]	Loss 0.6248 (0.6903)	
training:	Epoch: [1][175/204]	Loss 0.6948 (0.6903)	
training:	Epoch: [1][176/204]	Loss 0.6757 (0.6902)	
training:	Epoch: [1][177/204]	Loss 0.7050 (0.6903)	
training:	Epoch: [1][178/204]	Loss 0.6929 (0.6903)	
training:	Epoch: [1][179/204]	Loss 0.6447 (0.6901)	
training:	Epoch: [1][180/204]	Loss 0.6901 (0.6901)	
training:	Epoch: [1][181/204]	Loss 0.6635 (0.6899)	
training:	Epoch: [1][182/204]	Loss 0.6735 (0.6898)	
training:	Epoch: [1][183/204]	Loss 0.6558 (0.6896)	
training:	Epoch: [1][184/204]	Loss 0.6728 (0.6896)	
training:	Epoch: [1][185/204]	Loss 0.7371 (0.6898)	
training:	Epoch: [1][186/204]	Loss 0.6528 (0.6896)	
training:	Epoch: [1][187/204]	Loss 0.6744 (0.6895)	
training:	Epoch: [1][188/204]	Loss 0.6595 (0.6894)	
training:	Epoch: [1][189/204]	Loss 0.6772 (0.6893)	
training:	Epoch: [1][190/204]	Loss 0.7547 (0.6896)	
training:	Epoch: [1][191/204]	Loss 0.6866 (0.6896)	
training:	Epoch: [1][192/204]	Loss 0.6651 (0.6895)	
training:	Epoch: [1][193/204]	Loss 0.6774 (0.6894)	
training:	Epoch: [1][194/204]	Loss 0.6813 (0.6894)	
training:	Epoch: [1][195/204]	Loss 0.6844 (0.6894)	
training:	Epoch: [1][196/204]	Loss 0.7220 (0.6895)	
training:	Epoch: [1][197/204]	Loss 0.6866 (0.6895)	
training:	Epoch: [1][198/204]	Loss 0.6803 (0.6895)	
training:	Epoch: [1][199/204]	Loss 0.6536 (0.6893)	
training:	Epoch: [1][200/204]	Loss 0.6600 (0.6892)	
training:	Epoch: [1][201/204]	Loss 0.6894 (0.6892)	
training:	Epoch: [1][202/204]	Loss 0.6718 (0.6891)	
training:	Epoch: [1][203/204]	Loss 0.6466 (0.6889)	
training:	Epoch: [1][204/204]	Loss 0.6624 (0.6887)	
Training:	 Loss: 0.6877

Training:	 ACC: 0.5801 0.5843 0.6822 0.4780
Validation:	 ACC: 0.5769 0.5821 0.6919 0.4619
Validation:	 Best_BACC: 0.5769 0.5821 0.6919 0.4619
Validation:	 Loss: 0.6743
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/204]	Loss 0.6247 (0.6247)	
training:	Epoch: [2][2/204]	Loss 0.6476 (0.6361)	
training:	Epoch: [2][3/204]	Loss 0.6348 (0.6357)	
training:	Epoch: [2][4/204]	Loss 0.7007 (0.6519)	
training:	Epoch: [2][5/204]	Loss 0.6479 (0.6511)	
training:	Epoch: [2][6/204]	Loss 0.6641 (0.6533)	
training:	Epoch: [2][7/204]	Loss 0.6884 (0.6583)	
training:	Epoch: [2][8/204]	Loss 0.6743 (0.6603)	
training:	Epoch: [2][9/204]	Loss 0.6411 (0.6582)	
training:	Epoch: [2][10/204]	Loss 0.6810 (0.6605)	
training:	Epoch: [2][11/204]	Loss 0.6631 (0.6607)	
training:	Epoch: [2][12/204]	Loss 0.7211 (0.6657)	
training:	Epoch: [2][13/204]	Loss 0.6803 (0.6669)	
training:	Epoch: [2][14/204]	Loss 0.6280 (0.6641)	
training:	Epoch: [2][15/204]	Loss 0.7343 (0.6688)	
training:	Epoch: [2][16/204]	Loss 0.6529 (0.6678)	
training:	Epoch: [2][17/204]	Loss 0.6388 (0.6661)	
training:	Epoch: [2][18/204]	Loss 0.6797 (0.6668)	
training:	Epoch: [2][19/204]	Loss 0.6865 (0.6679)	
training:	Epoch: [2][20/204]	Loss 0.6558 (0.6673)	
training:	Epoch: [2][21/204]	Loss 0.6850 (0.6681)	
training:	Epoch: [2][22/204]	Loss 0.6909 (0.6691)	
training:	Epoch: [2][23/204]	Loss 0.6980 (0.6704)	
training:	Epoch: [2][24/204]	Loss 0.7299 (0.6729)	
training:	Epoch: [2][25/204]	Loss 0.6396 (0.6715)	
training:	Epoch: [2][26/204]	Loss 0.6740 (0.6716)	
training:	Epoch: [2][27/204]	Loss 0.6147 (0.6695)	
training:	Epoch: [2][28/204]	Loss 0.6440 (0.6686)	
training:	Epoch: [2][29/204]	Loss 0.6717 (0.6687)	
training:	Epoch: [2][30/204]	Loss 0.6140 (0.6669)	
training:	Epoch: [2][31/204]	Loss 0.6686 (0.6670)	
training:	Epoch: [2][32/204]	Loss 0.6619 (0.6668)	
training:	Epoch: [2][33/204]	Loss 0.6730 (0.6670)	
training:	Epoch: [2][34/204]	Loss 0.7022 (0.6680)	
training:	Epoch: [2][35/204]	Loss 0.6891 (0.6686)	
training:	Epoch: [2][36/204]	Loss 0.6496 (0.6681)	
training:	Epoch: [2][37/204]	Loss 0.6626 (0.6679)	
training:	Epoch: [2][38/204]	Loss 0.6785 (0.6682)	
training:	Epoch: [2][39/204]	Loss 0.7044 (0.6692)	
training:	Epoch: [2][40/204]	Loss 0.6915 (0.6697)	
training:	Epoch: [2][41/204]	Loss 0.6572 (0.6694)	
training:	Epoch: [2][42/204]	Loss 0.6652 (0.6693)	
training:	Epoch: [2][43/204]	Loss 0.6878 (0.6697)	
training:	Epoch: [2][44/204]	Loss 0.6741 (0.6698)	
training:	Epoch: [2][45/204]	Loss 0.6695 (0.6698)	
training:	Epoch: [2][46/204]	Loss 0.6978 (0.6704)	
training:	Epoch: [2][47/204]	Loss 0.6655 (0.6703)	
training:	Epoch: [2][48/204]	Loss 0.6666 (0.6703)	
training:	Epoch: [2][49/204]	Loss 0.7074 (0.6710)	
training:	Epoch: [2][50/204]	Loss 0.7403 (0.6724)	
training:	Epoch: [2][51/204]	Loss 0.7071 (0.6731)	
training:	Epoch: [2][52/204]	Loss 0.6624 (0.6729)	
training:	Epoch: [2][53/204]	Loss 0.6933 (0.6733)	
training:	Epoch: [2][54/204]	Loss 0.7045 (0.6738)	
training:	Epoch: [2][55/204]	Loss 0.6464 (0.6733)	
training:	Epoch: [2][56/204]	Loss 0.6701 (0.6733)	
training:	Epoch: [2][57/204]	Loss 0.6469 (0.6728)	
training:	Epoch: [2][58/204]	Loss 0.7064 (0.6734)	
training:	Epoch: [2][59/204]	Loss 0.6708 (0.6733)	
training:	Epoch: [2][60/204]	Loss 0.6425 (0.6728)	
training:	Epoch: [2][61/204]	Loss 0.6962 (0.6732)	
training:	Epoch: [2][62/204]	Loss 0.6826 (0.6734)	
training:	Epoch: [2][63/204]	Loss 0.7264 (0.6742)	
training:	Epoch: [2][64/204]	Loss 0.6564 (0.6739)	
training:	Epoch: [2][65/204]	Loss 0.6423 (0.6734)	
training:	Epoch: [2][66/204]	Loss 0.6650 (0.6733)	
training:	Epoch: [2][67/204]	Loss 0.6825 (0.6735)	
training:	Epoch: [2][68/204]	Loss 0.7108 (0.6740)	
training:	Epoch: [2][69/204]	Loss 0.6009 (0.6729)	
training:	Epoch: [2][70/204]	Loss 0.6836 (0.6731)	
training:	Epoch: [2][71/204]	Loss 0.6570 (0.6729)	
training:	Epoch: [2][72/204]	Loss 0.7497 (0.6739)	
training:	Epoch: [2][73/204]	Loss 0.6648 (0.6738)	
training:	Epoch: [2][74/204]	Loss 0.6988 (0.6741)	
training:	Epoch: [2][75/204]	Loss 0.6542 (0.6739)	
training:	Epoch: [2][76/204]	Loss 0.6596 (0.6737)	
training:	Epoch: [2][77/204]	Loss 0.6999 (0.6740)	
training:	Epoch: [2][78/204]	Loss 0.6965 (0.6743)	
training:	Epoch: [2][79/204]	Loss 0.6582 (0.6741)	
training:	Epoch: [2][80/204]	Loss 0.6139 (0.6734)	
training:	Epoch: [2][81/204]	Loss 0.6909 (0.6736)	
training:	Epoch: [2][82/204]	Loss 0.7188 (0.6741)	
training:	Epoch: [2][83/204]	Loss 0.6927 (0.6744)	
training:	Epoch: [2][84/204]	Loss 0.6793 (0.6744)	
training:	Epoch: [2][85/204]	Loss 0.6394 (0.6740)	
training:	Epoch: [2][86/204]	Loss 0.7082 (0.6744)	
training:	Epoch: [2][87/204]	Loss 0.6474 (0.6741)	
training:	Epoch: [2][88/204]	Loss 0.6545 (0.6739)	
training:	Epoch: [2][89/204]	Loss 0.6698 (0.6738)	
training:	Epoch: [2][90/204]	Loss 0.7122 (0.6742)	
training:	Epoch: [2][91/204]	Loss 0.6533 (0.6740)	
training:	Epoch: [2][92/204]	Loss 0.6685 (0.6740)	
training:	Epoch: [2][93/204]	Loss 0.6115 (0.6733)	
training:	Epoch: [2][94/204]	Loss 0.6951 (0.6735)	
training:	Epoch: [2][95/204]	Loss 0.6658 (0.6734)	
training:	Epoch: [2][96/204]	Loss 0.6529 (0.6732)	
training:	Epoch: [2][97/204]	Loss 0.6594 (0.6731)	
training:	Epoch: [2][98/204]	Loss 0.6682 (0.6730)	
training:	Epoch: [2][99/204]	Loss 0.7316 (0.6736)	
training:	Epoch: [2][100/204]	Loss 0.7025 (0.6739)	
training:	Epoch: [2][101/204]	Loss 0.6511 (0.6737)	
training:	Epoch: [2][102/204]	Loss 0.6780 (0.6737)	
training:	Epoch: [2][103/204]	Loss 0.6886 (0.6739)	
training:	Epoch: [2][104/204]	Loss 0.6349 (0.6735)	
training:	Epoch: [2][105/204]	Loss 0.6790 (0.6735)	
training:	Epoch: [2][106/204]	Loss 0.6409 (0.6732)	
training:	Epoch: [2][107/204]	Loss 0.6820 (0.6733)	
training:	Epoch: [2][108/204]	Loss 0.7006 (0.6736)	
training:	Epoch: [2][109/204]	Loss 0.6365 (0.6732)	
training:	Epoch: [2][110/204]	Loss 0.6296 (0.6728)	
training:	Epoch: [2][111/204]	Loss 0.6453 (0.6726)	
training:	Epoch: [2][112/204]	Loss 0.6327 (0.6722)	
training:	Epoch: [2][113/204]	Loss 0.6489 (0.6720)	
training:	Epoch: [2][114/204]	Loss 0.6067 (0.6715)	
training:	Epoch: [2][115/204]	Loss 0.7168 (0.6718)	
training:	Epoch: [2][116/204]	Loss 0.7321 (0.6724)	
training:	Epoch: [2][117/204]	Loss 0.6674 (0.6723)	
training:	Epoch: [2][118/204]	Loss 0.6864 (0.6724)	
training:	Epoch: [2][119/204]	Loss 0.6340 (0.6721)	
training:	Epoch: [2][120/204]	Loss 0.6779 (0.6722)	
training:	Epoch: [2][121/204]	Loss 0.7272 (0.6726)	
training:	Epoch: [2][122/204]	Loss 0.6703 (0.6726)	
training:	Epoch: [2][123/204]	Loss 0.6586 (0.6725)	
training:	Epoch: [2][124/204]	Loss 0.6806 (0.6726)	
training:	Epoch: [2][125/204]	Loss 0.6770 (0.6726)	
training:	Epoch: [2][126/204]	Loss 0.6334 (0.6723)	
training:	Epoch: [2][127/204]	Loss 0.6643 (0.6722)	
training:	Epoch: [2][128/204]	Loss 0.6381 (0.6720)	
training:	Epoch: [2][129/204]	Loss 0.6598 (0.6719)	
training:	Epoch: [2][130/204]	Loss 0.6699 (0.6718)	
training:	Epoch: [2][131/204]	Loss 0.6895 (0.6720)	
training:	Epoch: [2][132/204]	Loss 0.6626 (0.6719)	
training:	Epoch: [2][133/204]	Loss 0.6374 (0.6716)	
training:	Epoch: [2][134/204]	Loss 0.5904 (0.6710)	
training:	Epoch: [2][135/204]	Loss 0.6733 (0.6711)	
training:	Epoch: [2][136/204]	Loss 0.6036 (0.6706)	
training:	Epoch: [2][137/204]	Loss 0.6702 (0.6706)	
training:	Epoch: [2][138/204]	Loss 0.6953 (0.6707)	
training:	Epoch: [2][139/204]	Loss 0.6523 (0.6706)	
training:	Epoch: [2][140/204]	Loss 0.6414 (0.6704)	
training:	Epoch: [2][141/204]	Loss 0.6462 (0.6702)	
training:	Epoch: [2][142/204]	Loss 0.6473 (0.6701)	
training:	Epoch: [2][143/204]	Loss 0.7143 (0.6704)	
training:	Epoch: [2][144/204]	Loss 0.7092 (0.6706)	
training:	Epoch: [2][145/204]	Loss 0.6515 (0.6705)	
training:	Epoch: [2][146/204]	Loss 0.6477 (0.6704)	
training:	Epoch: [2][147/204]	Loss 0.6671 (0.6703)	
training:	Epoch: [2][148/204]	Loss 0.6619 (0.6703)	
training:	Epoch: [2][149/204]	Loss 0.6438 (0.6701)	
training:	Epoch: [2][150/204]	Loss 0.6391 (0.6699)	
training:	Epoch: [2][151/204]	Loss 0.6566 (0.6698)	
training:	Epoch: [2][152/204]	Loss 0.6990 (0.6700)	
training:	Epoch: [2][153/204]	Loss 0.7092 (0.6703)	
training:	Epoch: [2][154/204]	Loss 0.6557 (0.6702)	
training:	Epoch: [2][155/204]	Loss 0.7119 (0.6704)	
training:	Epoch: [2][156/204]	Loss 0.7059 (0.6707)	
training:	Epoch: [2][157/204]	Loss 0.6818 (0.6707)	
training:	Epoch: [2][158/204]	Loss 0.6302 (0.6705)	
training:	Epoch: [2][159/204]	Loss 0.7015 (0.6707)	
training:	Epoch: [2][160/204]	Loss 0.5854 (0.6701)	
training:	Epoch: [2][161/204]	Loss 0.6868 (0.6702)	
training:	Epoch: [2][162/204]	Loss 0.6515 (0.6701)	
training:	Epoch: [2][163/204]	Loss 0.6640 (0.6701)	
training:	Epoch: [2][164/204]	Loss 0.6857 (0.6702)	
training:	Epoch: [2][165/204]	Loss 0.6556 (0.6701)	
training:	Epoch: [2][166/204]	Loss 0.6429 (0.6699)	
training:	Epoch: [2][167/204]	Loss 0.5988 (0.6695)	
training:	Epoch: [2][168/204]	Loss 0.6640 (0.6695)	
training:	Epoch: [2][169/204]	Loss 0.6546 (0.6694)	
training:	Epoch: [2][170/204]	Loss 0.6453 (0.6692)	
training:	Epoch: [2][171/204]	Loss 0.7152 (0.6695)	
training:	Epoch: [2][172/204]	Loss 0.5919 (0.6691)	
training:	Epoch: [2][173/204]	Loss 0.6312 (0.6688)	
training:	Epoch: [2][174/204]	Loss 0.6555 (0.6688)	
training:	Epoch: [2][175/204]	Loss 0.6443 (0.6686)	
training:	Epoch: [2][176/204]	Loss 0.6890 (0.6687)	
training:	Epoch: [2][177/204]	Loss 0.6628 (0.6687)	
training:	Epoch: [2][178/204]	Loss 0.6698 (0.6687)	
training:	Epoch: [2][179/204]	Loss 0.6472 (0.6686)	
training:	Epoch: [2][180/204]	Loss 0.6822 (0.6687)	
training:	Epoch: [2][181/204]	Loss 0.6648 (0.6686)	
training:	Epoch: [2][182/204]	Loss 0.5981 (0.6683)	
training:	Epoch: [2][183/204]	Loss 0.6752 (0.6683)	
training:	Epoch: [2][184/204]	Loss 0.6999 (0.6685)	
training:	Epoch: [2][185/204]	Loss 0.6873 (0.6686)	
training:	Epoch: [2][186/204]	Loss 0.6420 (0.6684)	
training:	Epoch: [2][187/204]	Loss 0.6660 (0.6684)	
training:	Epoch: [2][188/204]	Loss 0.6425 (0.6683)	
training:	Epoch: [2][189/204]	Loss 0.6697 (0.6683)	
training:	Epoch: [2][190/204]	Loss 0.7345 (0.6686)	
training:	Epoch: [2][191/204]	Loss 0.6014 (0.6683)	
training:	Epoch: [2][192/204]	Loss 0.6675 (0.6683)	
training:	Epoch: [2][193/204]	Loss 0.6618 (0.6682)	
training:	Epoch: [2][194/204]	Loss 0.6821 (0.6683)	
training:	Epoch: [2][195/204]	Loss 0.6180 (0.6681)	
training:	Epoch: [2][196/204]	Loss 0.6468 (0.6679)	
training:	Epoch: [2][197/204]	Loss 0.6257 (0.6677)	
training:	Epoch: [2][198/204]	Loss 0.6450 (0.6676)	
training:	Epoch: [2][199/204]	Loss 0.7045 (0.6678)	
training:	Epoch: [2][200/204]	Loss 0.6866 (0.6679)	
training:	Epoch: [2][201/204]	Loss 0.6501 (0.6678)	
training:	Epoch: [2][202/204]	Loss 0.6312 (0.6676)	
training:	Epoch: [2][203/204]	Loss 0.6507 (0.6675)	
training:	Epoch: [2][204/204]	Loss 0.6944 (0.6677)	
Training:	 Loss: 0.6666

Training:	 ACC: 0.6169 0.6178 0.6373 0.5966
Validation:	 ACC: 0.6130 0.6137 0.6285 0.5975
Validation:	 Best_BACC: 0.6130 0.6137 0.6285 0.5975
Validation:	 Loss: 0.6564
Pretraining:	Epoch 3/200
----------
training:	Epoch: [3][1/204]	Loss 0.6679 (0.6679)	
training:	Epoch: [3][2/204]	Loss 0.5947 (0.6313)	
training:	Epoch: [3][3/204]	Loss 0.6547 (0.6391)	
training:	Epoch: [3][4/204]	Loss 0.6905 (0.6520)	
training:	Epoch: [3][5/204]	Loss 0.7055 (0.6627)	
training:	Epoch: [3][6/204]	Loss 0.6400 (0.6589)	
training:	Epoch: [3][7/204]	Loss 0.6591 (0.6589)	
training:	Epoch: [3][8/204]	Loss 0.6667 (0.6599)	
training:	Epoch: [3][9/204]	Loss 0.6433 (0.6581)	
training:	Epoch: [3][10/204]	Loss 0.6520 (0.6574)	
training:	Epoch: [3][11/204]	Loss 0.6101 (0.6531)	
training:	Epoch: [3][12/204]	Loss 0.7426 (0.6606)	
training:	Epoch: [3][13/204]	Loss 0.6309 (0.6583)	
training:	Epoch: [3][14/204]	Loss 0.6640 (0.6587)	
training:	Epoch: [3][15/204]	Loss 0.6493 (0.6581)	
training:	Epoch: [3][16/204]	Loss 0.6586 (0.6581)	
training:	Epoch: [3][17/204]	Loss 0.6961 (0.6604)	
training:	Epoch: [3][18/204]	Loss 0.6854 (0.6617)	
training:	Epoch: [3][19/204]	Loss 0.7184 (0.6647)	
training:	Epoch: [3][20/204]	Loss 0.6535 (0.6642)	
training:	Epoch: [3][21/204]	Loss 0.6476 (0.6634)	
training:	Epoch: [3][22/204]	Loss 0.6402 (0.6623)	
training:	Epoch: [3][23/204]	Loss 0.7062 (0.6642)	
training:	Epoch: [3][24/204]	Loss 0.6520 (0.6637)	
training:	Epoch: [3][25/204]	Loss 0.7133 (0.6657)	
training:	Epoch: [3][26/204]	Loss 0.6757 (0.6661)	
training:	Epoch: [3][27/204]	Loss 0.6388 (0.6651)	
training:	Epoch: [3][28/204]	Loss 0.6079 (0.6630)	
training:	Epoch: [3][29/204]	Loss 0.6960 (0.6642)	
training:	Epoch: [3][30/204]	Loss 0.6692 (0.6643)	
training:	Epoch: [3][31/204]	Loss 0.6357 (0.6634)	
training:	Epoch: [3][32/204]	Loss 0.6664 (0.6635)	
training:	Epoch: [3][33/204]	Loss 0.5938 (0.6614)	
training:	Epoch: [3][34/204]	Loss 0.6429 (0.6608)	
training:	Epoch: [3][35/204]	Loss 0.6902 (0.6617)	
training:	Epoch: [3][36/204]	Loss 0.6185 (0.6605)	
training:	Epoch: [3][37/204]	Loss 0.6399 (0.6599)	
training:	Epoch: [3][38/204]	Loss 0.6580 (0.6599)	
training:	Epoch: [3][39/204]	Loss 0.6480 (0.6596)	
training:	Epoch: [3][40/204]	Loss 0.6210 (0.6586)	
training:	Epoch: [3][41/204]	Loss 0.6645 (0.6588)	
training:	Epoch: [3][42/204]	Loss 0.6276 (0.6580)	
training:	Epoch: [3][43/204]	Loss 0.6588 (0.6580)	
training:	Epoch: [3][44/204]	Loss 0.7362 (0.6598)	
training:	Epoch: [3][45/204]	Loss 0.6842 (0.6603)	
training:	Epoch: [3][46/204]	Loss 0.6767 (0.6607)	
training:	Epoch: [3][47/204]	Loss 0.6453 (0.6604)	
training:	Epoch: [3][48/204]	Loss 0.6703 (0.6606)	
training:	Epoch: [3][49/204]	Loss 0.6634 (0.6606)	
training:	Epoch: [3][50/204]	Loss 0.6455 (0.6603)	
training:	Epoch: [3][51/204]	Loss 0.6498 (0.6601)	
training:	Epoch: [3][52/204]	Loss 0.5677 (0.6584)	
training:	Epoch: [3][53/204]	Loss 0.6185 (0.6576)	
training:	Epoch: [3][54/204]	Loss 0.6836 (0.6581)	
training:	Epoch: [3][55/204]	Loss 0.6391 (0.6577)	
training:	Epoch: [3][56/204]	Loss 0.6922 (0.6584)	
training:	Epoch: [3][57/204]	Loss 0.6592 (0.6584)	
training:	Epoch: [3][58/204]	Loss 0.7202 (0.6594)	
training:	Epoch: [3][59/204]	Loss 0.6979 (0.6601)	
training:	Epoch: [3][60/204]	Loss 0.6939 (0.6607)	
training:	Epoch: [3][61/204]	Loss 0.7164 (0.6616)	
training:	Epoch: [3][62/204]	Loss 0.6456 (0.6613)	
training:	Epoch: [3][63/204]	Loss 0.7043 (0.6620)	
training:	Epoch: [3][64/204]	Loss 0.6779 (0.6622)	
training:	Epoch: [3][65/204]	Loss 0.6732 (0.6624)	
training:	Epoch: [3][66/204]	Loss 0.6338 (0.6620)	
training:	Epoch: [3][67/204]	Loss 0.6975 (0.6625)	
training:	Epoch: [3][68/204]	Loss 0.6169 (0.6618)	
training:	Epoch: [3][69/204]	Loss 0.6532 (0.6617)	
training:	Epoch: [3][70/204]	Loss 0.6032 (0.6609)	
training:	Epoch: [3][71/204]	Loss 0.6141 (0.6602)	
training:	Epoch: [3][72/204]	Loss 0.6691 (0.6603)	
training:	Epoch: [3][73/204]	Loss 0.6644 (0.6604)	
training:	Epoch: [3][74/204]	Loss 0.6321 (0.6600)	
training:	Epoch: [3][75/204]	Loss 0.7109 (0.6607)	
training:	Epoch: [3][76/204]	Loss 0.6709 (0.6608)	
training:	Epoch: [3][77/204]	Loss 0.6399 (0.6606)	
training:	Epoch: [3][78/204]	Loss 0.7058 (0.6611)	
training:	Epoch: [3][79/204]	Loss 0.6775 (0.6613)	
training:	Epoch: [3][80/204]	Loss 0.6159 (0.6608)	
training:	Epoch: [3][81/204]	Loss 0.6557 (0.6607)	
training:	Epoch: [3][82/204]	Loss 0.6861 (0.6610)	
training:	Epoch: [3][83/204]	Loss 0.6955 (0.6614)	
training:	Epoch: [3][84/204]	Loss 0.6801 (0.6617)	
training:	Epoch: [3][85/204]	Loss 0.6338 (0.6613)	
training:	Epoch: [3][86/204]	Loss 0.7266 (0.6621)	
training:	Epoch: [3][87/204]	Loss 0.6857 (0.6624)	
training:	Epoch: [3][88/204]	Loss 0.6187 (0.6619)	
training:	Epoch: [3][89/204]	Loss 0.6904 (0.6622)	
training:	Epoch: [3][90/204]	Loss 0.6561 (0.6621)	
training:	Epoch: [3][91/204]	Loss 0.6587 (0.6621)	
training:	Epoch: [3][92/204]	Loss 0.6181 (0.6616)	
training:	Epoch: [3][93/204]	Loss 0.6022 (0.6610)	
training:	Epoch: [3][94/204]	Loss 0.6559 (0.6609)	
training:	Epoch: [3][95/204]	Loss 0.6500 (0.6608)	
training:	Epoch: [3][96/204]	Loss 0.6366 (0.6605)	
training:	Epoch: [3][97/204]	Loss 0.6411 (0.6603)	
training:	Epoch: [3][98/204]	Loss 0.6544 (0.6603)	
training:	Epoch: [3][99/204]	Loss 0.5976 (0.6596)	
training:	Epoch: [3][100/204]	Loss 0.6275 (0.6593)	
training:	Epoch: [3][101/204]	Loss 0.6724 (0.6595)	
training:	Epoch: [3][102/204]	Loss 0.6478 (0.6593)	
training:	Epoch: [3][103/204]	Loss 0.6499 (0.6592)	
training:	Epoch: [3][104/204]	Loss 0.6021 (0.6587)	
training:	Epoch: [3][105/204]	Loss 0.6329 (0.6585)	
training:	Epoch: [3][106/204]	Loss 0.6058 (0.6580)	
training:	Epoch: [3][107/204]	Loss 0.6123 (0.6575)	
training:	Epoch: [3][108/204]	Loss 0.6293 (0.6573)	
training:	Epoch: [3][109/204]	Loss 0.6610 (0.6573)	
training:	Epoch: [3][110/204]	Loss 0.6017 (0.6568)	
training:	Epoch: [3][111/204]	Loss 0.6984 (0.6572)	
training:	Epoch: [3][112/204]	Loss 0.6591 (0.6572)	
training:	Epoch: [3][113/204]	Loss 0.7035 (0.6576)	
training:	Epoch: [3][114/204]	Loss 0.6854 (0.6578)	
training:	Epoch: [3][115/204]	Loss 0.6674 (0.6579)	
training:	Epoch: [3][116/204]	Loss 0.6021 (0.6574)	
training:	Epoch: [3][117/204]	Loss 0.6684 (0.6575)	
training:	Epoch: [3][118/204]	Loss 0.7025 (0.6579)	
training:	Epoch: [3][119/204]	Loss 0.6686 (0.6580)	
training:	Epoch: [3][120/204]	Loss 0.6369 (0.6578)	
training:	Epoch: [3][121/204]	Loss 0.7109 (0.6583)	
training:	Epoch: [3][122/204]	Loss 0.7113 (0.6587)	
training:	Epoch: [3][123/204]	Loss 0.6530 (0.6587)	
training:	Epoch: [3][124/204]	Loss 0.6178 (0.6583)	
training:	Epoch: [3][125/204]	Loss 0.6177 (0.6580)	
training:	Epoch: [3][126/204]	Loss 0.6097 (0.6576)	
training:	Epoch: [3][127/204]	Loss 0.6633 (0.6577)	
training:	Epoch: [3][128/204]	Loss 0.6422 (0.6575)	
training:	Epoch: [3][129/204]	Loss 0.6309 (0.6573)	
training:	Epoch: [3][130/204]	Loss 0.6766 (0.6575)	
training:	Epoch: [3][131/204]	Loss 0.5684 (0.6568)	
training:	Epoch: [3][132/204]	Loss 0.6853 (0.6570)	
training:	Epoch: [3][133/204]	Loss 0.6326 (0.6568)	
training:	Epoch: [3][134/204]	Loss 0.6192 (0.6566)	
training:	Epoch: [3][135/204]	Loss 0.6830 (0.6568)	
training:	Epoch: [3][136/204]	Loss 0.6470 (0.6567)	
training:	Epoch: [3][137/204]	Loss 0.6393 (0.6566)	
training:	Epoch: [3][138/204]	Loss 0.6364 (0.6564)	
training:	Epoch: [3][139/204]	Loss 0.6244 (0.6562)	
training:	Epoch: [3][140/204]	Loss 0.6377 (0.6561)	
training:	Epoch: [3][141/204]	Loss 0.6393 (0.6559)	
training:	Epoch: [3][142/204]	Loss 0.6597 (0.6560)	
training:	Epoch: [3][143/204]	Loss 0.6650 (0.6560)	
training:	Epoch: [3][144/204]	Loss 0.6238 (0.6558)	
training:	Epoch: [3][145/204]	Loss 0.6169 (0.6555)	
training:	Epoch: [3][146/204]	Loss 0.6597 (0.6556)	
training:	Epoch: [3][147/204]	Loss 0.6198 (0.6553)	
training:	Epoch: [3][148/204]	Loss 0.5734 (0.6548)	
training:	Epoch: [3][149/204]	Loss 0.6714 (0.6549)	
training:	Epoch: [3][150/204]	Loss 0.7111 (0.6552)	
training:	Epoch: [3][151/204]	Loss 0.6511 (0.6552)	
training:	Epoch: [3][152/204]	Loss 0.6607 (0.6553)	
training:	Epoch: [3][153/204]	Loss 0.5918 (0.6548)	
training:	Epoch: [3][154/204]	Loss 0.6106 (0.6546)	
training:	Epoch: [3][155/204]	Loss 0.6918 (0.6548)	
training:	Epoch: [3][156/204]	Loss 0.5890 (0.6544)	
training:	Epoch: [3][157/204]	Loss 0.6069 (0.6541)	
training:	Epoch: [3][158/204]	Loss 0.6260 (0.6539)	
training:	Epoch: [3][159/204]	Loss 0.6408 (0.6538)	
training:	Epoch: [3][160/204]	Loss 0.6701 (0.6539)	
training:	Epoch: [3][161/204]	Loss 0.6143 (0.6537)	
training:	Epoch: [3][162/204]	Loss 0.7035 (0.6540)	
training:	Epoch: [3][163/204]	Loss 0.6048 (0.6537)	
training:	Epoch: [3][164/204]	Loss 0.6651 (0.6537)	
training:	Epoch: [3][165/204]	Loss 0.6651 (0.6538)	
training:	Epoch: [3][166/204]	Loss 0.6166 (0.6536)	
training:	Epoch: [3][167/204]	Loss 0.6517 (0.6536)	
training:	Epoch: [3][168/204]	Loss 0.6660 (0.6536)	
training:	Epoch: [3][169/204]	Loss 0.6859 (0.6538)	
training:	Epoch: [3][170/204]	Loss 0.6664 (0.6539)	
training:	Epoch: [3][171/204]	Loss 0.5901 (0.6535)	
training:	Epoch: [3][172/204]	Loss 0.6027 (0.6532)	
training:	Epoch: [3][173/204]	Loss 0.6076 (0.6530)	
training:	Epoch: [3][174/204]	Loss 0.6454 (0.6529)	
training:	Epoch: [3][175/204]	Loss 0.6510 (0.6529)	
training:	Epoch: [3][176/204]	Loss 0.6485 (0.6529)	
training:	Epoch: [3][177/204]	Loss 0.5922 (0.6526)	
training:	Epoch: [3][178/204]	Loss 0.6148 (0.6523)	
training:	Epoch: [3][179/204]	Loss 0.6332 (0.6522)	
training:	Epoch: [3][180/204]	Loss 0.6452 (0.6522)	
training:	Epoch: [3][181/204]	Loss 0.6262 (0.6521)	
training:	Epoch: [3][182/204]	Loss 0.6405 (0.6520)	
training:	Epoch: [3][183/204]	Loss 0.6798 (0.6521)	
training:	Epoch: [3][184/204]	Loss 0.6596 (0.6522)	
training:	Epoch: [3][185/204]	Loss 0.6212 (0.6520)	
training:	Epoch: [3][186/204]	Loss 0.6504 (0.6520)	
training:	Epoch: [3][187/204]	Loss 0.7064 (0.6523)	
training:	Epoch: [3][188/204]	Loss 0.6018 (0.6520)	
training:	Epoch: [3][189/204]	Loss 0.6360 (0.6519)	
training:	Epoch: [3][190/204]	Loss 0.5723 (0.6515)	
training:	Epoch: [3][191/204]	Loss 0.5700 (0.6511)	
training:	Epoch: [3][192/204]	Loss 0.7484 (0.6516)	
training:	Epoch: [3][193/204]	Loss 0.5643 (0.6512)	
training:	Epoch: [3][194/204]	Loss 0.6341 (0.6511)	
training:	Epoch: [3][195/204]	Loss 0.6079 (0.6508)	
training:	Epoch: [3][196/204]	Loss 0.6392 (0.6508)	
training:	Epoch: [3][197/204]	Loss 0.6583 (0.6508)	
training:	Epoch: [3][198/204]	Loss 0.6683 (0.6509)	
training:	Epoch: [3][199/204]	Loss 0.6773 (0.6510)	
training:	Epoch: [3][200/204]	Loss 0.6207 (0.6509)	
training:	Epoch: [3][201/204]	Loss 0.6379 (0.6508)	
training:	Epoch: [3][202/204]	Loss 0.6926 (0.6510)	
training:	Epoch: [3][203/204]	Loss 0.6441 (0.6510)	
training:	Epoch: [3][204/204]	Loss 0.7064 (0.6513)	
Training:	 Loss: 0.6503

Training:	 ACC: 0.6424 0.6435 0.6693 0.6154
Validation:	 ACC: 0.6347 0.6356 0.6561 0.6132
Validation:	 Best_BACC: 0.6347 0.6356 0.6561 0.6132
Validation:	 Loss: 0.6409
Pretraining:	Epoch 4/200
----------
training:	Epoch: [4][1/204]	Loss 0.5632 (0.5632)	
training:	Epoch: [4][2/204]	Loss 0.6328 (0.5980)	
training:	Epoch: [4][3/204]	Loss 0.6585 (0.6182)	
training:	Epoch: [4][4/204]	Loss 0.6021 (0.6142)	
training:	Epoch: [4][5/204]	Loss 0.6061 (0.6126)	
training:	Epoch: [4][6/204]	Loss 0.6664 (0.6215)	
training:	Epoch: [4][7/204]	Loss 0.5707 (0.6143)	
training:	Epoch: [4][8/204]	Loss 0.6910 (0.6239)	
training:	Epoch: [4][9/204]	Loss 0.6882 (0.6310)	
training:	Epoch: [4][10/204]	Loss 0.6433 (0.6323)	
training:	Epoch: [4][11/204]	Loss 0.6792 (0.6365)	
training:	Epoch: [4][12/204]	Loss 0.6302 (0.6360)	
training:	Epoch: [4][13/204]	Loss 0.6615 (0.6380)	
training:	Epoch: [4][14/204]	Loss 0.7097 (0.6431)	
training:	Epoch: [4][15/204]	Loss 0.6665 (0.6446)	
training:	Epoch: [4][16/204]	Loss 0.6740 (0.6465)	
training:	Epoch: [4][17/204]	Loss 0.5324 (0.6398)	
training:	Epoch: [4][18/204]	Loss 0.6688 (0.6414)	
training:	Epoch: [4][19/204]	Loss 0.6427 (0.6415)	
training:	Epoch: [4][20/204]	Loss 0.5427 (0.6365)	
training:	Epoch: [4][21/204]	Loss 0.6505 (0.6372)	
training:	Epoch: [4][22/204]	Loss 0.5866 (0.6349)	
training:	Epoch: [4][23/204]	Loss 0.6676 (0.6363)	
training:	Epoch: [4][24/204]	Loss 0.6752 (0.6379)	
training:	Epoch: [4][25/204]	Loss 0.5703 (0.6352)	
training:	Epoch: [4][26/204]	Loss 0.5840 (0.6332)	
training:	Epoch: [4][27/204]	Loss 0.6262 (0.6330)	
training:	Epoch: [4][28/204]	Loss 0.6818 (0.6347)	
training:	Epoch: [4][29/204]	Loss 0.5814 (0.6329)	
training:	Epoch: [4][30/204]	Loss 0.7184 (0.6357)	
training:	Epoch: [4][31/204]	Loss 0.6617 (0.6366)	
training:	Epoch: [4][32/204]	Loss 0.6730 (0.6377)	
training:	Epoch: [4][33/204]	Loss 0.6621 (0.6385)	
training:	Epoch: [4][34/204]	Loss 0.7105 (0.6406)	
training:	Epoch: [4][35/204]	Loss 0.5787 (0.6388)	
training:	Epoch: [4][36/204]	Loss 0.7237 (0.6412)	
training:	Epoch: [4][37/204]	Loss 0.6543 (0.6415)	
training:	Epoch: [4][38/204]	Loss 0.6383 (0.6414)	
training:	Epoch: [4][39/204]	Loss 0.6793 (0.6424)	
training:	Epoch: [4][40/204]	Loss 0.6190 (0.6418)	
training:	Epoch: [4][41/204]	Loss 0.6464 (0.6419)	
training:	Epoch: [4][42/204]	Loss 0.6625 (0.6424)	
training:	Epoch: [4][43/204]	Loss 0.6009 (0.6415)	
training:	Epoch: [4][44/204]	Loss 0.5667 (0.6398)	
training:	Epoch: [4][45/204]	Loss 0.6875 (0.6408)	
training:	Epoch: [4][46/204]	Loss 0.6598 (0.6412)	
training:	Epoch: [4][47/204]	Loss 0.5907 (0.6402)	
training:	Epoch: [4][48/204]	Loss 0.6876 (0.6411)	
training:	Epoch: [4][49/204]	Loss 0.6272 (0.6409)	
training:	Epoch: [4][50/204]	Loss 0.6289 (0.6406)	
training:	Epoch: [4][51/204]	Loss 0.6741 (0.6413)	
training:	Epoch: [4][52/204]	Loss 0.6345 (0.6411)	
training:	Epoch: [4][53/204]	Loss 0.6061 (0.6405)	
training:	Epoch: [4][54/204]	Loss 0.6452 (0.6406)	
training:	Epoch: [4][55/204]	Loss 0.5672 (0.6392)	
training:	Epoch: [4][56/204]	Loss 0.6434 (0.6393)	
training:	Epoch: [4][57/204]	Loss 0.6238 (0.6390)	
training:	Epoch: [4][58/204]	Loss 0.6569 (0.6394)	
training:	Epoch: [4][59/204]	Loss 0.5517 (0.6379)	
training:	Epoch: [4][60/204]	Loss 0.6257 (0.6377)	
training:	Epoch: [4][61/204]	Loss 0.6692 (0.6382)	
training:	Epoch: [4][62/204]	Loss 0.7044 (0.6392)	
training:	Epoch: [4][63/204]	Loss 0.5686 (0.6381)	
training:	Epoch: [4][64/204]	Loss 0.5734 (0.6371)	
training:	Epoch: [4][65/204]	Loss 0.6490 (0.6373)	
training:	Epoch: [4][66/204]	Loss 0.6426 (0.6374)	
training:	Epoch: [4][67/204]	Loss 0.6160 (0.6371)	
training:	Epoch: [4][68/204]	Loss 0.6351 (0.6370)	
training:	Epoch: [4][69/204]	Loss 0.6609 (0.6374)	
training:	Epoch: [4][70/204]	Loss 0.6877 (0.6381)	
training:	Epoch: [4][71/204]	Loss 0.6160 (0.6378)	
training:	Epoch: [4][72/204]	Loss 0.6041 (0.6373)	
training:	Epoch: [4][73/204]	Loss 0.6139 (0.6370)	
training:	Epoch: [4][74/204]	Loss 0.6410 (0.6371)	
training:	Epoch: [4][75/204]	Loss 0.7057 (0.6380)	
training:	Epoch: [4][76/204]	Loss 0.7027 (0.6388)	
training:	Epoch: [4][77/204]	Loss 0.6609 (0.6391)	
training:	Epoch: [4][78/204]	Loss 0.6445 (0.6392)	
training:	Epoch: [4][79/204]	Loss 0.6164 (0.6389)	
training:	Epoch: [4][80/204]	Loss 0.6123 (0.6386)	
training:	Epoch: [4][81/204]	Loss 0.6733 (0.6390)	
training:	Epoch: [4][82/204]	Loss 0.6480 (0.6391)	
training:	Epoch: [4][83/204]	Loss 0.6598 (0.6393)	
training:	Epoch: [4][84/204]	Loss 0.5602 (0.6384)	
training:	Epoch: [4][85/204]	Loss 0.5982 (0.6379)	
training:	Epoch: [4][86/204]	Loss 0.6698 (0.6383)	
training:	Epoch: [4][87/204]	Loss 0.7002 (0.6390)	
training:	Epoch: [4][88/204]	Loss 0.6314 (0.6389)	
training:	Epoch: [4][89/204]	Loss 0.6896 (0.6395)	
training:	Epoch: [4][90/204]	Loss 0.6614 (0.6397)	
training:	Epoch: [4][91/204]	Loss 0.6058 (0.6394)	
training:	Epoch: [4][92/204]	Loss 0.6206 (0.6392)	
training:	Epoch: [4][93/204]	Loss 0.5272 (0.6380)	
training:	Epoch: [4][94/204]	Loss 0.6715 (0.6383)	
training:	Epoch: [4][95/204]	Loss 0.6863 (0.6388)	
training:	Epoch: [4][96/204]	Loss 0.7208 (0.6397)	
training:	Epoch: [4][97/204]	Loss 0.6526 (0.6398)	
training:	Epoch: [4][98/204]	Loss 0.6483 (0.6399)	
training:	Epoch: [4][99/204]	Loss 0.5776 (0.6393)	
training:	Epoch: [4][100/204]	Loss 0.5892 (0.6388)	
training:	Epoch: [4][101/204]	Loss 0.6130 (0.6385)	
training:	Epoch: [4][102/204]	Loss 0.5924 (0.6381)	
training:	Epoch: [4][103/204]	Loss 0.6248 (0.6379)	
training:	Epoch: [4][104/204]	Loss 0.6714 (0.6382)	
training:	Epoch: [4][105/204]	Loss 0.5765 (0.6377)	
training:	Epoch: [4][106/204]	Loss 0.6585 (0.6379)	
training:	Epoch: [4][107/204]	Loss 0.6607 (0.6381)	
training:	Epoch: [4][108/204]	Loss 0.6576 (0.6382)	
training:	Epoch: [4][109/204]	Loss 0.6956 (0.6388)	
training:	Epoch: [4][110/204]	Loss 0.6040 (0.6385)	
training:	Epoch: [4][111/204]	Loss 0.6751 (0.6388)	
training:	Epoch: [4][112/204]	Loss 0.7099 (0.6394)	
training:	Epoch: [4][113/204]	Loss 0.6632 (0.6396)	
training:	Epoch: [4][114/204]	Loss 0.6075 (0.6394)	
training:	Epoch: [4][115/204]	Loss 0.6276 (0.6392)	
training:	Epoch: [4][116/204]	Loss 0.6769 (0.6396)	
training:	Epoch: [4][117/204]	Loss 0.6442 (0.6396)	
training:	Epoch: [4][118/204]	Loss 0.6369 (0.6396)	
training:	Epoch: [4][119/204]	Loss 0.7051 (0.6401)	
training:	Epoch: [4][120/204]	Loss 0.6865 (0.6405)	
training:	Epoch: [4][121/204]	Loss 0.5677 (0.6399)	
training:	Epoch: [4][122/204]	Loss 0.6597 (0.6401)	
training:	Epoch: [4][123/204]	Loss 0.6562 (0.6402)	
training:	Epoch: [4][124/204]	Loss 0.6192 (0.6400)	
training:	Epoch: [4][125/204]	Loss 0.6008 (0.6397)	
training:	Epoch: [4][126/204]	Loss 0.5739 (0.6392)	
training:	Epoch: [4][127/204]	Loss 0.6984 (0.6397)	
training:	Epoch: [4][128/204]	Loss 0.6775 (0.6400)	
training:	Epoch: [4][129/204]	Loss 0.5868 (0.6396)	
training:	Epoch: [4][130/204]	Loss 0.6171 (0.6394)	
training:	Epoch: [4][131/204]	Loss 0.6373 (0.6394)	
training:	Epoch: [4][132/204]	Loss 0.6441 (0.6394)	
training:	Epoch: [4][133/204]	Loss 0.6933 (0.6398)	
training:	Epoch: [4][134/204]	Loss 0.6526 (0.6399)	
training:	Epoch: [4][135/204]	Loss 0.6426 (0.6399)	
training:	Epoch: [4][136/204]	Loss 0.6716 (0.6402)	
training:	Epoch: [4][137/204]	Loss 0.6161 (0.6400)	
training:	Epoch: [4][138/204]	Loss 0.6028 (0.6397)	
training:	Epoch: [4][139/204]	Loss 0.6740 (0.6400)	
training:	Epoch: [4][140/204]	Loss 0.6239 (0.6398)	
training:	Epoch: [4][141/204]	Loss 0.5959 (0.6395)	
training:	Epoch: [4][142/204]	Loss 0.6233 (0.6394)	
training:	Epoch: [4][143/204]	Loss 0.6278 (0.6393)	
training:	Epoch: [4][144/204]	Loss 0.6345 (0.6393)	
training:	Epoch: [4][145/204]	Loss 0.5935 (0.6390)	
training:	Epoch: [4][146/204]	Loss 0.6199 (0.6389)	
training:	Epoch: [4][147/204]	Loss 0.7141 (0.6394)	
training:	Epoch: [4][148/204]	Loss 0.5084 (0.6385)	
training:	Epoch: [4][149/204]	Loss 0.5598 (0.6380)	
training:	Epoch: [4][150/204]	Loss 0.7181 (0.6385)	
training:	Epoch: [4][151/204]	Loss 0.6249 (0.6384)	
training:	Epoch: [4][152/204]	Loss 0.5749 (0.6380)	
training:	Epoch: [4][153/204]	Loss 0.6177 (0.6379)	
training:	Epoch: [4][154/204]	Loss 0.6354 (0.6378)	
training:	Epoch: [4][155/204]	Loss 0.6085 (0.6376)	
training:	Epoch: [4][156/204]	Loss 0.6001 (0.6374)	
training:	Epoch: [4][157/204]	Loss 0.7355 (0.6380)	
training:	Epoch: [4][158/204]	Loss 0.5760 (0.6376)	
training:	Epoch: [4][159/204]	Loss 0.5882 (0.6373)	
training:	Epoch: [4][160/204]	Loss 0.6506 (0.6374)	
training:	Epoch: [4][161/204]	Loss 0.6535 (0.6375)	
training:	Epoch: [4][162/204]	Loss 0.5620 (0.6370)	
training:	Epoch: [4][163/204]	Loss 0.6405 (0.6371)	
training:	Epoch: [4][164/204]	Loss 0.5666 (0.6366)	
training:	Epoch: [4][165/204]	Loss 0.5867 (0.6363)	
training:	Epoch: [4][166/204]	Loss 0.6205 (0.6362)	
training:	Epoch: [4][167/204]	Loss 0.6226 (0.6362)	
training:	Epoch: [4][168/204]	Loss 0.6792 (0.6364)	
training:	Epoch: [4][169/204]	Loss 0.6249 (0.6363)	
training:	Epoch: [4][170/204]	Loss 0.6677 (0.6365)	
training:	Epoch: [4][171/204]	Loss 0.5982 (0.6363)	
training:	Epoch: [4][172/204]	Loss 0.6385 (0.6363)	
training:	Epoch: [4][173/204]	Loss 0.5680 (0.6359)	
training:	Epoch: [4][174/204]	Loss 0.6776 (0.6362)	
training:	Epoch: [4][175/204]	Loss 0.4999 (0.6354)	
training:	Epoch: [4][176/204]	Loss 0.6111 (0.6352)	
training:	Epoch: [4][177/204]	Loss 0.5869 (0.6350)	
training:	Epoch: [4][178/204]	Loss 0.6295 (0.6349)	
training:	Epoch: [4][179/204]	Loss 0.6463 (0.6350)	
training:	Epoch: [4][180/204]	Loss 0.6932 (0.6353)	
training:	Epoch: [4][181/204]	Loss 0.6149 (0.6352)	
training:	Epoch: [4][182/204]	Loss 0.6795 (0.6355)	
training:	Epoch: [4][183/204]	Loss 0.6561 (0.6356)	
training:	Epoch: [4][184/204]	Loss 0.6106 (0.6354)	
training:	Epoch: [4][185/204]	Loss 0.6621 (0.6356)	
training:	Epoch: [4][186/204]	Loss 0.7045 (0.6359)	
training:	Epoch: [4][187/204]	Loss 0.6361 (0.6360)	
training:	Epoch: [4][188/204]	Loss 0.6953 (0.6363)	
training:	Epoch: [4][189/204]	Loss 0.6131 (0.6361)	
training:	Epoch: [4][190/204]	Loss 0.6065 (0.6360)	
training:	Epoch: [4][191/204]	Loss 0.6631 (0.6361)	
training:	Epoch: [4][192/204]	Loss 0.5556 (0.6357)	
training:	Epoch: [4][193/204]	Loss 0.6328 (0.6357)	
training:	Epoch: [4][194/204]	Loss 0.6957 (0.6360)	
training:	Epoch: [4][195/204]	Loss 0.6145 (0.6359)	
training:	Epoch: [4][196/204]	Loss 0.6551 (0.6360)	
training:	Epoch: [4][197/204]	Loss 0.6707 (0.6362)	
training:	Epoch: [4][198/204]	Loss 0.7144 (0.6366)	
training:	Epoch: [4][199/204]	Loss 0.6792 (0.6368)	
training:	Epoch: [4][200/204]	Loss 0.6694 (0.6369)	
training:	Epoch: [4][201/204]	Loss 0.6158 (0.6368)	
training:	Epoch: [4][202/204]	Loss 0.6253 (0.6368)	
training:	Epoch: [4][203/204]	Loss 0.6053 (0.6366)	
training:	Epoch: [4][204/204]	Loss 0.5951 (0.6364)	
Training:	 Loss: 0.6354

Training:	 ACC: 0.6590 0.6594 0.6690 0.6489
Validation:	 ACC: 0.6519 0.6522 0.6592 0.6446
Validation:	 Best_BACC: 0.6519 0.6522 0.6592 0.6446
Validation:	 Loss: 0.6287
Pretraining:	Epoch 5/200
----------
training:	Epoch: [5][1/204]	Loss 0.5813 (0.5813)	
training:	Epoch: [5][2/204]	Loss 0.6287 (0.6050)	
training:	Epoch: [5][3/204]	Loss 0.6493 (0.6198)	
training:	Epoch: [5][4/204]	Loss 0.5847 (0.6110)	
training:	Epoch: [5][5/204]	Loss 0.6483 (0.6185)	
training:	Epoch: [5][6/204]	Loss 0.5962 (0.6147)	
training:	Epoch: [5][7/204]	Loss 0.5714 (0.6086)	
training:	Epoch: [5][8/204]	Loss 0.6307 (0.6113)	
training:	Epoch: [5][9/204]	Loss 0.6126 (0.6115)	
training:	Epoch: [5][10/204]	Loss 0.5548 (0.6058)	
training:	Epoch: [5][11/204]	Loss 0.6770 (0.6123)	
training:	Epoch: [5][12/204]	Loss 0.5698 (0.6087)	
training:	Epoch: [5][13/204]	Loss 0.5750 (0.6061)	
training:	Epoch: [5][14/204]	Loss 0.5899 (0.6050)	
training:	Epoch: [5][15/204]	Loss 0.6277 (0.6065)	
training:	Epoch: [5][16/204]	Loss 0.6171 (0.6072)	
training:	Epoch: [5][17/204]	Loss 0.6529 (0.6098)	
training:	Epoch: [5][18/204]	Loss 0.5765 (0.6080)	
training:	Epoch: [5][19/204]	Loss 0.5482 (0.6048)	
training:	Epoch: [5][20/204]	Loss 0.6290 (0.6061)	
training:	Epoch: [5][21/204]	Loss 0.6231 (0.6069)	
training:	Epoch: [5][22/204]	Loss 0.6907 (0.6107)	
training:	Epoch: [5][23/204]	Loss 0.5894 (0.6097)	
training:	Epoch: [5][24/204]	Loss 0.6675 (0.6122)	
training:	Epoch: [5][25/204]	Loss 0.6550 (0.6139)	
training:	Epoch: [5][26/204]	Loss 0.7022 (0.6173)	
training:	Epoch: [5][27/204]	Loss 0.6192 (0.6173)	
training:	Epoch: [5][28/204]	Loss 0.6115 (0.6171)	
training:	Epoch: [5][29/204]	Loss 0.6289 (0.6175)	
training:	Epoch: [5][30/204]	Loss 0.6309 (0.6180)	
training:	Epoch: [5][31/204]	Loss 0.5855 (0.6169)	
training:	Epoch: [5][32/204]	Loss 0.6358 (0.6175)	
training:	Epoch: [5][33/204]	Loss 0.6986 (0.6200)	
training:	Epoch: [5][34/204]	Loss 0.5131 (0.6168)	
training:	Epoch: [5][35/204]	Loss 0.6296 (0.6172)	
training:	Epoch: [5][36/204]	Loss 0.6054 (0.6169)	
training:	Epoch: [5][37/204]	Loss 0.6074 (0.6166)	
training:	Epoch: [5][38/204]	Loss 0.7383 (0.6198)	
training:	Epoch: [5][39/204]	Loss 0.6300 (0.6201)	
training:	Epoch: [5][40/204]	Loss 0.6091 (0.6198)	
training:	Epoch: [5][41/204]	Loss 0.6639 (0.6209)	
training:	Epoch: [5][42/204]	Loss 0.5343 (0.6188)	
training:	Epoch: [5][43/204]	Loss 0.6140 (0.6187)	
training:	Epoch: [5][44/204]	Loss 0.6419 (0.6192)	
training:	Epoch: [5][45/204]	Loss 0.6704 (0.6204)	
training:	Epoch: [5][46/204]	Loss 0.6057 (0.6201)	
training:	Epoch: [5][47/204]	Loss 0.5913 (0.6194)	
training:	Epoch: [5][48/204]	Loss 0.6683 (0.6205)	
training:	Epoch: [5][49/204]	Loss 0.6628 (0.6213)	
training:	Epoch: [5][50/204]	Loss 0.6024 (0.6209)	
training:	Epoch: [5][51/204]	Loss 0.6735 (0.6220)	
training:	Epoch: [5][52/204]	Loss 0.6331 (0.6222)	
training:	Epoch: [5][53/204]	Loss 0.5759 (0.6213)	
training:	Epoch: [5][54/204]	Loss 0.6000 (0.6209)	
training:	Epoch: [5][55/204]	Loss 0.6559 (0.6216)	
training:	Epoch: [5][56/204]	Loss 0.5536 (0.6203)	
training:	Epoch: [5][57/204]	Loss 0.6181 (0.6203)	
training:	Epoch: [5][58/204]	Loss 0.6975 (0.6216)	
training:	Epoch: [5][59/204]	Loss 0.6368 (0.6219)	
training:	Epoch: [5][60/204]	Loss 0.5806 (0.6212)	
training:	Epoch: [5][61/204]	Loss 0.5913 (0.6207)	
training:	Epoch: [5][62/204]	Loss 0.6020 (0.6204)	
training:	Epoch: [5][63/204]	Loss 0.6061 (0.6202)	
training:	Epoch: [5][64/204]	Loss 0.6825 (0.6212)	
training:	Epoch: [5][65/204]	Loss 0.6380 (0.6214)	
training:	Epoch: [5][66/204]	Loss 0.6650 (0.6221)	
training:	Epoch: [5][67/204]	Loss 0.6899 (0.6231)	
training:	Epoch: [5][68/204]	Loss 0.5746 (0.6224)	
training:	Epoch: [5][69/204]	Loss 0.6255 (0.6224)	
training:	Epoch: [5][70/204]	Loss 0.6394 (0.6227)	
training:	Epoch: [5][71/204]	Loss 0.5807 (0.6221)	
training:	Epoch: [5][72/204]	Loss 0.6520 (0.6225)	
training:	Epoch: [5][73/204]	Loss 0.5551 (0.6216)	
training:	Epoch: [5][74/204]	Loss 0.5923 (0.6212)	
training:	Epoch: [5][75/204]	Loss 0.6273 (0.6213)	
training:	Epoch: [5][76/204]	Loss 0.6431 (0.6215)	
training:	Epoch: [5][77/204]	Loss 0.5969 (0.6212)	
training:	Epoch: [5][78/204]	Loss 0.6178 (0.6212)	
training:	Epoch: [5][79/204]	Loss 0.6204 (0.6212)	
training:	Epoch: [5][80/204]	Loss 0.5841 (0.6207)	
training:	Epoch: [5][81/204]	Loss 0.5591 (0.6199)	
training:	Epoch: [5][82/204]	Loss 0.6234 (0.6200)	
training:	Epoch: [5][83/204]	Loss 0.6093 (0.6199)	
training:	Epoch: [5][84/204]	Loss 0.5306 (0.6188)	
training:	Epoch: [5][85/204]	Loss 0.5608 (0.6181)	
training:	Epoch: [5][86/204]	Loss 0.5773 (0.6176)	
training:	Epoch: [5][87/204]	Loss 0.6228 (0.6177)	
training:	Epoch: [5][88/204]	Loss 0.5803 (0.6173)	
training:	Epoch: [5][89/204]	Loss 0.5845 (0.6169)	
training:	Epoch: [5][90/204]	Loss 0.6585 (0.6174)	
training:	Epoch: [5][91/204]	Loss 0.5655 (0.6168)	
training:	Epoch: [5][92/204]	Loss 0.5747 (0.6163)	
training:	Epoch: [5][93/204]	Loss 0.6095 (0.6163)	
training:	Epoch: [5][94/204]	Loss 0.6078 (0.6162)	
training:	Epoch: [5][95/204]	Loss 0.5938 (0.6159)	
training:	Epoch: [5][96/204]	Loss 0.6014 (0.6158)	
training:	Epoch: [5][97/204]	Loss 0.5815 (0.6154)	
training:	Epoch: [5][98/204]	Loss 0.6456 (0.6157)	
training:	Epoch: [5][99/204]	Loss 0.5907 (0.6155)	
training:	Epoch: [5][100/204]	Loss 0.6890 (0.6162)	
training:	Epoch: [5][101/204]	Loss 0.6458 (0.6165)	
training:	Epoch: [5][102/204]	Loss 0.7245 (0.6176)	
training:	Epoch: [5][103/204]	Loss 0.6182 (0.6176)	
training:	Epoch: [5][104/204]	Loss 0.6712 (0.6181)	
training:	Epoch: [5][105/204]	Loss 0.6188 (0.6181)	
training:	Epoch: [5][106/204]	Loss 0.5799 (0.6177)	
training:	Epoch: [5][107/204]	Loss 0.5693 (0.6173)	
training:	Epoch: [5][108/204]	Loss 0.5825 (0.6170)	
training:	Epoch: [5][109/204]	Loss 0.6030 (0.6168)	
training:	Epoch: [5][110/204]	Loss 0.7182 (0.6178)	
training:	Epoch: [5][111/204]	Loss 0.6391 (0.6180)	
training:	Epoch: [5][112/204]	Loss 0.6038 (0.6178)	
training:	Epoch: [5][113/204]	Loss 0.6882 (0.6184)	
training:	Epoch: [5][114/204]	Loss 0.6022 (0.6183)	
training:	Epoch: [5][115/204]	Loss 0.6975 (0.6190)	
training:	Epoch: [5][116/204]	Loss 0.6595 (0.6193)	
training:	Epoch: [5][117/204]	Loss 0.7061 (0.6201)	
training:	Epoch: [5][118/204]	Loss 0.6315 (0.6202)	
training:	Epoch: [5][119/204]	Loss 0.5490 (0.6196)	
training:	Epoch: [5][120/204]	Loss 0.6729 (0.6200)	
training:	Epoch: [5][121/204]	Loss 0.6797 (0.6205)	
training:	Epoch: [5][122/204]	Loss 0.6344 (0.6206)	
training:	Epoch: [5][123/204]	Loss 0.6012 (0.6205)	
training:	Epoch: [5][124/204]	Loss 0.5502 (0.6199)	
training:	Epoch: [5][125/204]	Loss 0.6811 (0.6204)	
training:	Epoch: [5][126/204]	Loss 0.7223 (0.6212)	
training:	Epoch: [5][127/204]	Loss 0.6187 (0.6212)	
training:	Epoch: [5][128/204]	Loss 0.6725 (0.6216)	
training:	Epoch: [5][129/204]	Loss 0.5820 (0.6213)	
training:	Epoch: [5][130/204]	Loss 0.7355 (0.6222)	
training:	Epoch: [5][131/204]	Loss 0.5678 (0.6217)	
training:	Epoch: [5][132/204]	Loss 0.6125 (0.6217)	
training:	Epoch: [5][133/204]	Loss 0.6378 (0.6218)	
training:	Epoch: [5][134/204]	Loss 0.7045 (0.6224)	
training:	Epoch: [5][135/204]	Loss 0.6785 (0.6228)	
training:	Epoch: [5][136/204]	Loss 0.6694 (0.6232)	
training:	Epoch: [5][137/204]	Loss 0.6377 (0.6233)	
training:	Epoch: [5][138/204]	Loss 0.6076 (0.6232)	
training:	Epoch: [5][139/204]	Loss 0.6250 (0.6232)	
training:	Epoch: [5][140/204]	Loss 0.5671 (0.6228)	
training:	Epoch: [5][141/204]	Loss 0.5201 (0.6221)	
training:	Epoch: [5][142/204]	Loss 0.7325 (0.6228)	
training:	Epoch: [5][143/204]	Loss 0.6627 (0.6231)	
training:	Epoch: [5][144/204]	Loss 0.6506 (0.6233)	
training:	Epoch: [5][145/204]	Loss 0.6479 (0.6235)	
training:	Epoch: [5][146/204]	Loss 0.5778 (0.6232)	
training:	Epoch: [5][147/204]	Loss 0.5936 (0.6230)	
training:	Epoch: [5][148/204]	Loss 0.6490 (0.6231)	
training:	Epoch: [5][149/204]	Loss 0.6783 (0.6235)	
training:	Epoch: [5][150/204]	Loss 0.6706 (0.6238)	
training:	Epoch: [5][151/204]	Loss 0.6057 (0.6237)	
training:	Epoch: [5][152/204]	Loss 0.6093 (0.6236)	
training:	Epoch: [5][153/204]	Loss 0.6972 (0.6241)	
training:	Epoch: [5][154/204]	Loss 0.5803 (0.6238)	
training:	Epoch: [5][155/204]	Loss 0.6783 (0.6241)	
training:	Epoch: [5][156/204]	Loss 0.5732 (0.6238)	
training:	Epoch: [5][157/204]	Loss 0.6498 (0.6240)	
training:	Epoch: [5][158/204]	Loss 0.5436 (0.6235)	
training:	Epoch: [5][159/204]	Loss 0.7365 (0.6242)	
training:	Epoch: [5][160/204]	Loss 0.6406 (0.6243)	
training:	Epoch: [5][161/204]	Loss 0.6011 (0.6241)	
training:	Epoch: [5][162/204]	Loss 0.6399 (0.6242)	
training:	Epoch: [5][163/204]	Loss 0.6166 (0.6242)	
training:	Epoch: [5][164/204]	Loss 0.5495 (0.6237)	
training:	Epoch: [5][165/204]	Loss 0.6404 (0.6238)	
training:	Epoch: [5][166/204]	Loss 0.6020 (0.6237)	
training:	Epoch: [5][167/204]	Loss 0.7966 (0.6247)	
training:	Epoch: [5][168/204]	Loss 0.5964 (0.6246)	
training:	Epoch: [5][169/204]	Loss 0.6857 (0.6249)	
training:	Epoch: [5][170/204]	Loss 0.7557 (0.6257)	
training:	Epoch: [5][171/204]	Loss 0.5927 (0.6255)	
training:	Epoch: [5][172/204]	Loss 0.5624 (0.6251)	
training:	Epoch: [5][173/204]	Loss 0.5155 (0.6245)	
training:	Epoch: [5][174/204]	Loss 0.6269 (0.6245)	
training:	Epoch: [5][175/204]	Loss 0.5819 (0.6243)	
training:	Epoch: [5][176/204]	Loss 0.7743 (0.6251)	
training:	Epoch: [5][177/204]	Loss 0.5917 (0.6249)	
training:	Epoch: [5][178/204]	Loss 0.6056 (0.6248)	
training:	Epoch: [5][179/204]	Loss 0.6200 (0.6248)	
training:	Epoch: [5][180/204]	Loss 0.6956 (0.6252)	
training:	Epoch: [5][181/204]	Loss 0.6379 (0.6253)	
training:	Epoch: [5][182/204]	Loss 0.6420 (0.6254)	
training:	Epoch: [5][183/204]	Loss 0.5590 (0.6250)	
training:	Epoch: [5][184/204]	Loss 0.6251 (0.6250)	
training:	Epoch: [5][185/204]	Loss 0.6666 (0.6252)	
training:	Epoch: [5][186/204]	Loss 0.5768 (0.6250)	
training:	Epoch: [5][187/204]	Loss 0.6281 (0.6250)	
training:	Epoch: [5][188/204]	Loss 0.6246 (0.6250)	
training:	Epoch: [5][189/204]	Loss 0.5996 (0.6248)	
training:	Epoch: [5][190/204]	Loss 0.5666 (0.6245)	
training:	Epoch: [5][191/204]	Loss 0.5165 (0.6240)	
training:	Epoch: [5][192/204]	Loss 0.6025 (0.6239)	
training:	Epoch: [5][193/204]	Loss 0.5652 (0.6236)	
training:	Epoch: [5][194/204]	Loss 0.6662 (0.6238)	
training:	Epoch: [5][195/204]	Loss 0.6034 (0.6237)	
training:	Epoch: [5][196/204]	Loss 0.4876 (0.6230)	
training:	Epoch: [5][197/204]	Loss 0.6434 (0.6231)	
training:	Epoch: [5][198/204]	Loss 0.6356 (0.6231)	
training:	Epoch: [5][199/204]	Loss 0.6812 (0.6234)	
training:	Epoch: [5][200/204]	Loss 0.5516 (0.6231)	
training:	Epoch: [5][201/204]	Loss 0.6090 (0.6230)	
training:	Epoch: [5][202/204]	Loss 0.5982 (0.6229)	
training:	Epoch: [5][203/204]	Loss 0.6964 (0.6233)	
training:	Epoch: [5][204/204]	Loss 0.6149 (0.6232)	
Training:	 Loss: 0.6223

Training:	 ACC: 0.6716 0.6731 0.7090 0.6342
Validation:	 ACC: 0.6654 0.6672 0.7052 0.6256
Validation:	 Best_BACC: 0.6654 0.6672 0.7052 0.6256
Validation:	 Loss: 0.6181
Pretraining:	Epoch 6/200
----------
training:	Epoch: [6][1/204]	Loss 0.6195 (0.6195)	
training:	Epoch: [6][2/204]	Loss 0.6769 (0.6482)	
training:	Epoch: [6][3/204]	Loss 0.6783 (0.6583)	
training:	Epoch: [6][4/204]	Loss 0.6438 (0.6546)	
training:	Epoch: [6][5/204]	Loss 0.5894 (0.6416)	
training:	Epoch: [6][6/204]	Loss 0.5240 (0.6220)	
training:	Epoch: [6][7/204]	Loss 0.6136 (0.6208)	
training:	Epoch: [6][8/204]	Loss 0.6004 (0.6182)	
training:	Epoch: [6][9/204]	Loss 0.5670 (0.6125)	
training:	Epoch: [6][10/204]	Loss 0.5450 (0.6058)	
training:	Epoch: [6][11/204]	Loss 0.7079 (0.6151)	
training:	Epoch: [6][12/204]	Loss 0.6202 (0.6155)	
training:	Epoch: [6][13/204]	Loss 0.5545 (0.6108)	
training:	Epoch: [6][14/204]	Loss 0.5149 (0.6040)	
training:	Epoch: [6][15/204]	Loss 0.6494 (0.6070)	
training:	Epoch: [6][16/204]	Loss 0.6175 (0.6076)	
training:	Epoch: [6][17/204]	Loss 0.5847 (0.6063)	
training:	Epoch: [6][18/204]	Loss 0.6109 (0.6066)	
training:	Epoch: [6][19/204]	Loss 0.6034 (0.6064)	
training:	Epoch: [6][20/204]	Loss 0.6060 (0.6064)	
training:	Epoch: [6][21/204]	Loss 0.6081 (0.6064)	
training:	Epoch: [6][22/204]	Loss 0.6454 (0.6082)	
training:	Epoch: [6][23/204]	Loss 0.5828 (0.6071)	
training:	Epoch: [6][24/204]	Loss 0.5973 (0.6067)	
training:	Epoch: [6][25/204]	Loss 0.6685 (0.6092)	
training:	Epoch: [6][26/204]	Loss 0.5793 (0.6080)	
training:	Epoch: [6][27/204]	Loss 0.5676 (0.6065)	
training:	Epoch: [6][28/204]	Loss 0.6269 (0.6073)	
training:	Epoch: [6][29/204]	Loss 0.5975 (0.6069)	
training:	Epoch: [6][30/204]	Loss 0.6339 (0.6078)	
training:	Epoch: [6][31/204]	Loss 0.5218 (0.6050)	
training:	Epoch: [6][32/204]	Loss 0.5945 (0.6047)	
training:	Epoch: [6][33/204]	Loss 0.5813 (0.6040)	
training:	Epoch: [6][34/204]	Loss 0.6451 (0.6052)	
training:	Epoch: [6][35/204]	Loss 0.7015 (0.6080)	
training:	Epoch: [6][36/204]	Loss 0.5853 (0.6073)	
training:	Epoch: [6][37/204]	Loss 0.6796 (0.6093)	
training:	Epoch: [6][38/204]	Loss 0.6568 (0.6105)	
training:	Epoch: [6][39/204]	Loss 0.6989 (0.6128)	
training:	Epoch: [6][40/204]	Loss 0.6617 (0.6140)	
training:	Epoch: [6][41/204]	Loss 0.5018 (0.6113)	
training:	Epoch: [6][42/204]	Loss 0.5129 (0.6089)	
training:	Epoch: [6][43/204]	Loss 0.6062 (0.6089)	
training:	Epoch: [6][44/204]	Loss 0.5589 (0.6077)	
training:	Epoch: [6][45/204]	Loss 0.6266 (0.6082)	
training:	Epoch: [6][46/204]	Loss 0.5407 (0.6067)	
training:	Epoch: [6][47/204]	Loss 0.5916 (0.6064)	
training:	Epoch: [6][48/204]	Loss 0.6692 (0.6077)	
training:	Epoch: [6][49/204]	Loss 0.6951 (0.6095)	
training:	Epoch: [6][50/204]	Loss 0.7435 (0.6121)	
training:	Epoch: [6][51/204]	Loss 0.5089 (0.6101)	
training:	Epoch: [6][52/204]	Loss 0.6160 (0.6102)	
training:	Epoch: [6][53/204]	Loss 0.7466 (0.6128)	
training:	Epoch: [6][54/204]	Loss 0.5742 (0.6121)	
training:	Epoch: [6][55/204]	Loss 0.7040 (0.6138)	
training:	Epoch: [6][56/204]	Loss 0.6863 (0.6151)	
training:	Epoch: [6][57/204]	Loss 0.5995 (0.6148)	
training:	Epoch: [6][58/204]	Loss 0.6439 (0.6153)	
training:	Epoch: [6][59/204]	Loss 0.6157 (0.6153)	
training:	Epoch: [6][60/204]	Loss 0.5585 (0.6144)	
training:	Epoch: [6][61/204]	Loss 0.6316 (0.6146)	
training:	Epoch: [6][62/204]	Loss 0.5921 (0.6143)	
training:	Epoch: [6][63/204]	Loss 0.6972 (0.6156)	
training:	Epoch: [6][64/204]	Loss 0.6421 (0.6160)	
training:	Epoch: [6][65/204]	Loss 0.6188 (0.6160)	
training:	Epoch: [6][66/204]	Loss 0.6281 (0.6162)	
training:	Epoch: [6][67/204]	Loss 0.5951 (0.6159)	
training:	Epoch: [6][68/204]	Loss 0.6704 (0.6167)	
training:	Epoch: [6][69/204]	Loss 0.5550 (0.6158)	
training:	Epoch: [6][70/204]	Loss 0.6739 (0.6166)	
training:	Epoch: [6][71/204]	Loss 0.5654 (0.6159)	
training:	Epoch: [6][72/204]	Loss 0.5454 (0.6149)	
training:	Epoch: [6][73/204]	Loss 0.6408 (0.6153)	
training:	Epoch: [6][74/204]	Loss 0.6618 (0.6159)	
training:	Epoch: [6][75/204]	Loss 0.6766 (0.6167)	
training:	Epoch: [6][76/204]	Loss 0.6913 (0.6177)	
training:	Epoch: [6][77/204]	Loss 0.5999 (0.6175)	
training:	Epoch: [6][78/204]	Loss 0.5487 (0.6166)	
training:	Epoch: [6][79/204]	Loss 0.6820 (0.6174)	
training:	Epoch: [6][80/204]	Loss 0.5853 (0.6170)	
training:	Epoch: [6][81/204]	Loss 0.6252 (0.6171)	
training:	Epoch: [6][82/204]	Loss 0.5917 (0.6168)	
training:	Epoch: [6][83/204]	Loss 0.6748 (0.6175)	
training:	Epoch: [6][84/204]	Loss 0.6111 (0.6174)	
training:	Epoch: [6][85/204]	Loss 0.6618 (0.6180)	
training:	Epoch: [6][86/204]	Loss 0.6127 (0.6179)	
training:	Epoch: [6][87/204]	Loss 0.6378 (0.6181)	
training:	Epoch: [6][88/204]	Loss 0.4591 (0.6163)	
training:	Epoch: [6][89/204]	Loss 0.5394 (0.6155)	
training:	Epoch: [6][90/204]	Loss 0.6547 (0.6159)	
training:	Epoch: [6][91/204]	Loss 0.5733 (0.6154)	
training:	Epoch: [6][92/204]	Loss 0.5598 (0.6148)	
training:	Epoch: [6][93/204]	Loss 0.5652 (0.6143)	
training:	Epoch: [6][94/204]	Loss 0.6281 (0.6144)	
training:	Epoch: [6][95/204]	Loss 0.5970 (0.6143)	
training:	Epoch: [6][96/204]	Loss 0.7145 (0.6153)	
training:	Epoch: [6][97/204]	Loss 0.6772 (0.6159)	
training:	Epoch: [6][98/204]	Loss 0.5665 (0.6154)	
training:	Epoch: [6][99/204]	Loss 0.5860 (0.6151)	
training:	Epoch: [6][100/204]	Loss 0.5854 (0.6148)	
training:	Epoch: [6][101/204]	Loss 0.5727 (0.6144)	
training:	Epoch: [6][102/204]	Loss 0.5279 (0.6136)	
training:	Epoch: [6][103/204]	Loss 0.7110 (0.6145)	
training:	Epoch: [6][104/204]	Loss 0.5754 (0.6141)	
training:	Epoch: [6][105/204]	Loss 0.6486 (0.6145)	
training:	Epoch: [6][106/204]	Loss 0.6617 (0.6149)	
training:	Epoch: [6][107/204]	Loss 0.5338 (0.6142)	
training:	Epoch: [6][108/204]	Loss 0.5879 (0.6139)	
training:	Epoch: [6][109/204]	Loss 0.6258 (0.6140)	
training:	Epoch: [6][110/204]	Loss 0.6387 (0.6142)	
training:	Epoch: [6][111/204]	Loss 0.5648 (0.6138)	
training:	Epoch: [6][112/204]	Loss 0.5693 (0.6134)	
training:	Epoch: [6][113/204]	Loss 0.6010 (0.6133)	
training:	Epoch: [6][114/204]	Loss 0.5849 (0.6130)	
training:	Epoch: [6][115/204]	Loss 0.5901 (0.6128)	
training:	Epoch: [6][116/204]	Loss 0.5187 (0.6120)	
training:	Epoch: [6][117/204]	Loss 0.5858 (0.6118)	
training:	Epoch: [6][118/204]	Loss 0.5867 (0.6116)	
training:	Epoch: [6][119/204]	Loss 0.5840 (0.6114)	
training:	Epoch: [6][120/204]	Loss 0.6823 (0.6120)	
training:	Epoch: [6][121/204]	Loss 0.6161 (0.6120)	
training:	Epoch: [6][122/204]	Loss 0.7204 (0.6129)	
training:	Epoch: [6][123/204]	Loss 0.6145 (0.6129)	
training:	Epoch: [6][124/204]	Loss 0.6408 (0.6131)	
training:	Epoch: [6][125/204]	Loss 0.6809 (0.6137)	
training:	Epoch: [6][126/204]	Loss 0.6395 (0.6139)	
training:	Epoch: [6][127/204]	Loss 0.7070 (0.6146)	
training:	Epoch: [6][128/204]	Loss 0.5820 (0.6143)	
training:	Epoch: [6][129/204]	Loss 0.6007 (0.6142)	
training:	Epoch: [6][130/204]	Loss 0.6358 (0.6144)	
training:	Epoch: [6][131/204]	Loss 0.6006 (0.6143)	
training:	Epoch: [6][132/204]	Loss 0.5338 (0.6137)	
training:	Epoch: [6][133/204]	Loss 0.5939 (0.6135)	
training:	Epoch: [6][134/204]	Loss 0.5758 (0.6133)	
training:	Epoch: [6][135/204]	Loss 0.6609 (0.6136)	
training:	Epoch: [6][136/204]	Loss 0.5291 (0.6130)	
training:	Epoch: [6][137/204]	Loss 0.5692 (0.6127)	
training:	Epoch: [6][138/204]	Loss 0.6831 (0.6132)	
training:	Epoch: [6][139/204]	Loss 0.6045 (0.6131)	
training:	Epoch: [6][140/204]	Loss 0.5797 (0.6129)	
training:	Epoch: [6][141/204]	Loss 0.6413 (0.6131)	
training:	Epoch: [6][142/204]	Loss 0.6816 (0.6136)	
training:	Epoch: [6][143/204]	Loss 0.6626 (0.6139)	
training:	Epoch: [6][144/204]	Loss 0.5427 (0.6134)	
training:	Epoch: [6][145/204]	Loss 0.5646 (0.6131)	
training:	Epoch: [6][146/204]	Loss 0.4945 (0.6123)	
training:	Epoch: [6][147/204]	Loss 0.6984 (0.6128)	
training:	Epoch: [6][148/204]	Loss 0.6789 (0.6133)	
training:	Epoch: [6][149/204]	Loss 0.6979 (0.6139)	
training:	Epoch: [6][150/204]	Loss 0.6874 (0.6144)	
training:	Epoch: [6][151/204]	Loss 0.6180 (0.6144)	
training:	Epoch: [6][152/204]	Loss 0.5904 (0.6142)	
training:	Epoch: [6][153/204]	Loss 0.6258 (0.6143)	
training:	Epoch: [6][154/204]	Loss 0.6251 (0.6144)	
training:	Epoch: [6][155/204]	Loss 0.6036 (0.6143)	
training:	Epoch: [6][156/204]	Loss 0.5774 (0.6141)	
training:	Epoch: [6][157/204]	Loss 0.6016 (0.6140)	
training:	Epoch: [6][158/204]	Loss 0.5773 (0.6137)	
training:	Epoch: [6][159/204]	Loss 0.6166 (0.6138)	
training:	Epoch: [6][160/204]	Loss 0.6406 (0.6139)	
training:	Epoch: [6][161/204]	Loss 0.6356 (0.6141)	
training:	Epoch: [6][162/204]	Loss 0.5882 (0.6139)	
training:	Epoch: [6][163/204]	Loss 0.7172 (0.6145)	
training:	Epoch: [6][164/204]	Loss 0.4974 (0.6138)	
training:	Epoch: [6][165/204]	Loss 0.5789 (0.6136)	
training:	Epoch: [6][166/204]	Loss 0.5978 (0.6135)	
training:	Epoch: [6][167/204]	Loss 0.5182 (0.6129)	
training:	Epoch: [6][168/204]	Loss 0.5055 (0.6123)	
training:	Epoch: [6][169/204]	Loss 0.5737 (0.6121)	
training:	Epoch: [6][170/204]	Loss 0.6101 (0.6121)	
training:	Epoch: [6][171/204]	Loss 0.6358 (0.6122)	
training:	Epoch: [6][172/204]	Loss 0.5943 (0.6121)	
training:	Epoch: [6][173/204]	Loss 0.6206 (0.6122)	
training:	Epoch: [6][174/204]	Loss 0.6601 (0.6124)	
training:	Epoch: [6][175/204]	Loss 0.6800 (0.6128)	
training:	Epoch: [6][176/204]	Loss 0.6872 (0.6132)	
training:	Epoch: [6][177/204]	Loss 0.6101 (0.6132)	
training:	Epoch: [6][178/204]	Loss 0.5133 (0.6127)	
training:	Epoch: [6][179/204]	Loss 0.6626 (0.6129)	
training:	Epoch: [6][180/204]	Loss 0.5614 (0.6127)	
training:	Epoch: [6][181/204]	Loss 0.4580 (0.6118)	
training:	Epoch: [6][182/204]	Loss 0.5261 (0.6113)	
training:	Epoch: [6][183/204]	Loss 0.5791 (0.6111)	
training:	Epoch: [6][184/204]	Loss 0.6133 (0.6112)	
training:	Epoch: [6][185/204]	Loss 0.6412 (0.6113)	
training:	Epoch: [6][186/204]	Loss 0.6642 (0.6116)	
training:	Epoch: [6][187/204]	Loss 0.6574 (0.6119)	
training:	Epoch: [6][188/204]	Loss 0.5778 (0.6117)	
training:	Epoch: [6][189/204]	Loss 0.6322 (0.6118)	
training:	Epoch: [6][190/204]	Loss 0.5681 (0.6116)	
training:	Epoch: [6][191/204]	Loss 0.5635 (0.6113)	
training:	Epoch: [6][192/204]	Loss 0.6440 (0.6115)	
training:	Epoch: [6][193/204]	Loss 0.6375 (0.6116)	
training:	Epoch: [6][194/204]	Loss 0.5512 (0.6113)	
training:	Epoch: [6][195/204]	Loss 0.5765 (0.6111)	
training:	Epoch: [6][196/204]	Loss 0.5805 (0.6110)	
training:	Epoch: [6][197/204]	Loss 0.5399 (0.6106)	
training:	Epoch: [6][198/204]	Loss 0.5823 (0.6105)	
training:	Epoch: [6][199/204]	Loss 0.6196 (0.6105)	
training:	Epoch: [6][200/204]	Loss 0.5973 (0.6104)	
training:	Epoch: [6][201/204]	Loss 0.5875 (0.6103)	
training:	Epoch: [6][202/204]	Loss 0.6013 (0.6103)	
training:	Epoch: [6][203/204]	Loss 0.6264 (0.6104)	
training:	Epoch: [6][204/204]	Loss 0.6698 (0.6106)	
Training:	 Loss: 0.6097

Training:	 ACC: 0.6815 0.6825 0.7061 0.6569
Validation:	 ACC: 0.6750 0.6763 0.7042 0.6457
Validation:	 Best_BACC: 0.6750 0.6763 0.7042 0.6457
Validation:	 Loss: 0.6079
Pretraining:	Epoch 7/200
----------
training:	Epoch: [7][1/204]	Loss 0.5219 (0.5219)	
training:	Epoch: [7][2/204]	Loss 0.6453 (0.5836)	
training:	Epoch: [7][3/204]	Loss 0.6081 (0.5918)	
training:	Epoch: [7][4/204]	Loss 0.6681 (0.6109)	
training:	Epoch: [7][5/204]	Loss 0.5719 (0.6031)	
training:	Epoch: [7][6/204]	Loss 0.6079 (0.6039)	
training:	Epoch: [7][7/204]	Loss 0.6646 (0.6126)	
training:	Epoch: [7][8/204]	Loss 0.5313 (0.6024)	
training:	Epoch: [7][9/204]	Loss 0.5981 (0.6019)	
training:	Epoch: [7][10/204]	Loss 0.4970 (0.5914)	
training:	Epoch: [7][11/204]	Loss 0.5858 (0.5909)	
training:	Epoch: [7][12/204]	Loss 0.6519 (0.5960)	
training:	Epoch: [7][13/204]	Loss 0.6300 (0.5986)	
training:	Epoch: [7][14/204]	Loss 0.5101 (0.5923)	
training:	Epoch: [7][15/204]	Loss 0.5998 (0.5928)	
training:	Epoch: [7][16/204]	Loss 0.7001 (0.5995)	
training:	Epoch: [7][17/204]	Loss 0.6148 (0.6004)	
training:	Epoch: [7][18/204]	Loss 0.5775 (0.5991)	
training:	Epoch: [7][19/204]	Loss 0.5527 (0.5967)	
training:	Epoch: [7][20/204]	Loss 0.5105 (0.5924)	
training:	Epoch: [7][21/204]	Loss 0.5587 (0.5908)	
training:	Epoch: [7][22/204]	Loss 0.7016 (0.5958)	
training:	Epoch: [7][23/204]	Loss 0.6362 (0.5976)	
training:	Epoch: [7][24/204]	Loss 0.6798 (0.6010)	
training:	Epoch: [7][25/204]	Loss 0.5053 (0.5972)	
training:	Epoch: [7][26/204]	Loss 0.6252 (0.5982)	
training:	Epoch: [7][27/204]	Loss 0.5289 (0.5957)	
training:	Epoch: [7][28/204]	Loss 0.4962 (0.5921)	
training:	Epoch: [7][29/204]	Loss 0.6782 (0.5951)	
training:	Epoch: [7][30/204]	Loss 0.6181 (0.5959)	
training:	Epoch: [7][31/204]	Loss 0.6172 (0.5965)	
training:	Epoch: [7][32/204]	Loss 0.6971 (0.5997)	
training:	Epoch: [7][33/204]	Loss 0.5946 (0.5995)	
training:	Epoch: [7][34/204]	Loss 0.6494 (0.6010)	
training:	Epoch: [7][35/204]	Loss 0.6006 (0.6010)	
training:	Epoch: [7][36/204]	Loss 0.6069 (0.6012)	
training:	Epoch: [7][37/204]	Loss 0.5640 (0.6001)	
training:	Epoch: [7][38/204]	Loss 0.5900 (0.5999)	
training:	Epoch: [7][39/204]	Loss 0.5715 (0.5992)	
training:	Epoch: [7][40/204]	Loss 0.7062 (0.6018)	
training:	Epoch: [7][41/204]	Loss 0.6436 (0.6028)	
training:	Epoch: [7][42/204]	Loss 0.6379 (0.6037)	
training:	Epoch: [7][43/204]	Loss 0.5198 (0.6017)	
training:	Epoch: [7][44/204]	Loss 0.5583 (0.6007)	
training:	Epoch: [7][45/204]	Loss 0.6651 (0.6022)	
training:	Epoch: [7][46/204]	Loss 0.6740 (0.6037)	
training:	Epoch: [7][47/204]	Loss 0.6482 (0.6047)	
training:	Epoch: [7][48/204]	Loss 0.6514 (0.6057)	
training:	Epoch: [7][49/204]	Loss 0.5887 (0.6053)	
training:	Epoch: [7][50/204]	Loss 0.6093 (0.6054)	
training:	Epoch: [7][51/204]	Loss 0.5567 (0.6044)	
training:	Epoch: [7][52/204]	Loss 0.6175 (0.6047)	
training:	Epoch: [7][53/204]	Loss 0.6687 (0.6059)	
training:	Epoch: [7][54/204]	Loss 0.6469 (0.6067)	
training:	Epoch: [7][55/204]	Loss 0.5800 (0.6062)	
training:	Epoch: [7][56/204]	Loss 0.5917 (0.6059)	
training:	Epoch: [7][57/204]	Loss 0.5343 (0.6047)	
training:	Epoch: [7][58/204]	Loss 0.6443 (0.6053)	
training:	Epoch: [7][59/204]	Loss 0.5781 (0.6049)	
training:	Epoch: [7][60/204]	Loss 0.5888 (0.6046)	
training:	Epoch: [7][61/204]	Loss 0.6034 (0.6046)	
training:	Epoch: [7][62/204]	Loss 0.5242 (0.6033)	
training:	Epoch: [7][63/204]	Loss 0.6636 (0.6043)	
training:	Epoch: [7][64/204]	Loss 0.5726 (0.6038)	
training:	Epoch: [7][65/204]	Loss 0.5883 (0.6035)	
training:	Epoch: [7][66/204]	Loss 0.5778 (0.6031)	
training:	Epoch: [7][67/204]	Loss 0.5458 (0.6023)	
training:	Epoch: [7][68/204]	Loss 0.5502 (0.6015)	
training:	Epoch: [7][69/204]	Loss 0.6537 (0.6023)	
training:	Epoch: [7][70/204]	Loss 0.6150 (0.6024)	
training:	Epoch: [7][71/204]	Loss 0.6249 (0.6028)	
training:	Epoch: [7][72/204]	Loss 0.4259 (0.6003)	
training:	Epoch: [7][73/204]	Loss 0.5779 (0.6000)	
training:	Epoch: [7][74/204]	Loss 0.5695 (0.5996)	
training:	Epoch: [7][75/204]	Loss 0.5903 (0.5995)	
training:	Epoch: [7][76/204]	Loss 0.4892 (0.5980)	
training:	Epoch: [7][77/204]	Loss 0.5636 (0.5976)	
training:	Epoch: [7][78/204]	Loss 0.5819 (0.5974)	
training:	Epoch: [7][79/204]	Loss 0.5807 (0.5972)	
training:	Epoch: [7][80/204]	Loss 0.5740 (0.5969)	
training:	Epoch: [7][81/204]	Loss 0.5558 (0.5964)	
training:	Epoch: [7][82/204]	Loss 0.5381 (0.5956)	
training:	Epoch: [7][83/204]	Loss 0.6336 (0.5961)	
training:	Epoch: [7][84/204]	Loss 0.5379 (0.5954)	
training:	Epoch: [7][85/204]	Loss 0.6215 (0.5957)	
training:	Epoch: [7][86/204]	Loss 0.4912 (0.5945)	
training:	Epoch: [7][87/204]	Loss 0.7020 (0.5957)	
training:	Epoch: [7][88/204]	Loss 0.6132 (0.5959)	
training:	Epoch: [7][89/204]	Loss 0.6563 (0.5966)	
training:	Epoch: [7][90/204]	Loss 0.5370 (0.5960)	
training:	Epoch: [7][91/204]	Loss 0.6777 (0.5969)	
training:	Epoch: [7][92/204]	Loss 0.6834 (0.5978)	
training:	Epoch: [7][93/204]	Loss 0.6490 (0.5983)	
training:	Epoch: [7][94/204]	Loss 0.5932 (0.5983)	
training:	Epoch: [7][95/204]	Loss 0.5864 (0.5982)	
training:	Epoch: [7][96/204]	Loss 0.6234 (0.5984)	
training:	Epoch: [7][97/204]	Loss 0.5666 (0.5981)	
training:	Epoch: [7][98/204]	Loss 0.5115 (0.5972)	
training:	Epoch: [7][99/204]	Loss 0.6817 (0.5981)	
training:	Epoch: [7][100/204]	Loss 0.6178 (0.5983)	
training:	Epoch: [7][101/204]	Loss 0.5777 (0.5981)	
training:	Epoch: [7][102/204]	Loss 0.6297 (0.5984)	
training:	Epoch: [7][103/204]	Loss 0.5859 (0.5983)	
training:	Epoch: [7][104/204]	Loss 0.5942 (0.5982)	
training:	Epoch: [7][105/204]	Loss 0.6608 (0.5988)	
training:	Epoch: [7][106/204]	Loss 0.6567 (0.5994)	
training:	Epoch: [7][107/204]	Loss 0.6864 (0.6002)	
training:	Epoch: [7][108/204]	Loss 0.5860 (0.6000)	
training:	Epoch: [7][109/204]	Loss 0.6727 (0.6007)	
training:	Epoch: [7][110/204]	Loss 0.6310 (0.6010)	
training:	Epoch: [7][111/204]	Loss 0.6396 (0.6013)	
training:	Epoch: [7][112/204]	Loss 0.6022 (0.6013)	
training:	Epoch: [7][113/204]	Loss 0.5322 (0.6007)	
training:	Epoch: [7][114/204]	Loss 0.5806 (0.6005)	
training:	Epoch: [7][115/204]	Loss 0.4976 (0.5997)	
training:	Epoch: [7][116/204]	Loss 0.6237 (0.5999)	
training:	Epoch: [7][117/204]	Loss 0.6238 (0.6001)	
training:	Epoch: [7][118/204]	Loss 0.6061 (0.6001)	
training:	Epoch: [7][119/204]	Loss 0.6111 (0.6002)	
training:	Epoch: [7][120/204]	Loss 0.5912 (0.6001)	
training:	Epoch: [7][121/204]	Loss 0.5648 (0.5998)	
training:	Epoch: [7][122/204]	Loss 0.5859 (0.5997)	
training:	Epoch: [7][123/204]	Loss 0.4811 (0.5988)	
training:	Epoch: [7][124/204]	Loss 0.6196 (0.5989)	
training:	Epoch: [7][125/204]	Loss 0.6075 (0.5990)	
training:	Epoch: [7][126/204]	Loss 0.5462 (0.5986)	
training:	Epoch: [7][127/204]	Loss 0.6589 (0.5991)	
training:	Epoch: [7][128/204]	Loss 0.6915 (0.5998)	
training:	Epoch: [7][129/204]	Loss 0.6312 (0.6000)	
training:	Epoch: [7][130/204]	Loss 0.6246 (0.6002)	
training:	Epoch: [7][131/204]	Loss 0.5501 (0.5998)	
training:	Epoch: [7][132/204]	Loss 0.6341 (0.6001)	
training:	Epoch: [7][133/204]	Loss 0.6053 (0.6001)	
training:	Epoch: [7][134/204]	Loss 0.5876 (0.6000)	
training:	Epoch: [7][135/204]	Loss 0.5532 (0.5997)	
training:	Epoch: [7][136/204]	Loss 0.5666 (0.5994)	
training:	Epoch: [7][137/204]	Loss 0.7770 (0.6007)	
training:	Epoch: [7][138/204]	Loss 0.5171 (0.6001)	
training:	Epoch: [7][139/204]	Loss 0.7077 (0.6009)	
training:	Epoch: [7][140/204]	Loss 0.7252 (0.6018)	
training:	Epoch: [7][141/204]	Loss 0.5541 (0.6015)	
training:	Epoch: [7][142/204]	Loss 0.6543 (0.6018)	
training:	Epoch: [7][143/204]	Loss 0.6411 (0.6021)	
training:	Epoch: [7][144/204]	Loss 0.6042 (0.6021)	
training:	Epoch: [7][145/204]	Loss 0.5721 (0.6019)	
training:	Epoch: [7][146/204]	Loss 0.5209 (0.6014)	
training:	Epoch: [7][147/204]	Loss 0.5563 (0.6010)	
training:	Epoch: [7][148/204]	Loss 0.7270 (0.6019)	
training:	Epoch: [7][149/204]	Loss 0.5397 (0.6015)	
training:	Epoch: [7][150/204]	Loss 0.6260 (0.6016)	
training:	Epoch: [7][151/204]	Loss 0.6378 (0.6019)	
training:	Epoch: [7][152/204]	Loss 0.6654 (0.6023)	
training:	Epoch: [7][153/204]	Loss 0.5765 (0.6021)	
training:	Epoch: [7][154/204]	Loss 0.5442 (0.6018)	
training:	Epoch: [7][155/204]	Loss 0.7207 (0.6025)	
training:	Epoch: [7][156/204]	Loss 0.5819 (0.6024)	
training:	Epoch: [7][157/204]	Loss 0.5795 (0.6022)	
training:	Epoch: [7][158/204]	Loss 0.5714 (0.6020)	
training:	Epoch: [7][159/204]	Loss 0.5970 (0.6020)	
training:	Epoch: [7][160/204]	Loss 0.6007 (0.6020)	
training:	Epoch: [7][161/204]	Loss 0.6747 (0.6025)	
training:	Epoch: [7][162/204]	Loss 0.5653 (0.6022)	
training:	Epoch: [7][163/204]	Loss 0.4801 (0.6015)	
training:	Epoch: [7][164/204]	Loss 0.6421 (0.6017)	
training:	Epoch: [7][165/204]	Loss 0.5885 (0.6017)	
training:	Epoch: [7][166/204]	Loss 0.6320 (0.6018)	
training:	Epoch: [7][167/204]	Loss 0.5951 (0.6018)	
training:	Epoch: [7][168/204]	Loss 0.5776 (0.6016)	
training:	Epoch: [7][169/204]	Loss 0.7757 (0.6027)	
training:	Epoch: [7][170/204]	Loss 0.5104 (0.6021)	
training:	Epoch: [7][171/204]	Loss 0.4928 (0.6015)	
training:	Epoch: [7][172/204]	Loss 0.6345 (0.6017)	
training:	Epoch: [7][173/204]	Loss 0.6808 (0.6021)	
training:	Epoch: [7][174/204]	Loss 0.5577 (0.6019)	
training:	Epoch: [7][175/204]	Loss 0.6352 (0.6021)	
training:	Epoch: [7][176/204]	Loss 0.4796 (0.6014)	
training:	Epoch: [7][177/204]	Loss 0.5734 (0.6012)	
training:	Epoch: [7][178/204]	Loss 0.5822 (0.6011)	
training:	Epoch: [7][179/204]	Loss 0.6524 (0.6014)	
training:	Epoch: [7][180/204]	Loss 0.4890 (0.6008)	
training:	Epoch: [7][181/204]	Loss 0.5321 (0.6004)	
training:	Epoch: [7][182/204]	Loss 0.6480 (0.6007)	
training:	Epoch: [7][183/204]	Loss 0.6407 (0.6009)	
training:	Epoch: [7][184/204]	Loss 0.6436 (0.6011)	
training:	Epoch: [7][185/204]	Loss 0.7151 (0.6017)	
training:	Epoch: [7][186/204]	Loss 0.7239 (0.6024)	
training:	Epoch: [7][187/204]	Loss 0.5240 (0.6020)	
training:	Epoch: [7][188/204]	Loss 0.6323 (0.6021)	
training:	Epoch: [7][189/204]	Loss 0.5298 (0.6017)	
training:	Epoch: [7][190/204]	Loss 0.5888 (0.6017)	
training:	Epoch: [7][191/204]	Loss 0.5429 (0.6014)	
training:	Epoch: [7][192/204]	Loss 0.6472 (0.6016)	
training:	Epoch: [7][193/204]	Loss 0.5800 (0.6015)	
training:	Epoch: [7][194/204]	Loss 0.5480 (0.6012)	
training:	Epoch: [7][195/204]	Loss 0.5833 (0.6011)	
training:	Epoch: [7][196/204]	Loss 0.5627 (0.6009)	
training:	Epoch: [7][197/204]	Loss 0.6364 (0.6011)	
training:	Epoch: [7][198/204]	Loss 0.6026 (0.6011)	
training:	Epoch: [7][199/204]	Loss 0.5361 (0.6008)	
training:	Epoch: [7][200/204]	Loss 0.5263 (0.6004)	
training:	Epoch: [7][201/204]	Loss 0.5599 (0.6002)	
training:	Epoch: [7][202/204]	Loss 0.5854 (0.6001)	
training:	Epoch: [7][203/204]	Loss 0.6049 (0.6002)	
training:	Epoch: [7][204/204]	Loss 0.6356 (0.6003)	
Training:	 Loss: 0.5994

Training:	 ACC: 0.6878 0.6897 0.7325 0.6432
Validation:	 ACC: 0.6845 0.6870 0.7400 0.6289
Validation:	 Best_BACC: 0.6845 0.6870 0.7400 0.6289
Validation:	 Loss: 0.5998
Pretraining:	Epoch 8/200
----------
training:	Epoch: [8][1/204]	Loss 0.5154 (0.5154)	
training:	Epoch: [8][2/204]	Loss 0.5563 (0.5358)	
training:	Epoch: [8][3/204]	Loss 0.6953 (0.5890)	
training:	Epoch: [8][4/204]	Loss 0.6009 (0.5919)	
training:	Epoch: [8][5/204]	Loss 0.6031 (0.5942)	
training:	Epoch: [8][6/204]	Loss 0.5512 (0.5870)	
training:	Epoch: [8][7/204]	Loss 0.5969 (0.5884)	
training:	Epoch: [8][8/204]	Loss 0.5479 (0.5834)	
training:	Epoch: [8][9/204]	Loss 0.6452 (0.5902)	
training:	Epoch: [8][10/204]	Loss 0.6044 (0.5916)	
training:	Epoch: [8][11/204]	Loss 0.4861 (0.5820)	
training:	Epoch: [8][12/204]	Loss 0.6160 (0.5849)	
training:	Epoch: [8][13/204]	Loss 0.5597 (0.5829)	
training:	Epoch: [8][14/204]	Loss 0.6544 (0.5880)	
training:	Epoch: [8][15/204]	Loss 0.6168 (0.5900)	
training:	Epoch: [8][16/204]	Loss 0.5970 (0.5904)	
training:	Epoch: [8][17/204]	Loss 0.6541 (0.5942)	
training:	Epoch: [8][18/204]	Loss 0.6108 (0.5951)	
training:	Epoch: [8][19/204]	Loss 0.6847 (0.5998)	
training:	Epoch: [8][20/204]	Loss 0.6137 (0.6005)	
training:	Epoch: [8][21/204]	Loss 0.6735 (0.6040)	
training:	Epoch: [8][22/204]	Loss 0.4841 (0.5985)	
training:	Epoch: [8][23/204]	Loss 0.6301 (0.5999)	
training:	Epoch: [8][24/204]	Loss 0.4452 (0.5934)	
training:	Epoch: [8][25/204]	Loss 0.5824 (0.5930)	
training:	Epoch: [8][26/204]	Loss 0.7481 (0.5990)	
training:	Epoch: [8][27/204]	Loss 0.5743 (0.5981)	
training:	Epoch: [8][28/204]	Loss 0.5910 (0.5978)	
training:	Epoch: [8][29/204]	Loss 0.5300 (0.5955)	
training:	Epoch: [8][30/204]	Loss 0.5989 (0.5956)	
training:	Epoch: [8][31/204]	Loss 0.6651 (0.5978)	
training:	Epoch: [8][32/204]	Loss 0.5926 (0.5977)	
training:	Epoch: [8][33/204]	Loss 0.6556 (0.5994)	
training:	Epoch: [8][34/204]	Loss 0.5860 (0.5990)	
training:	Epoch: [8][35/204]	Loss 0.5295 (0.5970)	
training:	Epoch: [8][36/204]	Loss 0.6044 (0.5972)	
training:	Epoch: [8][37/204]	Loss 0.6125 (0.5976)	
training:	Epoch: [8][38/204]	Loss 0.5778 (0.5971)	
training:	Epoch: [8][39/204]	Loss 0.6097 (0.5974)	
training:	Epoch: [8][40/204]	Loss 0.7190 (0.6005)	
training:	Epoch: [8][41/204]	Loss 0.5791 (0.6000)	
training:	Epoch: [8][42/204]	Loss 0.6160 (0.6003)	
training:	Epoch: [8][43/204]	Loss 0.6023 (0.6004)	
training:	Epoch: [8][44/204]	Loss 0.6536 (0.6016)	
training:	Epoch: [8][45/204]	Loss 0.6460 (0.6026)	
training:	Epoch: [8][46/204]	Loss 0.6193 (0.6030)	
training:	Epoch: [8][47/204]	Loss 0.5999 (0.6029)	
training:	Epoch: [8][48/204]	Loss 0.5746 (0.6023)	
training:	Epoch: [8][49/204]	Loss 0.4938 (0.6001)	
training:	Epoch: [8][50/204]	Loss 0.5991 (0.6001)	
training:	Epoch: [8][51/204]	Loss 0.5608 (0.5993)	
training:	Epoch: [8][52/204]	Loss 0.6784 (0.6008)	
training:	Epoch: [8][53/204]	Loss 0.6678 (0.6021)	
training:	Epoch: [8][54/204]	Loss 0.5015 (0.6002)	
training:	Epoch: [8][55/204]	Loss 0.5743 (0.5997)	
training:	Epoch: [8][56/204]	Loss 0.6350 (0.6004)	
training:	Epoch: [8][57/204]	Loss 0.5101 (0.5988)	
training:	Epoch: [8][58/204]	Loss 0.5639 (0.5982)	
training:	Epoch: [8][59/204]	Loss 0.5942 (0.5981)	
training:	Epoch: [8][60/204]	Loss 0.6517 (0.5990)	
training:	Epoch: [8][61/204]	Loss 0.5728 (0.5986)	
training:	Epoch: [8][62/204]	Loss 0.5379 (0.5976)	
training:	Epoch: [8][63/204]	Loss 0.5817 (0.5974)	
training:	Epoch: [8][64/204]	Loss 0.6009 (0.5974)	
training:	Epoch: [8][65/204]	Loss 0.5357 (0.5965)	
training:	Epoch: [8][66/204]	Loss 0.5423 (0.5956)	
training:	Epoch: [8][67/204]	Loss 0.5775 (0.5954)	
training:	Epoch: [8][68/204]	Loss 0.6010 (0.5955)	
training:	Epoch: [8][69/204]	Loss 0.5015 (0.5941)	
training:	Epoch: [8][70/204]	Loss 0.6178 (0.5944)	
training:	Epoch: [8][71/204]	Loss 0.5805 (0.5942)	
training:	Epoch: [8][72/204]	Loss 0.4681 (0.5925)	
training:	Epoch: [8][73/204]	Loss 0.5537 (0.5920)	
training:	Epoch: [8][74/204]	Loss 0.5824 (0.5918)	
training:	Epoch: [8][75/204]	Loss 0.5793 (0.5917)	
training:	Epoch: [8][76/204]	Loss 0.5574 (0.5912)	
training:	Epoch: [8][77/204]	Loss 0.6748 (0.5923)	
training:	Epoch: [8][78/204]	Loss 0.6029 (0.5924)	
training:	Epoch: [8][79/204]	Loss 0.5890 (0.5924)	
training:	Epoch: [8][80/204]	Loss 0.5425 (0.5918)	
training:	Epoch: [8][81/204]	Loss 0.5947 (0.5918)	
training:	Epoch: [8][82/204]	Loss 0.5138 (0.5908)	
training:	Epoch: [8][83/204]	Loss 0.6507 (0.5916)	
training:	Epoch: [8][84/204]	Loss 0.5656 (0.5913)	
training:	Epoch: [8][85/204]	Loss 0.7660 (0.5933)	
training:	Epoch: [8][86/204]	Loss 0.5841 (0.5932)	
training:	Epoch: [8][87/204]	Loss 0.6301 (0.5936)	
training:	Epoch: [8][88/204]	Loss 0.6783 (0.5946)	
training:	Epoch: [8][89/204]	Loss 0.6108 (0.5948)	
training:	Epoch: [8][90/204]	Loss 0.6337 (0.5952)	
training:	Epoch: [8][91/204]	Loss 0.5854 (0.5951)	
training:	Epoch: [8][92/204]	Loss 0.7072 (0.5963)	
training:	Epoch: [8][93/204]	Loss 0.5258 (0.5956)	
training:	Epoch: [8][94/204]	Loss 0.6228 (0.5958)	
training:	Epoch: [8][95/204]	Loss 0.6120 (0.5960)	
training:	Epoch: [8][96/204]	Loss 0.6357 (0.5964)	
training:	Epoch: [8][97/204]	Loss 0.5540 (0.5960)	
training:	Epoch: [8][98/204]	Loss 0.6148 (0.5962)	
training:	Epoch: [8][99/204]	Loss 0.5941 (0.5962)	
training:	Epoch: [8][100/204]	Loss 0.6360 (0.5966)	
training:	Epoch: [8][101/204]	Loss 0.5828 (0.5964)	
training:	Epoch: [8][102/204]	Loss 0.5562 (0.5960)	
training:	Epoch: [8][103/204]	Loss 0.5082 (0.5952)	
training:	Epoch: [8][104/204]	Loss 0.5877 (0.5951)	
training:	Epoch: [8][105/204]	Loss 0.4704 (0.5939)	
training:	Epoch: [8][106/204]	Loss 0.5659 (0.5937)	
training:	Epoch: [8][107/204]	Loss 0.6894 (0.5945)	
training:	Epoch: [8][108/204]	Loss 0.5424 (0.5941)	
training:	Epoch: [8][109/204]	Loss 0.5656 (0.5938)	
training:	Epoch: [8][110/204]	Loss 0.6170 (0.5940)	
training:	Epoch: [8][111/204]	Loss 0.5526 (0.5936)	
training:	Epoch: [8][112/204]	Loss 0.5991 (0.5937)	
training:	Epoch: [8][113/204]	Loss 0.4928 (0.5928)	
training:	Epoch: [8][114/204]	Loss 0.6962 (0.5937)	
training:	Epoch: [8][115/204]	Loss 0.6063 (0.5938)	
training:	Epoch: [8][116/204]	Loss 0.5734 (0.5936)	
training:	Epoch: [8][117/204]	Loss 0.4839 (0.5927)	
training:	Epoch: [8][118/204]	Loss 0.6948 (0.5936)	
training:	Epoch: [8][119/204]	Loss 0.5516 (0.5932)	
training:	Epoch: [8][120/204]	Loss 0.5910 (0.5932)	
training:	Epoch: [8][121/204]	Loss 0.5637 (0.5930)	
training:	Epoch: [8][122/204]	Loss 0.5246 (0.5924)	
training:	Epoch: [8][123/204]	Loss 0.5645 (0.5922)	
training:	Epoch: [8][124/204]	Loss 0.6139 (0.5923)	
training:	Epoch: [8][125/204]	Loss 0.6029 (0.5924)	
training:	Epoch: [8][126/204]	Loss 0.5937 (0.5924)	
training:	Epoch: [8][127/204]	Loss 0.5741 (0.5923)	
training:	Epoch: [8][128/204]	Loss 0.5566 (0.5920)	
training:	Epoch: [8][129/204]	Loss 0.5489 (0.5917)	
training:	Epoch: [8][130/204]	Loss 0.6250 (0.5919)	
training:	Epoch: [8][131/204]	Loss 0.4617 (0.5909)	
training:	Epoch: [8][132/204]	Loss 0.5546 (0.5907)	
training:	Epoch: [8][133/204]	Loss 0.6004 (0.5907)	
training:	Epoch: [8][134/204]	Loss 0.6781 (0.5914)	
training:	Epoch: [8][135/204]	Loss 0.6352 (0.5917)	
training:	Epoch: [8][136/204]	Loss 0.7039 (0.5925)	
training:	Epoch: [8][137/204]	Loss 0.6085 (0.5927)	
training:	Epoch: [8][138/204]	Loss 0.6448 (0.5930)	
training:	Epoch: [8][139/204]	Loss 0.6577 (0.5935)	
training:	Epoch: [8][140/204]	Loss 0.5841 (0.5934)	
training:	Epoch: [8][141/204]	Loss 0.5789 (0.5933)	
training:	Epoch: [8][142/204]	Loss 0.6364 (0.5936)	
training:	Epoch: [8][143/204]	Loss 0.6381 (0.5939)	
training:	Epoch: [8][144/204]	Loss 0.5871 (0.5939)	
training:	Epoch: [8][145/204]	Loss 0.6158 (0.5940)	
training:	Epoch: [8][146/204]	Loss 0.6154 (0.5942)	
training:	Epoch: [8][147/204]	Loss 0.5058 (0.5936)	
training:	Epoch: [8][148/204]	Loss 0.5832 (0.5935)	
training:	Epoch: [8][149/204]	Loss 0.6062 (0.5936)	
training:	Epoch: [8][150/204]	Loss 0.5773 (0.5935)	
training:	Epoch: [8][151/204]	Loss 0.6412 (0.5938)	
training:	Epoch: [8][152/204]	Loss 0.6292 (0.5940)	
training:	Epoch: [8][153/204]	Loss 0.6015 (0.5941)	
training:	Epoch: [8][154/204]	Loss 0.6142 (0.5942)	
training:	Epoch: [8][155/204]	Loss 0.5631 (0.5940)	
training:	Epoch: [8][156/204]	Loss 0.5339 (0.5936)	
training:	Epoch: [8][157/204]	Loss 0.5706 (0.5935)	
training:	Epoch: [8][158/204]	Loss 0.5915 (0.5935)	
training:	Epoch: [8][159/204]	Loss 0.5775 (0.5934)	
training:	Epoch: [8][160/204]	Loss 0.6338 (0.5936)	
training:	Epoch: [8][161/204]	Loss 0.6614 (0.5941)	
training:	Epoch: [8][162/204]	Loss 0.6044 (0.5941)	
training:	Epoch: [8][163/204]	Loss 0.5553 (0.5939)	
training:	Epoch: [8][164/204]	Loss 0.7219 (0.5947)	
training:	Epoch: [8][165/204]	Loss 0.5941 (0.5947)	
training:	Epoch: [8][166/204]	Loss 0.6695 (0.5951)	
training:	Epoch: [8][167/204]	Loss 0.5579 (0.5949)	
training:	Epoch: [8][168/204]	Loss 0.4872 (0.5942)	
training:	Epoch: [8][169/204]	Loss 0.5692 (0.5941)	
training:	Epoch: [8][170/204]	Loss 0.6336 (0.5943)	
training:	Epoch: [8][171/204]	Loss 0.5970 (0.5943)	
training:	Epoch: [8][172/204]	Loss 0.5216 (0.5939)	
training:	Epoch: [8][173/204]	Loss 0.5538 (0.5937)	
training:	Epoch: [8][174/204]	Loss 0.5308 (0.5933)	
training:	Epoch: [8][175/204]	Loss 0.5961 (0.5933)	
training:	Epoch: [8][176/204]	Loss 0.4868 (0.5927)	
training:	Epoch: [8][177/204]	Loss 0.5304 (0.5924)	
training:	Epoch: [8][178/204]	Loss 0.6468 (0.5927)	
training:	Epoch: [8][179/204]	Loss 0.5144 (0.5923)	
training:	Epoch: [8][180/204]	Loss 0.5691 (0.5921)	
training:	Epoch: [8][181/204]	Loss 0.5615 (0.5920)	
training:	Epoch: [8][182/204]	Loss 0.6472 (0.5923)	
training:	Epoch: [8][183/204]	Loss 0.5456 (0.5920)	
training:	Epoch: [8][184/204]	Loss 0.5999 (0.5920)	
training:	Epoch: [8][185/204]	Loss 0.6007 (0.5921)	
training:	Epoch: [8][186/204]	Loss 0.5763 (0.5920)	
training:	Epoch: [8][187/204]	Loss 0.6178 (0.5921)	
training:	Epoch: [8][188/204]	Loss 0.5690 (0.5920)	
training:	Epoch: [8][189/204]	Loss 0.5903 (0.5920)	
training:	Epoch: [8][190/204]	Loss 0.5873 (0.5920)	
training:	Epoch: [8][191/204]	Loss 0.7768 (0.5930)	
training:	Epoch: [8][192/204]	Loss 0.5706 (0.5928)	
training:	Epoch: [8][193/204]	Loss 0.6752 (0.5933)	
training:	Epoch: [8][194/204]	Loss 0.5425 (0.5930)	
training:	Epoch: [8][195/204]	Loss 0.6029 (0.5931)	
training:	Epoch: [8][196/204]	Loss 0.5426 (0.5928)	
training:	Epoch: [8][197/204]	Loss 0.5703 (0.5927)	
training:	Epoch: [8][198/204]	Loss 0.4625 (0.5920)	
training:	Epoch: [8][199/204]	Loss 0.4735 (0.5914)	
training:	Epoch: [8][200/204]	Loss 0.6219 (0.5916)	
training:	Epoch: [8][201/204]	Loss 0.6916 (0.5921)	
training:	Epoch: [8][202/204]	Loss 0.5847 (0.5920)	
training:	Epoch: [8][203/204]	Loss 0.6861 (0.5925)	
training:	Epoch: [8][204/204]	Loss 0.4855 (0.5920)	
Training:	 Loss: 0.5911

Training:	 ACC: 0.6983 0.6991 0.7187 0.6779
Validation:	 ACC: 0.6902 0.6918 0.7247 0.6558
Validation:	 Best_BACC: 0.6902 0.6918 0.7247 0.6558
Validation:	 Loss: 0.5910
Pretraining:	Epoch 9/200
----------
training:	Epoch: [9][1/204]	Loss 0.6884 (0.6884)	
training:	Epoch: [9][2/204]	Loss 0.5507 (0.6195)	
training:	Epoch: [9][3/204]	Loss 0.7933 (0.6775)	
training:	Epoch: [9][4/204]	Loss 0.5944 (0.6567)	
training:	Epoch: [9][5/204]	Loss 0.5552 (0.6364)	
training:	Epoch: [9][6/204]	Loss 0.6837 (0.6443)	
training:	Epoch: [9][7/204]	Loss 0.6005 (0.6380)	
training:	Epoch: [9][8/204]	Loss 0.5071 (0.6216)	
training:	Epoch: [9][9/204]	Loss 0.6512 (0.6249)	
training:	Epoch: [9][10/204]	Loss 0.6314 (0.6256)	
training:	Epoch: [9][11/204]	Loss 0.6021 (0.6234)	
training:	Epoch: [9][12/204]	Loss 0.6763 (0.6278)	
training:	Epoch: [9][13/204]	Loss 0.4629 (0.6152)	
training:	Epoch: [9][14/204]	Loss 0.6329 (0.6164)	
training:	Epoch: [9][15/204]	Loss 0.6463 (0.6184)	
training:	Epoch: [9][16/204]	Loss 0.6177 (0.6184)	
training:	Epoch: [9][17/204]	Loss 0.5896 (0.6167)	
training:	Epoch: [9][18/204]	Loss 0.5112 (0.6108)	
training:	Epoch: [9][19/204]	Loss 0.5793 (0.6091)	
training:	Epoch: [9][20/204]	Loss 0.4877 (0.6031)	
training:	Epoch: [9][21/204]	Loss 0.5607 (0.6011)	
training:	Epoch: [9][22/204]	Loss 0.5490 (0.5987)	
training:	Epoch: [9][23/204]	Loss 0.5172 (0.5951)	
training:	Epoch: [9][24/204]	Loss 0.5249 (0.5922)	
training:	Epoch: [9][25/204]	Loss 0.5964 (0.5924)	
training:	Epoch: [9][26/204]	Loss 0.5452 (0.5906)	
training:	Epoch: [9][27/204]	Loss 0.6274 (0.5919)	
training:	Epoch: [9][28/204]	Loss 0.5403 (0.5901)	
training:	Epoch: [9][29/204]	Loss 0.6872 (0.5934)	
training:	Epoch: [9][30/204]	Loss 0.6367 (0.5949)	
training:	Epoch: [9][31/204]	Loss 0.4983 (0.5918)	
training:	Epoch: [9][32/204]	Loss 0.4620 (0.5877)	
training:	Epoch: [9][33/204]	Loss 0.5803 (0.5875)	
training:	Epoch: [9][34/204]	Loss 0.7166 (0.5913)	
training:	Epoch: [9][35/204]	Loss 0.5852 (0.5911)	
training:	Epoch: [9][36/204]	Loss 0.4175 (0.5863)	
training:	Epoch: [9][37/204]	Loss 0.5982 (0.5866)	
training:	Epoch: [9][38/204]	Loss 0.6009 (0.5870)	
training:	Epoch: [9][39/204]	Loss 0.5877 (0.5870)	
training:	Epoch: [9][40/204]	Loss 0.5968 (0.5872)	
training:	Epoch: [9][41/204]	Loss 0.5684 (0.5868)	
training:	Epoch: [9][42/204]	Loss 0.7030 (0.5896)	
training:	Epoch: [9][43/204]	Loss 0.5908 (0.5896)	
training:	Epoch: [9][44/204]	Loss 0.4827 (0.5872)	
training:	Epoch: [9][45/204]	Loss 0.4872 (0.5849)	
training:	Epoch: [9][46/204]	Loss 0.4966 (0.5830)	
training:	Epoch: [9][47/204]	Loss 0.6384 (0.5842)	
training:	Epoch: [9][48/204]	Loss 0.4805 (0.5820)	
training:	Epoch: [9][49/204]	Loss 0.5490 (0.5814)	
training:	Epoch: [9][50/204]	Loss 0.5123 (0.5800)	
training:	Epoch: [9][51/204]	Loss 0.7025 (0.5824)	
training:	Epoch: [9][52/204]	Loss 0.4778 (0.5804)	
training:	Epoch: [9][53/204]	Loss 0.5923 (0.5806)	
training:	Epoch: [9][54/204]	Loss 0.5418 (0.5799)	
training:	Epoch: [9][55/204]	Loss 0.5289 (0.5790)	
training:	Epoch: [9][56/204]	Loss 0.4730 (0.5771)	
training:	Epoch: [9][57/204]	Loss 0.6808 (0.5789)	
training:	Epoch: [9][58/204]	Loss 0.5319 (0.5781)	
training:	Epoch: [9][59/204]	Loss 0.7586 (0.5811)	
training:	Epoch: [9][60/204]	Loss 0.5502 (0.5806)	
training:	Epoch: [9][61/204]	Loss 0.5346 (0.5799)	
training:	Epoch: [9][62/204]	Loss 0.5261 (0.5790)	
training:	Epoch: [9][63/204]	Loss 0.5874 (0.5791)	
training:	Epoch: [9][64/204]	Loss 0.6025 (0.5795)	
training:	Epoch: [9][65/204]	Loss 0.6841 (0.5811)	
training:	Epoch: [9][66/204]	Loss 0.6036 (0.5814)	
training:	Epoch: [9][67/204]	Loss 0.5140 (0.5804)	
training:	Epoch: [9][68/204]	Loss 0.6061 (0.5808)	
training:	Epoch: [9][69/204]	Loss 0.6115 (0.5813)	
training:	Epoch: [9][70/204]	Loss 0.6116 (0.5817)	
training:	Epoch: [9][71/204]	Loss 0.6131 (0.5821)	
training:	Epoch: [9][72/204]	Loss 0.6593 (0.5832)	
training:	Epoch: [9][73/204]	Loss 0.5391 (0.5826)	
training:	Epoch: [9][74/204]	Loss 0.5361 (0.5820)	
training:	Epoch: [9][75/204]	Loss 0.6539 (0.5829)	
training:	Epoch: [9][76/204]	Loss 0.5680 (0.5827)	
training:	Epoch: [9][77/204]	Loss 0.5535 (0.5824)	
training:	Epoch: [9][78/204]	Loss 0.5957 (0.5825)	
training:	Epoch: [9][79/204]	Loss 0.5819 (0.5825)	
training:	Epoch: [9][80/204]	Loss 0.7257 (0.5843)	
training:	Epoch: [9][81/204]	Loss 0.5685 (0.5841)	
training:	Epoch: [9][82/204]	Loss 0.5366 (0.5835)	
training:	Epoch: [9][83/204]	Loss 0.5779 (0.5835)	
training:	Epoch: [9][84/204]	Loss 0.5956 (0.5836)	
training:	Epoch: [9][85/204]	Loss 0.5593 (0.5833)	
training:	Epoch: [9][86/204]	Loss 0.5392 (0.5828)	
training:	Epoch: [9][87/204]	Loss 0.6521 (0.5836)	
training:	Epoch: [9][88/204]	Loss 0.6457 (0.5843)	
training:	Epoch: [9][89/204]	Loss 0.5881 (0.5844)	
training:	Epoch: [9][90/204]	Loss 0.4716 (0.5831)	
training:	Epoch: [9][91/204]	Loss 0.6185 (0.5835)	
training:	Epoch: [9][92/204]	Loss 0.6149 (0.5838)	
training:	Epoch: [9][93/204]	Loss 0.5746 (0.5837)	
training:	Epoch: [9][94/204]	Loss 0.5280 (0.5831)	
training:	Epoch: [9][95/204]	Loss 0.6252 (0.5836)	
training:	Epoch: [9][96/204]	Loss 0.6470 (0.5842)	
training:	Epoch: [9][97/204]	Loss 0.5615 (0.5840)	
training:	Epoch: [9][98/204]	Loss 0.6886 (0.5851)	
training:	Epoch: [9][99/204]	Loss 0.5673 (0.5849)	
training:	Epoch: [9][100/204]	Loss 0.6913 (0.5860)	
training:	Epoch: [9][101/204]	Loss 0.5104 (0.5852)	
training:	Epoch: [9][102/204]	Loss 0.5057 (0.5844)	
training:	Epoch: [9][103/204]	Loss 0.5934 (0.5845)	
training:	Epoch: [9][104/204]	Loss 0.6515 (0.5852)	
training:	Epoch: [9][105/204]	Loss 0.4732 (0.5841)	
training:	Epoch: [9][106/204]	Loss 0.6129 (0.5844)	
training:	Epoch: [9][107/204]	Loss 0.6300 (0.5848)	
training:	Epoch: [9][108/204]	Loss 0.5269 (0.5843)	
training:	Epoch: [9][109/204]	Loss 0.6226 (0.5846)	
training:	Epoch: [9][110/204]	Loss 0.4855 (0.5837)	
training:	Epoch: [9][111/204]	Loss 0.5398 (0.5833)	
training:	Epoch: [9][112/204]	Loss 0.6002 (0.5835)	
training:	Epoch: [9][113/204]	Loss 0.5836 (0.5835)	
training:	Epoch: [9][114/204]	Loss 0.6424 (0.5840)	
training:	Epoch: [9][115/204]	Loss 0.5574 (0.5838)	
training:	Epoch: [9][116/204]	Loss 0.6554 (0.5844)	
training:	Epoch: [9][117/204]	Loss 0.6677 (0.5851)	
training:	Epoch: [9][118/204]	Loss 0.6535 (0.5857)	
training:	Epoch: [9][119/204]	Loss 0.6019 (0.5858)	
training:	Epoch: [9][120/204]	Loss 0.5452 (0.5855)	
training:	Epoch: [9][121/204]	Loss 0.6434 (0.5859)	
training:	Epoch: [9][122/204]	Loss 0.6555 (0.5865)	
training:	Epoch: [9][123/204]	Loss 0.5872 (0.5865)	
training:	Epoch: [9][124/204]	Loss 0.5748 (0.5864)	
training:	Epoch: [9][125/204]	Loss 0.5002 (0.5857)	
training:	Epoch: [9][126/204]	Loss 0.6620 (0.5863)	
training:	Epoch: [9][127/204]	Loss 0.6241 (0.5866)	
training:	Epoch: [9][128/204]	Loss 0.6371 (0.5870)	
training:	Epoch: [9][129/204]	Loss 0.5648 (0.5869)	
training:	Epoch: [9][130/204]	Loss 0.6712 (0.5875)	
training:	Epoch: [9][131/204]	Loss 0.4843 (0.5867)	
training:	Epoch: [9][132/204]	Loss 0.6506 (0.5872)	
training:	Epoch: [9][133/204]	Loss 0.5175 (0.5867)	
training:	Epoch: [9][134/204]	Loss 0.5099 (0.5861)	
training:	Epoch: [9][135/204]	Loss 0.6120 (0.5863)	
training:	Epoch: [9][136/204]	Loss 0.5689 (0.5862)	
training:	Epoch: [9][137/204]	Loss 0.5645 (0.5860)	
training:	Epoch: [9][138/204]	Loss 0.4840 (0.5853)	
training:	Epoch: [9][139/204]	Loss 0.6069 (0.5854)	
training:	Epoch: [9][140/204]	Loss 0.5477 (0.5852)	
training:	Epoch: [9][141/204]	Loss 0.5001 (0.5846)	
training:	Epoch: [9][142/204]	Loss 0.5464 (0.5843)	
training:	Epoch: [9][143/204]	Loss 0.5350 (0.5839)	
training:	Epoch: [9][144/204]	Loss 0.6040 (0.5841)	
training:	Epoch: [9][145/204]	Loss 0.5411 (0.5838)	
training:	Epoch: [9][146/204]	Loss 0.5148 (0.5833)	
training:	Epoch: [9][147/204]	Loss 0.5739 (0.5832)	
training:	Epoch: [9][148/204]	Loss 0.4597 (0.5824)	
training:	Epoch: [9][149/204]	Loss 0.4616 (0.5816)	
training:	Epoch: [9][150/204]	Loss 0.6246 (0.5819)	
training:	Epoch: [9][151/204]	Loss 0.5854 (0.5819)	
training:	Epoch: [9][152/204]	Loss 0.5173 (0.5815)	
training:	Epoch: [9][153/204]	Loss 0.5549 (0.5813)	
training:	Epoch: [9][154/204]	Loss 0.5928 (0.5814)	
training:	Epoch: [9][155/204]	Loss 0.5975 (0.5815)	
training:	Epoch: [9][156/204]	Loss 0.5333 (0.5812)	
training:	Epoch: [9][157/204]	Loss 0.5189 (0.5808)	
training:	Epoch: [9][158/204]	Loss 0.6411 (0.5812)	
training:	Epoch: [9][159/204]	Loss 0.6323 (0.5815)	
training:	Epoch: [9][160/204]	Loss 0.5741 (0.5814)	
training:	Epoch: [9][161/204]	Loss 0.5866 (0.5815)	
training:	Epoch: [9][162/204]	Loss 0.6379 (0.5818)	
training:	Epoch: [9][163/204]	Loss 0.5800 (0.5818)	
training:	Epoch: [9][164/204]	Loss 0.7100 (0.5826)	
training:	Epoch: [9][165/204]	Loss 0.6760 (0.5832)	
training:	Epoch: [9][166/204]	Loss 0.5195 (0.5828)	
training:	Epoch: [9][167/204]	Loss 0.5399 (0.5825)	
training:	Epoch: [9][168/204]	Loss 0.6711 (0.5830)	
training:	Epoch: [9][169/204]	Loss 0.5646 (0.5829)	
training:	Epoch: [9][170/204]	Loss 0.5689 (0.5829)	
training:	Epoch: [9][171/204]	Loss 0.5556 (0.5827)	
training:	Epoch: [9][172/204]	Loss 0.6303 (0.5830)	
training:	Epoch: [9][173/204]	Loss 0.6554 (0.5834)	
training:	Epoch: [9][174/204]	Loss 0.5741 (0.5833)	
training:	Epoch: [9][175/204]	Loss 0.5989 (0.5834)	
training:	Epoch: [9][176/204]	Loss 0.6513 (0.5838)	
training:	Epoch: [9][177/204]	Loss 0.5953 (0.5839)	
training:	Epoch: [9][178/204]	Loss 0.4619 (0.5832)	
training:	Epoch: [9][179/204]	Loss 0.5741 (0.5831)	
training:	Epoch: [9][180/204]	Loss 0.5110 (0.5827)	
training:	Epoch: [9][181/204]	Loss 0.5779 (0.5827)	
training:	Epoch: [9][182/204]	Loss 0.5879 (0.5827)	
training:	Epoch: [9][183/204]	Loss 0.6077 (0.5829)	
training:	Epoch: [9][184/204]	Loss 0.6999 (0.5835)	
training:	Epoch: [9][185/204]	Loss 0.5550 (0.5834)	
training:	Epoch: [9][186/204]	Loss 0.6423 (0.5837)	
training:	Epoch: [9][187/204]	Loss 0.5887 (0.5837)	
training:	Epoch: [9][188/204]	Loss 0.6159 (0.5839)	
training:	Epoch: [9][189/204]	Loss 0.5917 (0.5839)	
training:	Epoch: [9][190/204]	Loss 0.6197 (0.5841)	
training:	Epoch: [9][191/204]	Loss 0.5178 (0.5838)	
training:	Epoch: [9][192/204]	Loss 0.5763 (0.5837)	
training:	Epoch: [9][193/204]	Loss 0.4741 (0.5832)	
training:	Epoch: [9][194/204]	Loss 0.5685 (0.5831)	
training:	Epoch: [9][195/204]	Loss 0.5245 (0.5828)	
training:	Epoch: [9][196/204]	Loss 0.6565 (0.5832)	
training:	Epoch: [9][197/204]	Loss 0.5636 (0.5831)	
training:	Epoch: [9][198/204]	Loss 0.5577 (0.5829)	
training:	Epoch: [9][199/204]	Loss 0.5796 (0.5829)	
training:	Epoch: [9][200/204]	Loss 0.5732 (0.5829)	
training:	Epoch: [9][201/204]	Loss 0.6725 (0.5833)	
training:	Epoch: [9][202/204]	Loss 0.5013 (0.5829)	
training:	Epoch: [9][203/204]	Loss 0.5209 (0.5826)	
training:	Epoch: [9][204/204]	Loss 0.6384 (0.5829)	
Training:	 Loss: 0.5820

Training:	 ACC: 0.7053 0.7059 0.7199 0.6907
Validation:	 ACC: 0.6964 0.6977 0.7257 0.6670
Validation:	 Best_BACC: 0.6964 0.6977 0.7257 0.6670
Validation:	 Loss: 0.5830
Pretraining:	Epoch 10/200
----------
training:	Epoch: [10][1/204]	Loss 0.6211 (0.6211)	
training:	Epoch: [10][2/204]	Loss 0.6211 (0.6211)	
training:	Epoch: [10][3/204]	Loss 0.4776 (0.5733)	
training:	Epoch: [10][4/204]	Loss 0.5073 (0.5568)	
training:	Epoch: [10][5/204]	Loss 0.5561 (0.5566)	
training:	Epoch: [10][6/204]	Loss 0.5426 (0.5543)	
training:	Epoch: [10][7/204]	Loss 0.6191 (0.5636)	
training:	Epoch: [10][8/204]	Loss 0.5019 (0.5559)	
training:	Epoch: [10][9/204]	Loss 0.5147 (0.5513)	
training:	Epoch: [10][10/204]	Loss 0.5474 (0.5509)	
training:	Epoch: [10][11/204]	Loss 0.5778 (0.5533)	
training:	Epoch: [10][12/204]	Loss 0.6333 (0.5600)	
training:	Epoch: [10][13/204]	Loss 0.5815 (0.5617)	
training:	Epoch: [10][14/204]	Loss 0.5128 (0.5582)	
training:	Epoch: [10][15/204]	Loss 0.4876 (0.5535)	
training:	Epoch: [10][16/204]	Loss 0.4748 (0.5485)	
training:	Epoch: [10][17/204]	Loss 0.5651 (0.5495)	
training:	Epoch: [10][18/204]	Loss 0.5985 (0.5522)	
training:	Epoch: [10][19/204]	Loss 0.6384 (0.5568)	
training:	Epoch: [10][20/204]	Loss 0.6474 (0.5613)	
training:	Epoch: [10][21/204]	Loss 0.4982 (0.5583)	
training:	Epoch: [10][22/204]	Loss 0.5867 (0.5596)	
training:	Epoch: [10][23/204]	Loss 0.6943 (0.5654)	
training:	Epoch: [10][24/204]	Loss 0.6319 (0.5682)	
training:	Epoch: [10][25/204]	Loss 0.5920 (0.5692)	
training:	Epoch: [10][26/204]	Loss 0.4886 (0.5661)	
training:	Epoch: [10][27/204]	Loss 0.5375 (0.5650)	
training:	Epoch: [10][28/204]	Loss 0.5779 (0.5655)	
training:	Epoch: [10][29/204]	Loss 0.5461 (0.5648)	
training:	Epoch: [10][30/204]	Loss 0.6128 (0.5664)	
training:	Epoch: [10][31/204]	Loss 0.7307 (0.5717)	
training:	Epoch: [10][32/204]	Loss 0.4933 (0.5693)	
training:	Epoch: [10][33/204]	Loss 0.6299 (0.5711)	
training:	Epoch: [10][34/204]	Loss 0.6238 (0.5726)	
training:	Epoch: [10][35/204]	Loss 0.5584 (0.5722)	
training:	Epoch: [10][36/204]	Loss 0.5889 (0.5727)	
training:	Epoch: [10][37/204]	Loss 0.6806 (0.5756)	
training:	Epoch: [10][38/204]	Loss 0.6367 (0.5772)	
training:	Epoch: [10][39/204]	Loss 0.6332 (0.5787)	
training:	Epoch: [10][40/204]	Loss 0.5131 (0.5770)	
training:	Epoch: [10][41/204]	Loss 0.6039 (0.5777)	
training:	Epoch: [10][42/204]	Loss 0.6729 (0.5799)	
training:	Epoch: [10][43/204]	Loss 0.5894 (0.5802)	
training:	Epoch: [10][44/204]	Loss 0.5893 (0.5804)	
training:	Epoch: [10][45/204]	Loss 0.5329 (0.5793)	
training:	Epoch: [10][46/204]	Loss 0.4163 (0.5758)	
training:	Epoch: [10][47/204]	Loss 0.6244 (0.5768)	
training:	Epoch: [10][48/204]	Loss 0.7159 (0.5797)	
training:	Epoch: [10][49/204]	Loss 0.5877 (0.5799)	
training:	Epoch: [10][50/204]	Loss 0.7719 (0.5837)	
training:	Epoch: [10][51/204]	Loss 0.6227 (0.5845)	
training:	Epoch: [10][52/204]	Loss 0.5556 (0.5839)	
training:	Epoch: [10][53/204]	Loss 0.6910 (0.5859)	
training:	Epoch: [10][54/204]	Loss 0.4759 (0.5839)	
training:	Epoch: [10][55/204]	Loss 0.6548 (0.5852)	
training:	Epoch: [10][56/204]	Loss 0.7289 (0.5878)	
training:	Epoch: [10][57/204]	Loss 0.5370 (0.5869)	
training:	Epoch: [10][58/204]	Loss 0.5223 (0.5858)	
training:	Epoch: [10][59/204]	Loss 0.6581 (0.5870)	
training:	Epoch: [10][60/204]	Loss 0.6944 (0.5888)	
training:	Epoch: [10][61/204]	Loss 0.5101 (0.5875)	
training:	Epoch: [10][62/204]	Loss 0.4287 (0.5849)	
training:	Epoch: [10][63/204]	Loss 0.5959 (0.5851)	
training:	Epoch: [10][64/204]	Loss 0.5672 (0.5848)	
training:	Epoch: [10][65/204]	Loss 0.6047 (0.5851)	
training:	Epoch: [10][66/204]	Loss 0.5880 (0.5852)	
training:	Epoch: [10][67/204]	Loss 0.6493 (0.5861)	
training:	Epoch: [10][68/204]	Loss 0.6068 (0.5864)	
training:	Epoch: [10][69/204]	Loss 0.6608 (0.5875)	
training:	Epoch: [10][70/204]	Loss 0.5868 (0.5875)	
training:	Epoch: [10][71/204]	Loss 0.4984 (0.5862)	
training:	Epoch: [10][72/204]	Loss 0.5898 (0.5863)	
training:	Epoch: [10][73/204]	Loss 0.5454 (0.5857)	
training:	Epoch: [10][74/204]	Loss 0.5312 (0.5850)	
training:	Epoch: [10][75/204]	Loss 0.6297 (0.5856)	
training:	Epoch: [10][76/204]	Loss 0.5818 (0.5855)	
training:	Epoch: [10][77/204]	Loss 0.7338 (0.5875)	
training:	Epoch: [10][78/204]	Loss 0.5039 (0.5864)	
training:	Epoch: [10][79/204]	Loss 0.5854 (0.5864)	
training:	Epoch: [10][80/204]	Loss 0.5268 (0.5856)	
training:	Epoch: [10][81/204]	Loss 0.5428 (0.5851)	
training:	Epoch: [10][82/204]	Loss 0.6673 (0.5861)	
training:	Epoch: [10][83/204]	Loss 0.5250 (0.5854)	
training:	Epoch: [10][84/204]	Loss 0.5199 (0.5846)	
training:	Epoch: [10][85/204]	Loss 0.5141 (0.5838)	
training:	Epoch: [10][86/204]	Loss 0.5694 (0.5836)	
training:	Epoch: [10][87/204]	Loss 0.4644 (0.5822)	
training:	Epoch: [10][88/204]	Loss 0.4875 (0.5812)	
training:	Epoch: [10][89/204]	Loss 0.5925 (0.5813)	
training:	Epoch: [10][90/204]	Loss 0.5611 (0.5811)	
training:	Epoch: [10][91/204]	Loss 0.6579 (0.5819)	
training:	Epoch: [10][92/204]	Loss 0.5533 (0.5816)	
training:	Epoch: [10][93/204]	Loss 0.4981 (0.5807)	
training:	Epoch: [10][94/204]	Loss 0.5719 (0.5806)	
training:	Epoch: [10][95/204]	Loss 0.5187 (0.5799)	
training:	Epoch: [10][96/204]	Loss 0.5598 (0.5797)	
training:	Epoch: [10][97/204]	Loss 0.6006 (0.5800)	
training:	Epoch: [10][98/204]	Loss 0.5669 (0.5798)	
training:	Epoch: [10][99/204]	Loss 0.6330 (0.5804)	
training:	Epoch: [10][100/204]	Loss 0.5758 (0.5803)	
training:	Epoch: [10][101/204]	Loss 0.5291 (0.5798)	
training:	Epoch: [10][102/204]	Loss 0.5168 (0.5792)	
training:	Epoch: [10][103/204]	Loss 0.5258 (0.5787)	
training:	Epoch: [10][104/204]	Loss 0.5071 (0.5780)	
training:	Epoch: [10][105/204]	Loss 0.5577 (0.5778)	
training:	Epoch: [10][106/204]	Loss 0.6234 (0.5782)	
training:	Epoch: [10][107/204]	Loss 0.5528 (0.5780)	
training:	Epoch: [10][108/204]	Loss 0.5228 (0.5775)	
training:	Epoch: [10][109/204]	Loss 0.5473 (0.5772)	
training:	Epoch: [10][110/204]	Loss 0.6462 (0.5778)	
training:	Epoch: [10][111/204]	Loss 0.7120 (0.5790)	
training:	Epoch: [10][112/204]	Loss 0.5060 (0.5784)	
training:	Epoch: [10][113/204]	Loss 0.4466 (0.5772)	
training:	Epoch: [10][114/204]	Loss 0.6281 (0.5777)	
training:	Epoch: [10][115/204]	Loss 0.5319 (0.5773)	
training:	Epoch: [10][116/204]	Loss 0.4819 (0.5764)	
training:	Epoch: [10][117/204]	Loss 0.6194 (0.5768)	
training:	Epoch: [10][118/204]	Loss 0.6352 (0.5773)	
training:	Epoch: [10][119/204]	Loss 0.5680 (0.5772)	
training:	Epoch: [10][120/204]	Loss 0.5639 (0.5771)	
training:	Epoch: [10][121/204]	Loss 0.5309 (0.5767)	
training:	Epoch: [10][122/204]	Loss 0.5699 (0.5767)	
training:	Epoch: [10][123/204]	Loss 0.7190 (0.5778)	
training:	Epoch: [10][124/204]	Loss 0.5244 (0.5774)	
training:	Epoch: [10][125/204]	Loss 0.4769 (0.5766)	
training:	Epoch: [10][126/204]	Loss 0.5270 (0.5762)	
training:	Epoch: [10][127/204]	Loss 0.4800 (0.5754)	
training:	Epoch: [10][128/204]	Loss 0.6118 (0.5757)	
training:	Epoch: [10][129/204]	Loss 0.6742 (0.5765)	
training:	Epoch: [10][130/204]	Loss 0.6563 (0.5771)	
training:	Epoch: [10][131/204]	Loss 0.6110 (0.5774)	
training:	Epoch: [10][132/204]	Loss 0.6420 (0.5779)	
training:	Epoch: [10][133/204]	Loss 0.5944 (0.5780)	
training:	Epoch: [10][134/204]	Loss 0.4528 (0.5770)	
training:	Epoch: [10][135/204]	Loss 0.6395 (0.5775)	
training:	Epoch: [10][136/204]	Loss 0.7337 (0.5787)	
training:	Epoch: [10][137/204]	Loss 0.6442 (0.5791)	
training:	Epoch: [10][138/204]	Loss 0.6083 (0.5793)	
training:	Epoch: [10][139/204]	Loss 0.5220 (0.5789)	
training:	Epoch: [10][140/204]	Loss 0.5571 (0.5788)	
training:	Epoch: [10][141/204]	Loss 0.4890 (0.5781)	
training:	Epoch: [10][142/204]	Loss 0.5962 (0.5783)	
training:	Epoch: [10][143/204]	Loss 0.5235 (0.5779)	
training:	Epoch: [10][144/204]	Loss 0.5974 (0.5780)	
training:	Epoch: [10][145/204]	Loss 0.4417 (0.5771)	
training:	Epoch: [10][146/204]	Loss 0.5586 (0.5770)	
training:	Epoch: [10][147/204]	Loss 0.5613 (0.5768)	
training:	Epoch: [10][148/204]	Loss 0.5281 (0.5765)	
training:	Epoch: [10][149/204]	Loss 0.5599 (0.5764)	
training:	Epoch: [10][150/204]	Loss 0.5983 (0.5766)	
training:	Epoch: [10][151/204]	Loss 0.6042 (0.5767)	
training:	Epoch: [10][152/204]	Loss 0.5547 (0.5766)	
training:	Epoch: [10][153/204]	Loss 0.5174 (0.5762)	
training:	Epoch: [10][154/204]	Loss 0.5411 (0.5760)	
training:	Epoch: [10][155/204]	Loss 0.6858 (0.5767)	
training:	Epoch: [10][156/204]	Loss 0.5370 (0.5764)	
training:	Epoch: [10][157/204]	Loss 0.5824 (0.5765)	
training:	Epoch: [10][158/204]	Loss 0.5950 (0.5766)	
training:	Epoch: [10][159/204]	Loss 0.5571 (0.5765)	
training:	Epoch: [10][160/204]	Loss 0.5813 (0.5765)	
training:	Epoch: [10][161/204]	Loss 0.5069 (0.5761)	
training:	Epoch: [10][162/204]	Loss 0.5420 (0.5759)	
training:	Epoch: [10][163/204]	Loss 0.5576 (0.5757)	
training:	Epoch: [10][164/204]	Loss 0.5360 (0.5755)	
training:	Epoch: [10][165/204]	Loss 0.5797 (0.5755)	
training:	Epoch: [10][166/204]	Loss 0.4929 (0.5750)	
training:	Epoch: [10][167/204]	Loss 0.5633 (0.5750)	
training:	Epoch: [10][168/204]	Loss 0.6114 (0.5752)	
training:	Epoch: [10][169/204]	Loss 0.4859 (0.5746)	
training:	Epoch: [10][170/204]	Loss 0.6600 (0.5751)	
training:	Epoch: [10][171/204]	Loss 0.5103 (0.5748)	
training:	Epoch: [10][172/204]	Loss 0.5136 (0.5744)	
training:	Epoch: [10][173/204]	Loss 0.4193 (0.5735)	
training:	Epoch: [10][174/204]	Loss 0.5229 (0.5732)	
training:	Epoch: [10][175/204]	Loss 0.5861 (0.5733)	
training:	Epoch: [10][176/204]	Loss 0.5775 (0.5733)	
training:	Epoch: [10][177/204]	Loss 0.4669 (0.5727)	
training:	Epoch: [10][178/204]	Loss 0.4769 (0.5722)	
training:	Epoch: [10][179/204]	Loss 0.6101 (0.5724)	
training:	Epoch: [10][180/204]	Loss 0.7031 (0.5731)	
training:	Epoch: [10][181/204]	Loss 0.6166 (0.5734)	
training:	Epoch: [10][182/204]	Loss 0.5648 (0.5733)	
training:	Epoch: [10][183/204]	Loss 0.6392 (0.5737)	
training:	Epoch: [10][184/204]	Loss 0.6337 (0.5740)	
training:	Epoch: [10][185/204]	Loss 0.5085 (0.5736)	
training:	Epoch: [10][186/204]	Loss 0.4623 (0.5730)	
training:	Epoch: [10][187/204]	Loss 0.5288 (0.5728)	
training:	Epoch: [10][188/204]	Loss 0.4214 (0.5720)	
training:	Epoch: [10][189/204]	Loss 0.5267 (0.5718)	
training:	Epoch: [10][190/204]	Loss 0.7436 (0.5727)	
training:	Epoch: [10][191/204]	Loss 0.5936 (0.5728)	
training:	Epoch: [10][192/204]	Loss 0.7170 (0.5735)	
training:	Epoch: [10][193/204]	Loss 0.4985 (0.5731)	
training:	Epoch: [10][194/204]	Loss 0.5797 (0.5732)	
training:	Epoch: [10][195/204]	Loss 0.6096 (0.5734)	
training:	Epoch: [10][196/204]	Loss 0.6254 (0.5736)	
training:	Epoch: [10][197/204]	Loss 0.5604 (0.5736)	
training:	Epoch: [10][198/204]	Loss 0.5267 (0.5733)	
training:	Epoch: [10][199/204]	Loss 0.5052 (0.5730)	
training:	Epoch: [10][200/204]	Loss 0.5552 (0.5729)	
training:	Epoch: [10][201/204]	Loss 0.6960 (0.5735)	
training:	Epoch: [10][202/204]	Loss 0.5509 (0.5734)	
training:	Epoch: [10][203/204]	Loss 0.5376 (0.5732)	
training:	Epoch: [10][204/204]	Loss 0.6590 (0.5736)	
Training:	 Loss: 0.5728

Training:	 ACC: 0.7108 0.7129 0.7616 0.6601
Validation:	 ACC: 0.7068 0.7100 0.7769 0.6368
Validation:	 Best_BACC: 0.7068 0.7100 0.7769 0.6368
Validation:	 Loss: 0.5769
Pretraining:	Epoch 11/200
----------
training:	Epoch: [11][1/204]	Loss 0.4768 (0.4768)	
training:	Epoch: [11][2/204]	Loss 0.6017 (0.5393)	
training:	Epoch: [11][3/204]	Loss 0.5550 (0.5445)	
training:	Epoch: [11][4/204]	Loss 0.5767 (0.5526)	
training:	Epoch: [11][5/204]	Loss 0.6080 (0.5636)	
training:	Epoch: [11][6/204]	Loss 0.5607 (0.5632)	
training:	Epoch: [11][7/204]	Loss 0.6135 (0.5704)	
training:	Epoch: [11][8/204]	Loss 0.5419 (0.5668)	
training:	Epoch: [11][9/204]	Loss 0.4970 (0.5590)	
training:	Epoch: [11][10/204]	Loss 0.3741 (0.5405)	
training:	Epoch: [11][11/204]	Loss 0.5893 (0.5450)	
training:	Epoch: [11][12/204]	Loss 0.5893 (0.5487)	
training:	Epoch: [11][13/204]	Loss 0.5046 (0.5453)	
training:	Epoch: [11][14/204]	Loss 0.6397 (0.5520)	
training:	Epoch: [11][15/204]	Loss 0.5783 (0.5538)	
training:	Epoch: [11][16/204]	Loss 0.5472 (0.5534)	
training:	Epoch: [11][17/204]	Loss 0.5248 (0.5517)	
training:	Epoch: [11][18/204]	Loss 0.5329 (0.5506)	
training:	Epoch: [11][19/204]	Loss 0.4836 (0.5471)	
training:	Epoch: [11][20/204]	Loss 0.5556 (0.5475)	
training:	Epoch: [11][21/204]	Loss 0.6021 (0.5501)	
training:	Epoch: [11][22/204]	Loss 0.5963 (0.5522)	
training:	Epoch: [11][23/204]	Loss 0.4751 (0.5489)	
training:	Epoch: [11][24/204]	Loss 0.5013 (0.5469)	
training:	Epoch: [11][25/204]	Loss 0.6872 (0.5525)	
training:	Epoch: [11][26/204]	Loss 0.6350 (0.5557)	
training:	Epoch: [11][27/204]	Loss 0.5206 (0.5544)	
training:	Epoch: [11][28/204]	Loss 0.5505 (0.5542)	
training:	Epoch: [11][29/204]	Loss 0.6581 (0.5578)	
training:	Epoch: [11][30/204]	Loss 0.5587 (0.5579)	
training:	Epoch: [11][31/204]	Loss 0.5181 (0.5566)	
training:	Epoch: [11][32/204]	Loss 0.4545 (0.5534)	
training:	Epoch: [11][33/204]	Loss 0.6409 (0.5560)	
training:	Epoch: [11][34/204]	Loss 0.5327 (0.5554)	
training:	Epoch: [11][35/204]	Loss 0.5476 (0.5551)	
training:	Epoch: [11][36/204]	Loss 0.5517 (0.5550)	
training:	Epoch: [11][37/204]	Loss 0.4186 (0.5513)	
training:	Epoch: [11][38/204]	Loss 0.4692 (0.5492)	
training:	Epoch: [11][39/204]	Loss 0.6325 (0.5513)	
training:	Epoch: [11][40/204]	Loss 0.5924 (0.5523)	
training:	Epoch: [11][41/204]	Loss 0.5607 (0.5526)	
training:	Epoch: [11][42/204]	Loss 0.5497 (0.5525)	
training:	Epoch: [11][43/204]	Loss 0.5096 (0.5515)	
training:	Epoch: [11][44/204]	Loss 0.6215 (0.5531)	
training:	Epoch: [11][45/204]	Loss 0.6599 (0.5555)	
training:	Epoch: [11][46/204]	Loss 0.6144 (0.5567)	
training:	Epoch: [11][47/204]	Loss 0.7244 (0.5603)	
training:	Epoch: [11][48/204]	Loss 0.6785 (0.5628)	
training:	Epoch: [11][49/204]	Loss 0.5785 (0.5631)	
training:	Epoch: [11][50/204]	Loss 0.5478 (0.5628)	
training:	Epoch: [11][51/204]	Loss 0.6107 (0.5637)	
training:	Epoch: [11][52/204]	Loss 0.5669 (0.5638)	
training:	Epoch: [11][53/204]	Loss 0.5204 (0.5630)	
training:	Epoch: [11][54/204]	Loss 0.6395 (0.5644)	
training:	Epoch: [11][55/204]	Loss 0.5391 (0.5639)	
training:	Epoch: [11][56/204]	Loss 0.5194 (0.5631)	
training:	Epoch: [11][57/204]	Loss 0.7136 (0.5658)	
training:	Epoch: [11][58/204]	Loss 0.5305 (0.5652)	
training:	Epoch: [11][59/204]	Loss 0.5372 (0.5647)	
training:	Epoch: [11][60/204]	Loss 0.6054 (0.5654)	
training:	Epoch: [11][61/204]	Loss 0.6225 (0.5663)	
training:	Epoch: [11][62/204]	Loss 0.6434 (0.5675)	
training:	Epoch: [11][63/204]	Loss 0.6490 (0.5688)	
training:	Epoch: [11][64/204]	Loss 0.5441 (0.5684)	
training:	Epoch: [11][65/204]	Loss 0.4523 (0.5667)	
training:	Epoch: [11][66/204]	Loss 0.6131 (0.5674)	
training:	Epoch: [11][67/204]	Loss 0.5071 (0.5665)	
training:	Epoch: [11][68/204]	Loss 0.6332 (0.5674)	
training:	Epoch: [11][69/204]	Loss 0.5790 (0.5676)	
training:	Epoch: [11][70/204]	Loss 0.5794 (0.5678)	
training:	Epoch: [11][71/204]	Loss 0.5083 (0.5669)	
training:	Epoch: [11][72/204]	Loss 0.4605 (0.5655)	
training:	Epoch: [11][73/204]	Loss 0.6227 (0.5662)	
training:	Epoch: [11][74/204]	Loss 0.5437 (0.5659)	
training:	Epoch: [11][75/204]	Loss 0.5574 (0.5658)	
training:	Epoch: [11][76/204]	Loss 0.5595 (0.5657)	
training:	Epoch: [11][77/204]	Loss 0.4897 (0.5648)	
training:	Epoch: [11][78/204]	Loss 0.5572 (0.5647)	
training:	Epoch: [11][79/204]	Loss 0.6309 (0.5655)	
training:	Epoch: [11][80/204]	Loss 0.7171 (0.5674)	
training:	Epoch: [11][81/204]	Loss 0.5435 (0.5671)	
training:	Epoch: [11][82/204]	Loss 0.5598 (0.5670)	
training:	Epoch: [11][83/204]	Loss 0.4665 (0.5658)	
training:	Epoch: [11][84/204]	Loss 0.6006 (0.5662)	
training:	Epoch: [11][85/204]	Loss 0.5332 (0.5658)	
training:	Epoch: [11][86/204]	Loss 0.5615 (0.5658)	
training:	Epoch: [11][87/204]	Loss 0.6270 (0.5665)	
training:	Epoch: [11][88/204]	Loss 0.6336 (0.5672)	
training:	Epoch: [11][89/204]	Loss 0.6502 (0.5682)	
training:	Epoch: [11][90/204]	Loss 0.5368 (0.5678)	
training:	Epoch: [11][91/204]	Loss 0.4645 (0.5667)	
training:	Epoch: [11][92/204]	Loss 0.5626 (0.5666)	
training:	Epoch: [11][93/204]	Loss 0.4352 (0.5652)	
training:	Epoch: [11][94/204]	Loss 0.5158 (0.5647)	
training:	Epoch: [11][95/204]	Loss 0.5447 (0.5645)	
training:	Epoch: [11][96/204]	Loss 0.5657 (0.5645)	
training:	Epoch: [11][97/204]	Loss 0.6532 (0.5654)	
training:	Epoch: [11][98/204]	Loss 0.5696 (0.5655)	
training:	Epoch: [11][99/204]	Loss 0.4771 (0.5646)	
training:	Epoch: [11][100/204]	Loss 0.5677 (0.5646)	
training:	Epoch: [11][101/204]	Loss 0.7393 (0.5663)	
training:	Epoch: [11][102/204]	Loss 0.6484 (0.5671)	
training:	Epoch: [11][103/204]	Loss 0.5812 (0.5673)	
training:	Epoch: [11][104/204]	Loss 0.5171 (0.5668)	
training:	Epoch: [11][105/204]	Loss 0.6233 (0.5673)	
training:	Epoch: [11][106/204]	Loss 0.6325 (0.5679)	
training:	Epoch: [11][107/204]	Loss 0.4142 (0.5665)	
training:	Epoch: [11][108/204]	Loss 0.6723 (0.5675)	
training:	Epoch: [11][109/204]	Loss 0.5259 (0.5671)	
training:	Epoch: [11][110/204]	Loss 0.6779 (0.5681)	
training:	Epoch: [11][111/204]	Loss 0.5109 (0.5676)	
training:	Epoch: [11][112/204]	Loss 0.5334 (0.5673)	
training:	Epoch: [11][113/204]	Loss 0.6153 (0.5677)	
training:	Epoch: [11][114/204]	Loss 0.5438 (0.5675)	
training:	Epoch: [11][115/204]	Loss 0.6707 (0.5684)	
training:	Epoch: [11][116/204]	Loss 0.5929 (0.5686)	
training:	Epoch: [11][117/204]	Loss 0.5020 (0.5680)	
training:	Epoch: [11][118/204]	Loss 0.5538 (0.5679)	
training:	Epoch: [11][119/204]	Loss 0.6234 (0.5684)	
training:	Epoch: [11][120/204]	Loss 0.5352 (0.5681)	
training:	Epoch: [11][121/204]	Loss 0.4604 (0.5672)	
training:	Epoch: [11][122/204]	Loss 0.7028 (0.5683)	
training:	Epoch: [11][123/204]	Loss 0.4571 (0.5674)	
training:	Epoch: [11][124/204]	Loss 0.6188 (0.5678)	
training:	Epoch: [11][125/204]	Loss 0.5620 (0.5678)	
training:	Epoch: [11][126/204]	Loss 0.5465 (0.5676)	
training:	Epoch: [11][127/204]	Loss 0.5847 (0.5678)	
training:	Epoch: [11][128/204]	Loss 0.6174 (0.5682)	
training:	Epoch: [11][129/204]	Loss 0.4777 (0.5675)	
training:	Epoch: [11][130/204]	Loss 0.5321 (0.5672)	
training:	Epoch: [11][131/204]	Loss 0.7021 (0.5682)	
training:	Epoch: [11][132/204]	Loss 0.6335 (0.5687)	
training:	Epoch: [11][133/204]	Loss 0.5399 (0.5685)	
training:	Epoch: [11][134/204]	Loss 0.5332 (0.5682)	
training:	Epoch: [11][135/204]	Loss 0.5619 (0.5682)	
training:	Epoch: [11][136/204]	Loss 0.6262 (0.5686)	
training:	Epoch: [11][137/204]	Loss 0.6009 (0.5688)	
training:	Epoch: [11][138/204]	Loss 0.5274 (0.5685)	
training:	Epoch: [11][139/204]	Loss 0.5351 (0.5683)	
training:	Epoch: [11][140/204]	Loss 0.6086 (0.5686)	
training:	Epoch: [11][141/204]	Loss 0.5645 (0.5686)	
training:	Epoch: [11][142/204]	Loss 0.5680 (0.5686)	
training:	Epoch: [11][143/204]	Loss 0.4088 (0.5674)	
training:	Epoch: [11][144/204]	Loss 0.5733 (0.5675)	
training:	Epoch: [11][145/204]	Loss 0.6361 (0.5680)	
training:	Epoch: [11][146/204]	Loss 0.5186 (0.5676)	
training:	Epoch: [11][147/204]	Loss 0.5954 (0.5678)	
training:	Epoch: [11][148/204]	Loss 0.6296 (0.5682)	
training:	Epoch: [11][149/204]	Loss 0.5530 (0.5681)	
training:	Epoch: [11][150/204]	Loss 0.4561 (0.5674)	
training:	Epoch: [11][151/204]	Loss 0.5498 (0.5673)	
training:	Epoch: [11][152/204]	Loss 0.5010 (0.5668)	
training:	Epoch: [11][153/204]	Loss 0.4742 (0.5662)	
training:	Epoch: [11][154/204]	Loss 0.5830 (0.5663)	
training:	Epoch: [11][155/204]	Loss 0.5192 (0.5660)	
training:	Epoch: [11][156/204]	Loss 0.6757 (0.5667)	
training:	Epoch: [11][157/204]	Loss 0.6063 (0.5670)	
training:	Epoch: [11][158/204]	Loss 0.5958 (0.5672)	
training:	Epoch: [11][159/204]	Loss 0.4393 (0.5664)	
training:	Epoch: [11][160/204]	Loss 0.6078 (0.5666)	
training:	Epoch: [11][161/204]	Loss 0.5874 (0.5667)	
training:	Epoch: [11][162/204]	Loss 0.5629 (0.5667)	
training:	Epoch: [11][163/204]	Loss 0.5557 (0.5667)	
training:	Epoch: [11][164/204]	Loss 0.5660 (0.5666)	
training:	Epoch: [11][165/204]	Loss 0.6087 (0.5669)	
training:	Epoch: [11][166/204]	Loss 0.7114 (0.5678)	
training:	Epoch: [11][167/204]	Loss 0.5582 (0.5677)	
training:	Epoch: [11][168/204]	Loss 0.5865 (0.5678)	
training:	Epoch: [11][169/204]	Loss 0.6700 (0.5684)	
training:	Epoch: [11][170/204]	Loss 0.6076 (0.5687)	
training:	Epoch: [11][171/204]	Loss 0.7312 (0.5696)	
training:	Epoch: [11][172/204]	Loss 0.4221 (0.5688)	
training:	Epoch: [11][173/204]	Loss 0.5353 (0.5686)	
training:	Epoch: [11][174/204]	Loss 0.5171 (0.5683)	
training:	Epoch: [11][175/204]	Loss 0.4172 (0.5674)	
training:	Epoch: [11][176/204]	Loss 0.5089 (0.5671)	
training:	Epoch: [11][177/204]	Loss 0.5745 (0.5671)	
training:	Epoch: [11][178/204]	Loss 0.6474 (0.5676)	
training:	Epoch: [11][179/204]	Loss 0.4043 (0.5666)	
training:	Epoch: [11][180/204]	Loss 0.6083 (0.5669)	
training:	Epoch: [11][181/204]	Loss 0.6891 (0.5676)	
training:	Epoch: [11][182/204]	Loss 0.5905 (0.5677)	
training:	Epoch: [11][183/204]	Loss 0.6787 (0.5683)	
training:	Epoch: [11][184/204]	Loss 0.5858 (0.5684)	
training:	Epoch: [11][185/204]	Loss 0.4493 (0.5677)	
training:	Epoch: [11][186/204]	Loss 0.5266 (0.5675)	
training:	Epoch: [11][187/204]	Loss 0.4891 (0.5671)	
training:	Epoch: [11][188/204]	Loss 0.5196 (0.5668)	
training:	Epoch: [11][189/204]	Loss 0.5709 (0.5669)	
training:	Epoch: [11][190/204]	Loss 0.6136 (0.5671)	
training:	Epoch: [11][191/204]	Loss 0.5099 (0.5668)	
training:	Epoch: [11][192/204]	Loss 0.6257 (0.5671)	
training:	Epoch: [11][193/204]	Loss 0.4548 (0.5665)	
training:	Epoch: [11][194/204]	Loss 0.5138 (0.5663)	
training:	Epoch: [11][195/204]	Loss 0.5611 (0.5662)	
training:	Epoch: [11][196/204]	Loss 0.5229 (0.5660)	
training:	Epoch: [11][197/204]	Loss 0.5342 (0.5659)	
training:	Epoch: [11][198/204]	Loss 0.4246 (0.5651)	
training:	Epoch: [11][199/204]	Loss 0.5754 (0.5652)	
training:	Epoch: [11][200/204]	Loss 0.6101 (0.5654)	
training:	Epoch: [11][201/204]	Loss 0.4820 (0.5650)	
training:	Epoch: [11][202/204]	Loss 0.5574 (0.5650)	
training:	Epoch: [11][203/204]	Loss 0.5921 (0.5651)	
training:	Epoch: [11][204/204]	Loss 0.5638 (0.5651)	
Training:	 Loss: 0.5642

Training:	 ACC: 0.7179 0.7195 0.7572 0.6786
Validation:	 ACC: 0.7057 0.7084 0.7656 0.6457
Validation:	 Best_BACC: 0.7068 0.7100 0.7769 0.6368
Validation:	 Loss: 0.5705
Pretraining:	Epoch 12/200
----------
training:	Epoch: [12][1/204]	Loss 0.6261 (0.6261)	
training:	Epoch: [12][2/204]	Loss 0.6492 (0.6376)	
training:	Epoch: [12][3/204]	Loss 0.5763 (0.6172)	
training:	Epoch: [12][4/204]	Loss 0.5695 (0.6053)	
training:	Epoch: [12][5/204]	Loss 0.5369 (0.5916)	
training:	Epoch: [12][6/204]	Loss 0.5134 (0.5786)	
training:	Epoch: [12][7/204]	Loss 0.5869 (0.5798)	
training:	Epoch: [12][8/204]	Loss 0.5195 (0.5722)	
training:	Epoch: [12][9/204]	Loss 0.4599 (0.5598)	
training:	Epoch: [12][10/204]	Loss 0.5102 (0.5548)	
training:	Epoch: [12][11/204]	Loss 0.4844 (0.5484)	
training:	Epoch: [12][12/204]	Loss 0.4895 (0.5435)	
training:	Epoch: [12][13/204]	Loss 0.5457 (0.5437)	
training:	Epoch: [12][14/204]	Loss 0.5655 (0.5452)	
training:	Epoch: [12][15/204]	Loss 0.5488 (0.5455)	
training:	Epoch: [12][16/204]	Loss 0.5401 (0.5451)	
training:	Epoch: [12][17/204]	Loss 0.5934 (0.5480)	
training:	Epoch: [12][18/204]	Loss 0.4938 (0.5450)	
training:	Epoch: [12][19/204]	Loss 0.6528 (0.5506)	
training:	Epoch: [12][20/204]	Loss 0.6445 (0.5553)	
training:	Epoch: [12][21/204]	Loss 0.5452 (0.5548)	
training:	Epoch: [12][22/204]	Loss 0.5057 (0.5526)	
training:	Epoch: [12][23/204]	Loss 0.4323 (0.5474)	
training:	Epoch: [12][24/204]	Loss 0.4875 (0.5449)	
training:	Epoch: [12][25/204]	Loss 0.5691 (0.5458)	
training:	Epoch: [12][26/204]	Loss 0.5534 (0.5461)	
training:	Epoch: [12][27/204]	Loss 0.6875 (0.5514)	
training:	Epoch: [12][28/204]	Loss 0.6418 (0.5546)	
training:	Epoch: [12][29/204]	Loss 0.5156 (0.5533)	
training:	Epoch: [12][30/204]	Loss 0.5390 (0.5528)	
training:	Epoch: [12][31/204]	Loss 0.6513 (0.5560)	
training:	Epoch: [12][32/204]	Loss 0.4629 (0.5531)	
training:	Epoch: [12][33/204]	Loss 0.6700 (0.5566)	
training:	Epoch: [12][34/204]	Loss 0.5342 (0.5559)	
training:	Epoch: [12][35/204]	Loss 0.4825 (0.5538)	
training:	Epoch: [12][36/204]	Loss 0.5308 (0.5532)	
training:	Epoch: [12][37/204]	Loss 0.5963 (0.5544)	
training:	Epoch: [12][38/204]	Loss 0.5984 (0.5555)	
training:	Epoch: [12][39/204]	Loss 0.5669 (0.5558)	
training:	Epoch: [12][40/204]	Loss 0.6012 (0.5569)	
training:	Epoch: [12][41/204]	Loss 0.7005 (0.5604)	
training:	Epoch: [12][42/204]	Loss 0.6196 (0.5619)	
training:	Epoch: [12][43/204]	Loss 0.5378 (0.5613)	
training:	Epoch: [12][44/204]	Loss 0.5616 (0.5613)	
training:	Epoch: [12][45/204]	Loss 0.6501 (0.5633)	
training:	Epoch: [12][46/204]	Loss 0.5574 (0.5632)	
training:	Epoch: [12][47/204]	Loss 0.5999 (0.5639)	
training:	Epoch: [12][48/204]	Loss 0.4553 (0.5617)	
training:	Epoch: [12][49/204]	Loss 0.4651 (0.5597)	
training:	Epoch: [12][50/204]	Loss 0.4912 (0.5583)	
training:	Epoch: [12][51/204]	Loss 0.4748 (0.5567)	
training:	Epoch: [12][52/204]	Loss 0.6025 (0.5576)	
training:	Epoch: [12][53/204]	Loss 0.4941 (0.5564)	
training:	Epoch: [12][54/204]	Loss 0.5757 (0.5567)	
training:	Epoch: [12][55/204]	Loss 0.5956 (0.5574)	
training:	Epoch: [12][56/204]	Loss 0.6389 (0.5589)	
training:	Epoch: [12][57/204]	Loss 0.5911 (0.5595)	
training:	Epoch: [12][58/204]	Loss 0.5730 (0.5597)	
training:	Epoch: [12][59/204]	Loss 0.6802 (0.5617)	
training:	Epoch: [12][60/204]	Loss 0.5580 (0.5617)	
training:	Epoch: [12][61/204]	Loss 0.5528 (0.5615)	
training:	Epoch: [12][62/204]	Loss 0.4932 (0.5604)	
training:	Epoch: [12][63/204]	Loss 0.6129 (0.5613)	
training:	Epoch: [12][64/204]	Loss 0.4957 (0.5602)	
training:	Epoch: [12][65/204]	Loss 0.6680 (0.5619)	
training:	Epoch: [12][66/204]	Loss 0.5044 (0.5610)	
training:	Epoch: [12][67/204]	Loss 0.6247 (0.5620)	
training:	Epoch: [12][68/204]	Loss 0.5316 (0.5615)	
training:	Epoch: [12][69/204]	Loss 0.5994 (0.5621)	
training:	Epoch: [12][70/204]	Loss 0.7233 (0.5644)	
training:	Epoch: [12][71/204]	Loss 0.5586 (0.5643)	
training:	Epoch: [12][72/204]	Loss 0.5164 (0.5636)	
training:	Epoch: [12][73/204]	Loss 0.6033 (0.5642)	
training:	Epoch: [12][74/204]	Loss 0.5285 (0.5637)	
training:	Epoch: [12][75/204]	Loss 0.4726 (0.5625)	
training:	Epoch: [12][76/204]	Loss 0.5118 (0.5618)	
training:	Epoch: [12][77/204]	Loss 0.4588 (0.5605)	
training:	Epoch: [12][78/204]	Loss 0.5325 (0.5601)	
training:	Epoch: [12][79/204]	Loss 0.5061 (0.5594)	
training:	Epoch: [12][80/204]	Loss 0.4349 (0.5579)	
training:	Epoch: [12][81/204]	Loss 0.4712 (0.5568)	
training:	Epoch: [12][82/204]	Loss 0.6833 (0.5583)	
training:	Epoch: [12][83/204]	Loss 0.5380 (0.5581)	
training:	Epoch: [12][84/204]	Loss 0.5476 (0.5580)	
training:	Epoch: [12][85/204]	Loss 0.4693 (0.5569)	
training:	Epoch: [12][86/204]	Loss 0.6538 (0.5581)	
training:	Epoch: [12][87/204]	Loss 0.4479 (0.5568)	
training:	Epoch: [12][88/204]	Loss 0.6759 (0.5581)	
training:	Epoch: [12][89/204]	Loss 0.5458 (0.5580)	
training:	Epoch: [12][90/204]	Loss 0.4423 (0.5567)	
training:	Epoch: [12][91/204]	Loss 0.5727 (0.5569)	
training:	Epoch: [12][92/204]	Loss 0.4953 (0.5562)	
training:	Epoch: [12][93/204]	Loss 0.5879 (0.5566)	
training:	Epoch: [12][94/204]	Loss 0.5453 (0.5564)	
training:	Epoch: [12][95/204]	Loss 0.4446 (0.5553)	
training:	Epoch: [12][96/204]	Loss 0.7933 (0.5577)	
training:	Epoch: [12][97/204]	Loss 0.5083 (0.5572)	
training:	Epoch: [12][98/204]	Loss 0.6247 (0.5579)	
training:	Epoch: [12][99/204]	Loss 0.5803 (0.5582)	
training:	Epoch: [12][100/204]	Loss 0.6339 (0.5589)	
training:	Epoch: [12][101/204]	Loss 0.5620 (0.5589)	
training:	Epoch: [12][102/204]	Loss 0.5248 (0.5586)	
training:	Epoch: [12][103/204]	Loss 0.5623 (0.5586)	
training:	Epoch: [12][104/204]	Loss 0.5181 (0.5583)	
training:	Epoch: [12][105/204]	Loss 0.4968 (0.5577)	
training:	Epoch: [12][106/204]	Loss 0.5043 (0.5572)	
training:	Epoch: [12][107/204]	Loss 0.4942 (0.5566)	
training:	Epoch: [12][108/204]	Loss 0.6267 (0.5572)	
training:	Epoch: [12][109/204]	Loss 0.6093 (0.5577)	
training:	Epoch: [12][110/204]	Loss 0.5184 (0.5573)	
training:	Epoch: [12][111/204]	Loss 0.5275 (0.5571)	
training:	Epoch: [12][112/204]	Loss 0.4664 (0.5563)	
training:	Epoch: [12][113/204]	Loss 0.5789 (0.5565)	
training:	Epoch: [12][114/204]	Loss 0.4520 (0.5556)	
training:	Epoch: [12][115/204]	Loss 0.7022 (0.5568)	
training:	Epoch: [12][116/204]	Loss 0.5372 (0.5567)	
training:	Epoch: [12][117/204]	Loss 0.4806 (0.5560)	
training:	Epoch: [12][118/204]	Loss 0.5593 (0.5560)	
training:	Epoch: [12][119/204]	Loss 0.5339 (0.5558)	
training:	Epoch: [12][120/204]	Loss 0.5963 (0.5562)	
training:	Epoch: [12][121/204]	Loss 0.4871 (0.5556)	
training:	Epoch: [12][122/204]	Loss 0.5737 (0.5558)	
training:	Epoch: [12][123/204]	Loss 0.4963 (0.5553)	
training:	Epoch: [12][124/204]	Loss 0.5977 (0.5556)	
training:	Epoch: [12][125/204]	Loss 0.6740 (0.5566)	
training:	Epoch: [12][126/204]	Loss 0.5629 (0.5566)	
training:	Epoch: [12][127/204]	Loss 0.4760 (0.5560)	
training:	Epoch: [12][128/204]	Loss 0.6552 (0.5568)	
training:	Epoch: [12][129/204]	Loss 0.5937 (0.5570)	
training:	Epoch: [12][130/204]	Loss 0.5075 (0.5567)	
training:	Epoch: [12][131/204]	Loss 0.5686 (0.5568)	
training:	Epoch: [12][132/204]	Loss 0.4727 (0.5561)	
training:	Epoch: [12][133/204]	Loss 0.4486 (0.5553)	
training:	Epoch: [12][134/204]	Loss 0.5552 (0.5553)	
training:	Epoch: [12][135/204]	Loss 0.5970 (0.5556)	
training:	Epoch: [12][136/204]	Loss 0.4528 (0.5549)	
training:	Epoch: [12][137/204]	Loss 0.5714 (0.5550)	
training:	Epoch: [12][138/204]	Loss 0.5276 (0.5548)	
training:	Epoch: [12][139/204]	Loss 0.5512 (0.5548)	
training:	Epoch: [12][140/204]	Loss 0.6632 (0.5555)	
training:	Epoch: [12][141/204]	Loss 0.4906 (0.5551)	
training:	Epoch: [12][142/204]	Loss 0.5124 (0.5548)	
training:	Epoch: [12][143/204]	Loss 0.5082 (0.5544)	
training:	Epoch: [12][144/204]	Loss 0.5685 (0.5545)	
training:	Epoch: [12][145/204]	Loss 0.7587 (0.5560)	
training:	Epoch: [12][146/204]	Loss 0.4225 (0.5550)	
training:	Epoch: [12][147/204]	Loss 0.6117 (0.5554)	
training:	Epoch: [12][148/204]	Loss 0.5700 (0.5555)	
training:	Epoch: [12][149/204]	Loss 0.5539 (0.5555)	
training:	Epoch: [12][150/204]	Loss 0.5635 (0.5556)	
training:	Epoch: [12][151/204]	Loss 0.4648 (0.5550)	
training:	Epoch: [12][152/204]	Loss 0.5157 (0.5547)	
training:	Epoch: [12][153/204]	Loss 0.5382 (0.5546)	
training:	Epoch: [12][154/204]	Loss 0.5868 (0.5548)	
training:	Epoch: [12][155/204]	Loss 0.5775 (0.5550)	
training:	Epoch: [12][156/204]	Loss 0.5604 (0.5550)	
training:	Epoch: [12][157/204]	Loss 0.4909 (0.5546)	
training:	Epoch: [12][158/204]	Loss 0.5145 (0.5543)	
training:	Epoch: [12][159/204]	Loss 0.6899 (0.5552)	
training:	Epoch: [12][160/204]	Loss 0.6699 (0.5559)	
training:	Epoch: [12][161/204]	Loss 0.5045 (0.5556)	
training:	Epoch: [12][162/204]	Loss 0.5578 (0.5556)	
training:	Epoch: [12][163/204]	Loss 0.5755 (0.5557)	
training:	Epoch: [12][164/204]	Loss 0.5420 (0.5556)	
training:	Epoch: [12][165/204]	Loss 0.5955 (0.5559)	
training:	Epoch: [12][166/204]	Loss 0.5331 (0.5557)	
training:	Epoch: [12][167/204]	Loss 0.5797 (0.5559)	
training:	Epoch: [12][168/204]	Loss 0.5259 (0.5557)	
training:	Epoch: [12][169/204]	Loss 0.6283 (0.5561)	
training:	Epoch: [12][170/204]	Loss 0.4410 (0.5554)	
training:	Epoch: [12][171/204]	Loss 0.4788 (0.5550)	
training:	Epoch: [12][172/204]	Loss 0.7291 (0.5560)	
training:	Epoch: [12][173/204]	Loss 0.6075 (0.5563)	
training:	Epoch: [12][174/204]	Loss 0.5419 (0.5562)	
training:	Epoch: [12][175/204]	Loss 0.5913 (0.5564)	
training:	Epoch: [12][176/204]	Loss 0.5252 (0.5563)	
training:	Epoch: [12][177/204]	Loss 0.5929 (0.5565)	
training:	Epoch: [12][178/204]	Loss 0.6164 (0.5568)	
training:	Epoch: [12][179/204]	Loss 0.5485 (0.5567)	
training:	Epoch: [12][180/204]	Loss 0.5293 (0.5566)	
training:	Epoch: [12][181/204]	Loss 0.5255 (0.5564)	
training:	Epoch: [12][182/204]	Loss 0.4934 (0.5561)	
training:	Epoch: [12][183/204]	Loss 0.6039 (0.5563)	
training:	Epoch: [12][184/204]	Loss 0.5797 (0.5565)	
training:	Epoch: [12][185/204]	Loss 0.4882 (0.5561)	
training:	Epoch: [12][186/204]	Loss 0.6383 (0.5565)	
training:	Epoch: [12][187/204]	Loss 0.6050 (0.5568)	
training:	Epoch: [12][188/204]	Loss 0.5448 (0.5567)	
training:	Epoch: [12][189/204]	Loss 0.4549 (0.5562)	
training:	Epoch: [12][190/204]	Loss 0.4315 (0.5555)	
training:	Epoch: [12][191/204]	Loss 0.5026 (0.5553)	
training:	Epoch: [12][192/204]	Loss 0.6203 (0.5556)	
training:	Epoch: [12][193/204]	Loss 0.6082 (0.5559)	
training:	Epoch: [12][194/204]	Loss 0.6241 (0.5562)	
training:	Epoch: [12][195/204]	Loss 0.5266 (0.5561)	
training:	Epoch: [12][196/204]	Loss 0.6137 (0.5564)	
training:	Epoch: [12][197/204]	Loss 0.5396 (0.5563)	
training:	Epoch: [12][198/204]	Loss 0.5760 (0.5564)	
training:	Epoch: [12][199/204]	Loss 0.5974 (0.5566)	
training:	Epoch: [12][200/204]	Loss 0.4824 (0.5562)	
training:	Epoch: [12][201/204]	Loss 0.5016 (0.5559)	
training:	Epoch: [12][202/204]	Loss 0.4914 (0.5556)	
training:	Epoch: [12][203/204]	Loss 0.6300 (0.5560)	
training:	Epoch: [12][204/204]	Loss 0.6528 (0.5565)	
Training:	 Loss: 0.5556

Training:	 ACC: 0.7269 0.7265 0.7169 0.7369
Validation:	 ACC: 0.7114 0.7116 0.7155 0.7074
Validation:	 Best_BACC: 0.7114 0.7116 0.7155 0.7074
Validation:	 Loss: 0.5641
Pretraining:	Epoch 13/200
----------
training:	Epoch: [13][1/204]	Loss 0.6495 (0.6495)	
training:	Epoch: [13][2/204]	Loss 0.6249 (0.6372)	
training:	Epoch: [13][3/204]	Loss 0.5100 (0.5948)	
training:	Epoch: [13][4/204]	Loss 0.5176 (0.5755)	
training:	Epoch: [13][5/204]	Loss 0.6441 (0.5892)	
training:	Epoch: [13][6/204]	Loss 0.5308 (0.5795)	
training:	Epoch: [13][7/204]	Loss 0.5069 (0.5691)	
training:	Epoch: [13][8/204]	Loss 0.6079 (0.5740)	
training:	Epoch: [13][9/204]	Loss 0.5457 (0.5708)	
training:	Epoch: [13][10/204]	Loss 0.6287 (0.5766)	
training:	Epoch: [13][11/204]	Loss 0.4724 (0.5671)	
training:	Epoch: [13][12/204]	Loss 0.4320 (0.5559)	
training:	Epoch: [13][13/204]	Loss 0.6791 (0.5654)	
training:	Epoch: [13][14/204]	Loss 0.4465 (0.5569)	
training:	Epoch: [13][15/204]	Loss 0.5927 (0.5593)	
training:	Epoch: [13][16/204]	Loss 0.5141 (0.5564)	
training:	Epoch: [13][17/204]	Loss 0.6423 (0.5615)	
training:	Epoch: [13][18/204]	Loss 0.6152 (0.5645)	
training:	Epoch: [13][19/204]	Loss 0.4918 (0.5606)	
training:	Epoch: [13][20/204]	Loss 0.4488 (0.5551)	
training:	Epoch: [13][21/204]	Loss 0.5745 (0.5560)	
training:	Epoch: [13][22/204]	Loss 0.5520 (0.5558)	
training:	Epoch: [13][23/204]	Loss 0.5141 (0.5540)	
training:	Epoch: [13][24/204]	Loss 0.4912 (0.5514)	
training:	Epoch: [13][25/204]	Loss 0.6493 (0.5553)	
training:	Epoch: [13][26/204]	Loss 0.5713 (0.5559)	
training:	Epoch: [13][27/204]	Loss 0.6196 (0.5583)	
training:	Epoch: [13][28/204]	Loss 0.5632 (0.5584)	
training:	Epoch: [13][29/204]	Loss 0.5383 (0.5577)	
training:	Epoch: [13][30/204]	Loss 0.4872 (0.5554)	
training:	Epoch: [13][31/204]	Loss 0.5461 (0.5551)	
training:	Epoch: [13][32/204]	Loss 0.3851 (0.5498)	
training:	Epoch: [13][33/204]	Loss 0.5874 (0.5509)	
training:	Epoch: [13][34/204]	Loss 0.7461 (0.5567)	
training:	Epoch: [13][35/204]	Loss 0.6804 (0.5602)	
training:	Epoch: [13][36/204]	Loss 0.5186 (0.5590)	
training:	Epoch: [13][37/204]	Loss 0.4603 (0.5564)	
training:	Epoch: [13][38/204]	Loss 0.5899 (0.5573)	
training:	Epoch: [13][39/204]	Loss 0.4528 (0.5546)	
training:	Epoch: [13][40/204]	Loss 0.5767 (0.5551)	
training:	Epoch: [13][41/204]	Loss 0.5514 (0.5550)	
training:	Epoch: [13][42/204]	Loss 0.6908 (0.5583)	
training:	Epoch: [13][43/204]	Loss 0.4339 (0.5554)	
training:	Epoch: [13][44/204]	Loss 0.5067 (0.5543)	
training:	Epoch: [13][45/204]	Loss 0.6166 (0.5557)	
training:	Epoch: [13][46/204]	Loss 0.5786 (0.5562)	
training:	Epoch: [13][47/204]	Loss 0.5430 (0.5559)	
training:	Epoch: [13][48/204]	Loss 0.5405 (0.5556)	
training:	Epoch: [13][49/204]	Loss 0.5564 (0.5556)	
training:	Epoch: [13][50/204]	Loss 0.6416 (0.5573)	
training:	Epoch: [13][51/204]	Loss 0.4842 (0.5559)	
training:	Epoch: [13][52/204]	Loss 0.5375 (0.5555)	
training:	Epoch: [13][53/204]	Loss 0.5052 (0.5546)	
training:	Epoch: [13][54/204]	Loss 0.4704 (0.5530)	
training:	Epoch: [13][55/204]	Loss 0.6134 (0.5541)	
training:	Epoch: [13][56/204]	Loss 0.5827 (0.5546)	
training:	Epoch: [13][57/204]	Loss 0.4781 (0.5533)	
training:	Epoch: [13][58/204]	Loss 0.5181 (0.5527)	
training:	Epoch: [13][59/204]	Loss 0.6499 (0.5543)	
training:	Epoch: [13][60/204]	Loss 0.4236 (0.5521)	
training:	Epoch: [13][61/204]	Loss 0.7520 (0.5554)	
training:	Epoch: [13][62/204]	Loss 0.5486 (0.5553)	
training:	Epoch: [13][63/204]	Loss 0.5421 (0.5551)	
training:	Epoch: [13][64/204]	Loss 0.6005 (0.5558)	
training:	Epoch: [13][65/204]	Loss 0.4868 (0.5547)	
training:	Epoch: [13][66/204]	Loss 0.5384 (0.5545)	
training:	Epoch: [13][67/204]	Loss 0.4598 (0.5531)	
training:	Epoch: [13][68/204]	Loss 0.5654 (0.5533)	
training:	Epoch: [13][69/204]	Loss 0.5071 (0.5526)	
training:	Epoch: [13][70/204]	Loss 0.5070 (0.5519)	
training:	Epoch: [13][71/204]	Loss 0.4815 (0.5509)	
training:	Epoch: [13][72/204]	Loss 0.5844 (0.5514)	
training:	Epoch: [13][73/204]	Loss 0.4723 (0.5503)	
training:	Epoch: [13][74/204]	Loss 0.5746 (0.5507)	
training:	Epoch: [13][75/204]	Loss 0.4789 (0.5497)	
training:	Epoch: [13][76/204]	Loss 0.5479 (0.5497)	
training:	Epoch: [13][77/204]	Loss 0.5860 (0.5501)	
training:	Epoch: [13][78/204]	Loss 0.5001 (0.5495)	
training:	Epoch: [13][79/204]	Loss 0.4971 (0.5488)	
training:	Epoch: [13][80/204]	Loss 0.5967 (0.5494)	
training:	Epoch: [13][81/204]	Loss 0.6448 (0.5506)	
training:	Epoch: [13][82/204]	Loss 0.5681 (0.5508)	
training:	Epoch: [13][83/204]	Loss 0.6267 (0.5517)	
training:	Epoch: [13][84/204]	Loss 0.5155 (0.5513)	
training:	Epoch: [13][85/204]	Loss 0.5507 (0.5513)	
training:	Epoch: [13][86/204]	Loss 0.5214 (0.5510)	
training:	Epoch: [13][87/204]	Loss 0.4225 (0.5495)	
training:	Epoch: [13][88/204]	Loss 0.4883 (0.5488)	
training:	Epoch: [13][89/204]	Loss 0.6625 (0.5501)	
training:	Epoch: [13][90/204]	Loss 0.6101 (0.5507)	
training:	Epoch: [13][91/204]	Loss 0.5573 (0.5508)	
training:	Epoch: [13][92/204]	Loss 0.5826 (0.5511)	
training:	Epoch: [13][93/204]	Loss 0.4932 (0.5505)	
training:	Epoch: [13][94/204]	Loss 0.5208 (0.5502)	
training:	Epoch: [13][95/204]	Loss 0.4781 (0.5494)	
training:	Epoch: [13][96/204]	Loss 0.4840 (0.5488)	
training:	Epoch: [13][97/204]	Loss 0.5413 (0.5487)	
training:	Epoch: [13][98/204]	Loss 0.6915 (0.5501)	
training:	Epoch: [13][99/204]	Loss 0.4511 (0.5491)	
training:	Epoch: [13][100/204]	Loss 0.5258 (0.5489)	
training:	Epoch: [13][101/204]	Loss 0.5008 (0.5484)	
training:	Epoch: [13][102/204]	Loss 0.6270 (0.5492)	
training:	Epoch: [13][103/204]	Loss 0.5099 (0.5488)	
training:	Epoch: [13][104/204]	Loss 0.5426 (0.5488)	
training:	Epoch: [13][105/204]	Loss 0.4876 (0.5482)	
training:	Epoch: [13][106/204]	Loss 0.5934 (0.5486)	
training:	Epoch: [13][107/204]	Loss 0.6088 (0.5492)	
training:	Epoch: [13][108/204]	Loss 0.5046 (0.5488)	
training:	Epoch: [13][109/204]	Loss 0.5404 (0.5487)	
training:	Epoch: [13][110/204]	Loss 0.5310 (0.5485)	
training:	Epoch: [13][111/204]	Loss 0.6340 (0.5493)	
training:	Epoch: [13][112/204]	Loss 0.6365 (0.5501)	
training:	Epoch: [13][113/204]	Loss 0.5492 (0.5501)	
training:	Epoch: [13][114/204]	Loss 0.5629 (0.5502)	
training:	Epoch: [13][115/204]	Loss 0.5849 (0.5505)	
training:	Epoch: [13][116/204]	Loss 0.6723 (0.5515)	
training:	Epoch: [13][117/204]	Loss 0.4715 (0.5508)	
training:	Epoch: [13][118/204]	Loss 0.6306 (0.5515)	
training:	Epoch: [13][119/204]	Loss 0.6559 (0.5524)	
training:	Epoch: [13][120/204]	Loss 0.5478 (0.5524)	
training:	Epoch: [13][121/204]	Loss 0.5797 (0.5526)	
training:	Epoch: [13][122/204]	Loss 0.5076 (0.5522)	
training:	Epoch: [13][123/204]	Loss 0.5379 (0.5521)	
training:	Epoch: [13][124/204]	Loss 0.4356 (0.5512)	
training:	Epoch: [13][125/204]	Loss 0.5447 (0.5511)	
training:	Epoch: [13][126/204]	Loss 0.6027 (0.5515)	
training:	Epoch: [13][127/204]	Loss 0.5806 (0.5517)	
training:	Epoch: [13][128/204]	Loss 0.5259 (0.5515)	
training:	Epoch: [13][129/204]	Loss 0.4291 (0.5506)	
training:	Epoch: [13][130/204]	Loss 0.5085 (0.5503)	
training:	Epoch: [13][131/204]	Loss 0.6483 (0.5510)	
training:	Epoch: [13][132/204]	Loss 0.5130 (0.5507)	
training:	Epoch: [13][133/204]	Loss 0.5513 (0.5507)	
training:	Epoch: [13][134/204]	Loss 0.5332 (0.5506)	
training:	Epoch: [13][135/204]	Loss 0.5653 (0.5507)	
training:	Epoch: [13][136/204]	Loss 0.4880 (0.5503)	
training:	Epoch: [13][137/204]	Loss 0.6431 (0.5509)	
training:	Epoch: [13][138/204]	Loss 0.5788 (0.5511)	
training:	Epoch: [13][139/204]	Loss 0.4433 (0.5504)	
training:	Epoch: [13][140/204]	Loss 0.5887 (0.5506)	
training:	Epoch: [13][141/204]	Loss 0.6357 (0.5512)	
training:	Epoch: [13][142/204]	Loss 0.5550 (0.5513)	
training:	Epoch: [13][143/204]	Loss 0.5637 (0.5513)	
training:	Epoch: [13][144/204]	Loss 0.4401 (0.5506)	
training:	Epoch: [13][145/204]	Loss 0.4875 (0.5501)	
training:	Epoch: [13][146/204]	Loss 0.5725 (0.5503)	
training:	Epoch: [13][147/204]	Loss 0.5114 (0.5500)	
training:	Epoch: [13][148/204]	Loss 0.4771 (0.5495)	
training:	Epoch: [13][149/204]	Loss 0.5824 (0.5498)	
training:	Epoch: [13][150/204]	Loss 0.4927 (0.5494)	
training:	Epoch: [13][151/204]	Loss 0.5336 (0.5493)	
training:	Epoch: [13][152/204]	Loss 0.5206 (0.5491)	
training:	Epoch: [13][153/204]	Loss 0.3805 (0.5480)	
training:	Epoch: [13][154/204]	Loss 0.5062 (0.5477)	
training:	Epoch: [13][155/204]	Loss 0.6078 (0.5481)	
training:	Epoch: [13][156/204]	Loss 0.4776 (0.5476)	
training:	Epoch: [13][157/204]	Loss 0.5465 (0.5476)	
training:	Epoch: [13][158/204]	Loss 0.5210 (0.5475)	
training:	Epoch: [13][159/204]	Loss 0.5496 (0.5475)	
training:	Epoch: [13][160/204]	Loss 0.5273 (0.5474)	
training:	Epoch: [13][161/204]	Loss 0.6045 (0.5477)	
training:	Epoch: [13][162/204]	Loss 0.5587 (0.5478)	
training:	Epoch: [13][163/204]	Loss 0.5052 (0.5475)	
training:	Epoch: [13][164/204]	Loss 0.6076 (0.5479)	
training:	Epoch: [13][165/204]	Loss 0.6627 (0.5486)	
training:	Epoch: [13][166/204]	Loss 0.5458 (0.5486)	
training:	Epoch: [13][167/204]	Loss 0.6305 (0.5491)	
training:	Epoch: [13][168/204]	Loss 0.4642 (0.5485)	
training:	Epoch: [13][169/204]	Loss 0.5391 (0.5485)	
training:	Epoch: [13][170/204]	Loss 0.5504 (0.5485)	
training:	Epoch: [13][171/204]	Loss 0.7264 (0.5495)	
training:	Epoch: [13][172/204]	Loss 0.6263 (0.5500)	
training:	Epoch: [13][173/204]	Loss 0.5315 (0.5499)	
training:	Epoch: [13][174/204]	Loss 0.5181 (0.5497)	
training:	Epoch: [13][175/204]	Loss 0.5866 (0.5499)	
training:	Epoch: [13][176/204]	Loss 0.6506 (0.5505)	
training:	Epoch: [13][177/204]	Loss 0.4990 (0.5502)	
training:	Epoch: [13][178/204]	Loss 0.7012 (0.5510)	
training:	Epoch: [13][179/204]	Loss 0.5384 (0.5510)	
training:	Epoch: [13][180/204]	Loss 0.5639 (0.5510)	
training:	Epoch: [13][181/204]	Loss 0.4907 (0.5507)	
training:	Epoch: [13][182/204]	Loss 0.6278 (0.5511)	
training:	Epoch: [13][183/204]	Loss 0.5183 (0.5510)	
training:	Epoch: [13][184/204]	Loss 0.4964 (0.5507)	
training:	Epoch: [13][185/204]	Loss 0.5467 (0.5506)	
training:	Epoch: [13][186/204]	Loss 0.5213 (0.5505)	
training:	Epoch: [13][187/204]	Loss 0.5423 (0.5504)	
training:	Epoch: [13][188/204]	Loss 0.5940 (0.5507)	
training:	Epoch: [13][189/204]	Loss 0.4799 (0.5503)	
training:	Epoch: [13][190/204]	Loss 0.5601 (0.5503)	
training:	Epoch: [13][191/204]	Loss 0.5277 (0.5502)	
training:	Epoch: [13][192/204]	Loss 0.5603 (0.5503)	
training:	Epoch: [13][193/204]	Loss 0.5599 (0.5503)	
training:	Epoch: [13][194/204]	Loss 0.5883 (0.5505)	
training:	Epoch: [13][195/204]	Loss 0.4919 (0.5502)	
training:	Epoch: [13][196/204]	Loss 0.6302 (0.5506)	
training:	Epoch: [13][197/204]	Loss 0.4845 (0.5503)	
training:	Epoch: [13][198/204]	Loss 0.4882 (0.5500)	
training:	Epoch: [13][199/204]	Loss 0.6181 (0.5503)	
training:	Epoch: [13][200/204]	Loss 0.4731 (0.5499)	
training:	Epoch: [13][201/204]	Loss 0.6057 (0.5502)	
training:	Epoch: [13][202/204]	Loss 0.4530 (0.5497)	
training:	Epoch: [13][203/204]	Loss 0.5095 (0.5495)	
training:	Epoch: [13][204/204]	Loss 0.5643 (0.5496)	
Training:	 Loss: 0.5488

Training:	 ACC: 0.7329 0.7342 0.7646 0.7012
Validation:	 ACC: 0.7163 0.7186 0.7656 0.6670
Validation:	 Best_BACC: 0.7163 0.7186 0.7656 0.6670
Validation:	 Loss: 0.5589
Pretraining:	Epoch 14/200
----------
training:	Epoch: [14][1/204]	Loss 0.5026 (0.5026)	
training:	Epoch: [14][2/204]	Loss 0.4232 (0.4629)	
training:	Epoch: [14][3/204]	Loss 0.5046 (0.4768)	
training:	Epoch: [14][4/204]	Loss 0.4735 (0.4760)	
training:	Epoch: [14][5/204]	Loss 0.6664 (0.5141)	
training:	Epoch: [14][6/204]	Loss 0.3889 (0.4932)	
training:	Epoch: [14][7/204]	Loss 0.6677 (0.5181)	
training:	Epoch: [14][8/204]	Loss 0.6018 (0.5286)	
training:	Epoch: [14][9/204]	Loss 0.4369 (0.5184)	
training:	Epoch: [14][10/204]	Loss 0.5616 (0.5227)	
training:	Epoch: [14][11/204]	Loss 0.5108 (0.5216)	
training:	Epoch: [14][12/204]	Loss 0.4382 (0.5147)	
training:	Epoch: [14][13/204]	Loss 0.4743 (0.5116)	
training:	Epoch: [14][14/204]	Loss 0.4237 (0.5053)	
training:	Epoch: [14][15/204]	Loss 0.5573 (0.5088)	
training:	Epoch: [14][16/204]	Loss 0.4834 (0.5072)	
training:	Epoch: [14][17/204]	Loss 0.4631 (0.5046)	
training:	Epoch: [14][18/204]	Loss 0.7307 (0.5172)	
training:	Epoch: [14][19/204]	Loss 0.4510 (0.5137)	
training:	Epoch: [14][20/204]	Loss 0.4349 (0.5097)	
training:	Epoch: [14][21/204]	Loss 0.4892 (0.5088)	
training:	Epoch: [14][22/204]	Loss 0.6720 (0.5162)	
training:	Epoch: [14][23/204]	Loss 0.5419 (0.5173)	
training:	Epoch: [14][24/204]	Loss 0.4725 (0.5154)	
training:	Epoch: [14][25/204]	Loss 0.5051 (0.5150)	
training:	Epoch: [14][26/204]	Loss 0.5489 (0.5163)	
training:	Epoch: [14][27/204]	Loss 0.5485 (0.5175)	
training:	Epoch: [14][28/204]	Loss 0.5176 (0.5175)	
training:	Epoch: [14][29/204]	Loss 0.6395 (0.5217)	
training:	Epoch: [14][30/204]	Loss 0.5085 (0.5213)	
training:	Epoch: [14][31/204]	Loss 0.3797 (0.5167)	
training:	Epoch: [14][32/204]	Loss 0.4864 (0.5158)	
training:	Epoch: [14][33/204]	Loss 0.5250 (0.5160)	
training:	Epoch: [14][34/204]	Loss 0.7143 (0.5219)	
training:	Epoch: [14][35/204]	Loss 0.3918 (0.5182)	
training:	Epoch: [14][36/204]	Loss 0.5169 (0.5181)	
training:	Epoch: [14][37/204]	Loss 0.6153 (0.5208)	
training:	Epoch: [14][38/204]	Loss 0.4076 (0.5178)	
training:	Epoch: [14][39/204]	Loss 0.5222 (0.5179)	
training:	Epoch: [14][40/204]	Loss 0.4959 (0.5173)	
training:	Epoch: [14][41/204]	Loss 0.5872 (0.5190)	
training:	Epoch: [14][42/204]	Loss 0.5058 (0.5187)	
training:	Epoch: [14][43/204]	Loss 0.6319 (0.5214)	
training:	Epoch: [14][44/204]	Loss 0.4578 (0.5199)	
training:	Epoch: [14][45/204]	Loss 0.6086 (0.5219)	
training:	Epoch: [14][46/204]	Loss 0.6616 (0.5249)	
training:	Epoch: [14][47/204]	Loss 0.5174 (0.5248)	
training:	Epoch: [14][48/204]	Loss 0.6576 (0.5275)	
training:	Epoch: [14][49/204]	Loss 0.5638 (0.5283)	
training:	Epoch: [14][50/204]	Loss 0.5524 (0.5288)	
training:	Epoch: [14][51/204]	Loss 0.5978 (0.5301)	
training:	Epoch: [14][52/204]	Loss 0.5606 (0.5307)	
training:	Epoch: [14][53/204]	Loss 0.5683 (0.5314)	
training:	Epoch: [14][54/204]	Loss 0.5739 (0.5322)	
training:	Epoch: [14][55/204]	Loss 0.5684 (0.5328)	
training:	Epoch: [14][56/204]	Loss 0.4951 (0.5322)	
training:	Epoch: [14][57/204]	Loss 0.6272 (0.5338)	
training:	Epoch: [14][58/204]	Loss 0.5251 (0.5337)	
training:	Epoch: [14][59/204]	Loss 0.4703 (0.5326)	
training:	Epoch: [14][60/204]	Loss 0.5659 (0.5332)	
training:	Epoch: [14][61/204]	Loss 0.5721 (0.5338)	
training:	Epoch: [14][62/204]	Loss 0.6576 (0.5358)	
training:	Epoch: [14][63/204]	Loss 0.6280 (0.5373)	
training:	Epoch: [14][64/204]	Loss 0.6471 (0.5390)	
training:	Epoch: [14][65/204]	Loss 0.6449 (0.5406)	
training:	Epoch: [14][66/204]	Loss 0.4883 (0.5398)	
training:	Epoch: [14][67/204]	Loss 0.6538 (0.5415)	
training:	Epoch: [14][68/204]	Loss 0.5806 (0.5421)	
training:	Epoch: [14][69/204]	Loss 0.7183 (0.5447)	
training:	Epoch: [14][70/204]	Loss 0.5536 (0.5448)	
training:	Epoch: [14][71/204]	Loss 0.7252 (0.5473)	
training:	Epoch: [14][72/204]	Loss 0.5915 (0.5479)	
training:	Epoch: [14][73/204]	Loss 0.5031 (0.5473)	
training:	Epoch: [14][74/204]	Loss 0.6485 (0.5487)	
training:	Epoch: [14][75/204]	Loss 0.4948 (0.5480)	
training:	Epoch: [14][76/204]	Loss 0.4749 (0.5470)	
training:	Epoch: [14][77/204]	Loss 0.5058 (0.5465)	
training:	Epoch: [14][78/204]	Loss 0.5968 (0.5471)	
training:	Epoch: [14][79/204]	Loss 0.6297 (0.5482)	
training:	Epoch: [14][80/204]	Loss 0.5105 (0.5477)	
training:	Epoch: [14][81/204]	Loss 0.5080 (0.5472)	
training:	Epoch: [14][82/204]	Loss 0.4967 (0.5466)	
training:	Epoch: [14][83/204]	Loss 0.5049 (0.5461)	
training:	Epoch: [14][84/204]	Loss 0.5478 (0.5461)	
training:	Epoch: [14][85/204]	Loss 0.4673 (0.5452)	
training:	Epoch: [14][86/204]	Loss 0.5569 (0.5453)	
training:	Epoch: [14][87/204]	Loss 0.6101 (0.5461)	
training:	Epoch: [14][88/204]	Loss 0.4523 (0.5450)	
training:	Epoch: [14][89/204]	Loss 0.5773 (0.5454)	
training:	Epoch: [14][90/204]	Loss 0.5304 (0.5452)	
training:	Epoch: [14][91/204]	Loss 0.5471 (0.5452)	
training:	Epoch: [14][92/204]	Loss 0.4771 (0.5445)	
training:	Epoch: [14][93/204]	Loss 0.3934 (0.5428)	
training:	Epoch: [14][94/204]	Loss 0.5715 (0.5432)	
training:	Epoch: [14][95/204]	Loss 0.4235 (0.5419)	
training:	Epoch: [14][96/204]	Loss 0.5231 (0.5417)	
training:	Epoch: [14][97/204]	Loss 0.5734 (0.5420)	
training:	Epoch: [14][98/204]	Loss 0.4405 (0.5410)	
training:	Epoch: [14][99/204]	Loss 0.4702 (0.5403)	
training:	Epoch: [14][100/204]	Loss 0.5218 (0.5401)	
training:	Epoch: [14][101/204]	Loss 0.5188 (0.5399)	
training:	Epoch: [14][102/204]	Loss 0.3695 (0.5382)	
training:	Epoch: [14][103/204]	Loss 0.5782 (0.5386)	
training:	Epoch: [14][104/204]	Loss 0.5586 (0.5388)	
training:	Epoch: [14][105/204]	Loss 0.5205 (0.5386)	
training:	Epoch: [14][106/204]	Loss 0.6005 (0.5392)	
training:	Epoch: [14][107/204]	Loss 0.5084 (0.5389)	
training:	Epoch: [14][108/204]	Loss 0.4640 (0.5382)	
training:	Epoch: [14][109/204]	Loss 0.5422 (0.5383)	
training:	Epoch: [14][110/204]	Loss 0.5812 (0.5386)	
training:	Epoch: [14][111/204]	Loss 0.4642 (0.5380)	
training:	Epoch: [14][112/204]	Loss 0.5302 (0.5379)	
training:	Epoch: [14][113/204]	Loss 0.5253 (0.5378)	
training:	Epoch: [14][114/204]	Loss 0.4290 (0.5368)	
training:	Epoch: [14][115/204]	Loss 0.5940 (0.5373)	
training:	Epoch: [14][116/204]	Loss 0.4867 (0.5369)	
training:	Epoch: [14][117/204]	Loss 0.4788 (0.5364)	
training:	Epoch: [14][118/204]	Loss 0.5993 (0.5369)	
training:	Epoch: [14][119/204]	Loss 0.5035 (0.5367)	
training:	Epoch: [14][120/204]	Loss 0.6825 (0.5379)	
training:	Epoch: [14][121/204]	Loss 0.5085 (0.5376)	
training:	Epoch: [14][122/204]	Loss 0.4269 (0.5367)	
training:	Epoch: [14][123/204]	Loss 0.5439 (0.5368)	
training:	Epoch: [14][124/204]	Loss 0.4303 (0.5359)	
training:	Epoch: [14][125/204]	Loss 0.6175 (0.5366)	
training:	Epoch: [14][126/204]	Loss 0.5662 (0.5368)	
training:	Epoch: [14][127/204]	Loss 0.5994 (0.5373)	
training:	Epoch: [14][128/204]	Loss 0.4911 (0.5369)	
training:	Epoch: [14][129/204]	Loss 0.6498 (0.5378)	
training:	Epoch: [14][130/204]	Loss 0.6984 (0.5390)	
training:	Epoch: [14][131/204]	Loss 0.6495 (0.5399)	
training:	Epoch: [14][132/204]	Loss 0.5197 (0.5397)	
training:	Epoch: [14][133/204]	Loss 0.5916 (0.5401)	
training:	Epoch: [14][134/204]	Loss 0.5368 (0.5401)	
training:	Epoch: [14][135/204]	Loss 0.4626 (0.5395)	
training:	Epoch: [14][136/204]	Loss 0.6082 (0.5400)	
training:	Epoch: [14][137/204]	Loss 0.6389 (0.5408)	
training:	Epoch: [14][138/204]	Loss 0.4938 (0.5404)	
training:	Epoch: [14][139/204]	Loss 0.6184 (0.5410)	
training:	Epoch: [14][140/204]	Loss 0.6588 (0.5418)	
training:	Epoch: [14][141/204]	Loss 0.6870 (0.5428)	
training:	Epoch: [14][142/204]	Loss 0.4912 (0.5425)	
training:	Epoch: [14][143/204]	Loss 0.4682 (0.5420)	
training:	Epoch: [14][144/204]	Loss 0.5336 (0.5419)	
training:	Epoch: [14][145/204]	Loss 0.5108 (0.5417)	
training:	Epoch: [14][146/204]	Loss 0.5081 (0.5415)	
training:	Epoch: [14][147/204]	Loss 0.6276 (0.5420)	
training:	Epoch: [14][148/204]	Loss 0.7360 (0.5434)	
training:	Epoch: [14][149/204]	Loss 0.5067 (0.5431)	
training:	Epoch: [14][150/204]	Loss 0.4831 (0.5427)	
training:	Epoch: [14][151/204]	Loss 0.5417 (0.5427)	
training:	Epoch: [14][152/204]	Loss 0.7866 (0.5443)	
training:	Epoch: [14][153/204]	Loss 0.5242 (0.5442)	
training:	Epoch: [14][154/204]	Loss 0.6107 (0.5446)	
training:	Epoch: [14][155/204]	Loss 0.4549 (0.5440)	
training:	Epoch: [14][156/204]	Loss 0.5458 (0.5440)	
training:	Epoch: [14][157/204]	Loss 0.6137 (0.5445)	
training:	Epoch: [14][158/204]	Loss 0.7627 (0.5459)	
training:	Epoch: [14][159/204]	Loss 0.5188 (0.5457)	
training:	Epoch: [14][160/204]	Loss 0.5675 (0.5458)	
training:	Epoch: [14][161/204]	Loss 0.5141 (0.5456)	
training:	Epoch: [14][162/204]	Loss 0.5246 (0.5455)	
training:	Epoch: [14][163/204]	Loss 0.5591 (0.5456)	
training:	Epoch: [14][164/204]	Loss 0.5388 (0.5456)	
training:	Epoch: [14][165/204]	Loss 0.5231 (0.5454)	
training:	Epoch: [14][166/204]	Loss 0.5436 (0.5454)	
training:	Epoch: [14][167/204]	Loss 0.5205 (0.5453)	
training:	Epoch: [14][168/204]	Loss 0.5043 (0.5450)	
training:	Epoch: [14][169/204]	Loss 0.4930 (0.5447)	
training:	Epoch: [14][170/204]	Loss 0.5638 (0.5448)	
training:	Epoch: [14][171/204]	Loss 0.5965 (0.5451)	
training:	Epoch: [14][172/204]	Loss 0.4903 (0.5448)	
training:	Epoch: [14][173/204]	Loss 0.6162 (0.5452)	
training:	Epoch: [14][174/204]	Loss 0.5340 (0.5451)	
training:	Epoch: [14][175/204]	Loss 0.4676 (0.5447)	
training:	Epoch: [14][176/204]	Loss 0.6021 (0.5450)	
training:	Epoch: [14][177/204]	Loss 0.4891 (0.5447)	
training:	Epoch: [14][178/204]	Loss 0.4491 (0.5442)	
training:	Epoch: [14][179/204]	Loss 0.5496 (0.5442)	
training:	Epoch: [14][180/204]	Loss 0.5092 (0.5440)	
training:	Epoch: [14][181/204]	Loss 0.5880 (0.5443)	
training:	Epoch: [14][182/204]	Loss 0.6028 (0.5446)	
training:	Epoch: [14][183/204]	Loss 0.4139 (0.5439)	
training:	Epoch: [14][184/204]	Loss 0.5020 (0.5436)	
training:	Epoch: [14][185/204]	Loss 0.5914 (0.5439)	
training:	Epoch: [14][186/204]	Loss 0.5026 (0.5437)	
training:	Epoch: [14][187/204]	Loss 0.5650 (0.5438)	
training:	Epoch: [14][188/204]	Loss 0.4982 (0.5435)	
training:	Epoch: [14][189/204]	Loss 0.5164 (0.5434)	
training:	Epoch: [14][190/204]	Loss 0.5574 (0.5435)	
training:	Epoch: [14][191/204]	Loss 0.4750 (0.5431)	
training:	Epoch: [14][192/204]	Loss 0.6358 (0.5436)	
training:	Epoch: [14][193/204]	Loss 0.5357 (0.5436)	
training:	Epoch: [14][194/204]	Loss 0.5268 (0.5435)	
training:	Epoch: [14][195/204]	Loss 0.5122 (0.5433)	
training:	Epoch: [14][196/204]	Loss 0.4691 (0.5429)	
training:	Epoch: [14][197/204]	Loss 0.6694 (0.5436)	
training:	Epoch: [14][198/204]	Loss 0.4638 (0.5432)	
training:	Epoch: [14][199/204]	Loss 0.5971 (0.5434)	
training:	Epoch: [14][200/204]	Loss 0.6071 (0.5438)	
training:	Epoch: [14][201/204]	Loss 0.4578 (0.5433)	
training:	Epoch: [14][202/204]	Loss 0.4701 (0.5430)	
training:	Epoch: [14][203/204]	Loss 0.5377 (0.5429)	
training:	Epoch: [14][204/204]	Loss 0.6054 (0.5433)	
Training:	 Loss: 0.5424

Training:	 ACC: 0.7357 0.7374 0.7778 0.6936
Validation:	 ACC: 0.7242 0.7271 0.7892 0.6592
Validation:	 Best_BACC: 0.7242 0.7271 0.7892 0.6592
Validation:	 Loss: 0.5544
Pretraining:	Epoch 15/200
----------
training:	Epoch: [15][1/204]	Loss 0.5402 (0.5402)	
training:	Epoch: [15][2/204]	Loss 0.5618 (0.5510)	
training:	Epoch: [15][3/204]	Loss 0.6519 (0.5846)	
training:	Epoch: [15][4/204]	Loss 0.6612 (0.6038)	
training:	Epoch: [15][5/204]	Loss 0.5933 (0.6017)	
training:	Epoch: [15][6/204]	Loss 0.6255 (0.6056)	
training:	Epoch: [15][7/204]	Loss 0.5288 (0.5947)	
training:	Epoch: [15][8/204]	Loss 0.5214 (0.5855)	
training:	Epoch: [15][9/204]	Loss 0.6480 (0.5925)	
training:	Epoch: [15][10/204]	Loss 0.5345 (0.5867)	
training:	Epoch: [15][11/204]	Loss 0.5383 (0.5823)	
training:	Epoch: [15][12/204]	Loss 0.5616 (0.5805)	
training:	Epoch: [15][13/204]	Loss 0.5586 (0.5789)	
training:	Epoch: [15][14/204]	Loss 0.4867 (0.5723)	
training:	Epoch: [15][15/204]	Loss 0.5417 (0.5702)	
training:	Epoch: [15][16/204]	Loss 0.5768 (0.5707)	
training:	Epoch: [15][17/204]	Loss 0.4373 (0.5628)	
training:	Epoch: [15][18/204]	Loss 0.5172 (0.5603)	
training:	Epoch: [15][19/204]	Loss 0.6221 (0.5635)	
training:	Epoch: [15][20/204]	Loss 0.6112 (0.5659)	
training:	Epoch: [15][21/204]	Loss 0.3929 (0.5577)	
training:	Epoch: [15][22/204]	Loss 0.6221 (0.5606)	
training:	Epoch: [15][23/204]	Loss 0.5729 (0.5611)	
training:	Epoch: [15][24/204]	Loss 0.4796 (0.5577)	
training:	Epoch: [15][25/204]	Loss 0.6138 (0.5600)	
training:	Epoch: [15][26/204]	Loss 0.5474 (0.5595)	
training:	Epoch: [15][27/204]	Loss 0.4339 (0.5548)	
training:	Epoch: [15][28/204]	Loss 0.4771 (0.5521)	
training:	Epoch: [15][29/204]	Loss 0.5979 (0.5536)	
training:	Epoch: [15][30/204]	Loss 0.4934 (0.5516)	
training:	Epoch: [15][31/204]	Loss 0.5101 (0.5503)	
training:	Epoch: [15][32/204]	Loss 0.5375 (0.5499)	
training:	Epoch: [15][33/204]	Loss 0.6639 (0.5534)	
training:	Epoch: [15][34/204]	Loss 0.5691 (0.5538)	
training:	Epoch: [15][35/204]	Loss 0.4787 (0.5517)	
training:	Epoch: [15][36/204]	Loss 0.4491 (0.5488)	
training:	Epoch: [15][37/204]	Loss 0.5643 (0.5492)	
training:	Epoch: [15][38/204]	Loss 0.5896 (0.5503)	
training:	Epoch: [15][39/204]	Loss 0.4681 (0.5482)	
training:	Epoch: [15][40/204]	Loss 0.5841 (0.5491)	
training:	Epoch: [15][41/204]	Loss 0.5395 (0.5489)	
training:	Epoch: [15][42/204]	Loss 0.4236 (0.5459)	
training:	Epoch: [15][43/204]	Loss 0.5942 (0.5470)	
training:	Epoch: [15][44/204]	Loss 0.5512 (0.5471)	
training:	Epoch: [15][45/204]	Loss 0.4762 (0.5455)	
training:	Epoch: [15][46/204]	Loss 0.3755 (0.5418)	
training:	Epoch: [15][47/204]	Loss 0.5203 (0.5414)	
training:	Epoch: [15][48/204]	Loss 0.5679 (0.5419)	
training:	Epoch: [15][49/204]	Loss 0.5547 (0.5422)	
training:	Epoch: [15][50/204]	Loss 0.5789 (0.5429)	
training:	Epoch: [15][51/204]	Loss 0.4750 (0.5416)	
training:	Epoch: [15][52/204]	Loss 0.5785 (0.5423)	
training:	Epoch: [15][53/204]	Loss 0.6127 (0.5436)	
training:	Epoch: [15][54/204]	Loss 0.5837 (0.5444)	
training:	Epoch: [15][55/204]	Loss 0.4646 (0.5429)	
training:	Epoch: [15][56/204]	Loss 0.4779 (0.5417)	
training:	Epoch: [15][57/204]	Loss 0.4467 (0.5401)	
training:	Epoch: [15][58/204]	Loss 0.6484 (0.5419)	
training:	Epoch: [15][59/204]	Loss 0.5473 (0.5420)	
training:	Epoch: [15][60/204]	Loss 0.6001 (0.5430)	
training:	Epoch: [15][61/204]	Loss 0.5535 (0.5432)	
training:	Epoch: [15][62/204]	Loss 0.5736 (0.5437)	
training:	Epoch: [15][63/204]	Loss 0.6517 (0.5454)	
training:	Epoch: [15][64/204]	Loss 0.5328 (0.5452)	
training:	Epoch: [15][65/204]	Loss 0.4528 (0.5438)	
training:	Epoch: [15][66/204]	Loss 0.4266 (0.5420)	
training:	Epoch: [15][67/204]	Loss 0.5952 (0.5428)	
training:	Epoch: [15][68/204]	Loss 0.4496 (0.5414)	
training:	Epoch: [15][69/204]	Loss 0.5982 (0.5422)	
training:	Epoch: [15][70/204]	Loss 0.6261 (0.5434)	
training:	Epoch: [15][71/204]	Loss 0.4496 (0.5421)	
training:	Epoch: [15][72/204]	Loss 0.5374 (0.5420)	
training:	Epoch: [15][73/204]	Loss 0.4542 (0.5408)	
training:	Epoch: [15][74/204]	Loss 0.5421 (0.5409)	
training:	Epoch: [15][75/204]	Loss 0.5234 (0.5406)	
training:	Epoch: [15][76/204]	Loss 0.5671 (0.5410)	
training:	Epoch: [15][77/204]	Loss 0.4263 (0.5395)	
training:	Epoch: [15][78/204]	Loss 0.4992 (0.5390)	
training:	Epoch: [15][79/204]	Loss 0.5287 (0.5388)	
training:	Epoch: [15][80/204]	Loss 0.5508 (0.5390)	
training:	Epoch: [15][81/204]	Loss 0.4509 (0.5379)	
training:	Epoch: [15][82/204]	Loss 0.6108 (0.5388)	
training:	Epoch: [15][83/204]	Loss 0.4636 (0.5379)	
training:	Epoch: [15][84/204]	Loss 0.5293 (0.5378)	
training:	Epoch: [15][85/204]	Loss 0.4741 (0.5370)	
training:	Epoch: [15][86/204]	Loss 0.4371 (0.5359)	
training:	Epoch: [15][87/204]	Loss 0.5622 (0.5362)	
training:	Epoch: [15][88/204]	Loss 0.5599 (0.5364)	
training:	Epoch: [15][89/204]	Loss 0.6115 (0.5373)	
training:	Epoch: [15][90/204]	Loss 0.5152 (0.5370)	
training:	Epoch: [15][91/204]	Loss 0.6263 (0.5380)	
training:	Epoch: [15][92/204]	Loss 0.5860 (0.5385)	
training:	Epoch: [15][93/204]	Loss 0.4383 (0.5375)	
training:	Epoch: [15][94/204]	Loss 0.5507 (0.5376)	
training:	Epoch: [15][95/204]	Loss 0.5856 (0.5381)	
training:	Epoch: [15][96/204]	Loss 0.4915 (0.5376)	
training:	Epoch: [15][97/204]	Loss 0.5629 (0.5379)	
training:	Epoch: [15][98/204]	Loss 0.6217 (0.5387)	
training:	Epoch: [15][99/204]	Loss 0.3835 (0.5372)	
training:	Epoch: [15][100/204]	Loss 0.7048 (0.5389)	
training:	Epoch: [15][101/204]	Loss 0.4644 (0.5381)	
training:	Epoch: [15][102/204]	Loss 0.4436 (0.5372)	
training:	Epoch: [15][103/204]	Loss 0.4719 (0.5366)	
training:	Epoch: [15][104/204]	Loss 0.4589 (0.5358)	
training:	Epoch: [15][105/204]	Loss 0.5656 (0.5361)	
training:	Epoch: [15][106/204]	Loss 0.4690 (0.5355)	
training:	Epoch: [15][107/204]	Loss 0.4182 (0.5344)	
training:	Epoch: [15][108/204]	Loss 0.5658 (0.5347)	
training:	Epoch: [15][109/204]	Loss 0.6576 (0.5358)	
training:	Epoch: [15][110/204]	Loss 0.4587 (0.5351)	
training:	Epoch: [15][111/204]	Loss 0.5211 (0.5350)	
training:	Epoch: [15][112/204]	Loss 0.6157 (0.5357)	
training:	Epoch: [15][113/204]	Loss 0.5903 (0.5362)	
training:	Epoch: [15][114/204]	Loss 0.5913 (0.5366)	
training:	Epoch: [15][115/204]	Loss 0.5325 (0.5366)	
training:	Epoch: [15][116/204]	Loss 0.5699 (0.5369)	
training:	Epoch: [15][117/204]	Loss 0.4666 (0.5363)	
training:	Epoch: [15][118/204]	Loss 0.6028 (0.5369)	
training:	Epoch: [15][119/204]	Loss 0.5891 (0.5373)	
training:	Epoch: [15][120/204]	Loss 0.5862 (0.5377)	
training:	Epoch: [15][121/204]	Loss 0.3955 (0.5365)	
training:	Epoch: [15][122/204]	Loss 0.4995 (0.5362)	
training:	Epoch: [15][123/204]	Loss 0.5250 (0.5361)	
training:	Epoch: [15][124/204]	Loss 0.5748 (0.5364)	
training:	Epoch: [15][125/204]	Loss 0.6057 (0.5370)	
training:	Epoch: [15][126/204]	Loss 0.4832 (0.5366)	
training:	Epoch: [15][127/204]	Loss 0.5366 (0.5366)	
training:	Epoch: [15][128/204]	Loss 0.6105 (0.5372)	
training:	Epoch: [15][129/204]	Loss 0.5487 (0.5372)	
training:	Epoch: [15][130/204]	Loss 0.5017 (0.5370)	
training:	Epoch: [15][131/204]	Loss 0.5522 (0.5371)	
training:	Epoch: [15][132/204]	Loss 0.5488 (0.5372)	
training:	Epoch: [15][133/204]	Loss 0.6120 (0.5377)	
training:	Epoch: [15][134/204]	Loss 0.5380 (0.5377)	
training:	Epoch: [15][135/204]	Loss 0.4566 (0.5371)	
training:	Epoch: [15][136/204]	Loss 0.4205 (0.5363)	
training:	Epoch: [15][137/204]	Loss 0.5605 (0.5365)	
training:	Epoch: [15][138/204]	Loss 0.3854 (0.5354)	
training:	Epoch: [15][139/204]	Loss 0.6222 (0.5360)	
training:	Epoch: [15][140/204]	Loss 0.5818 (0.5363)	
training:	Epoch: [15][141/204]	Loss 0.4628 (0.5358)	
training:	Epoch: [15][142/204]	Loss 0.5205 (0.5357)	
training:	Epoch: [15][143/204]	Loss 0.6771 (0.5367)	
training:	Epoch: [15][144/204]	Loss 0.6725 (0.5376)	
training:	Epoch: [15][145/204]	Loss 0.5471 (0.5377)	
training:	Epoch: [15][146/204]	Loss 0.6004 (0.5381)	
training:	Epoch: [15][147/204]	Loss 0.4301 (0.5374)	
training:	Epoch: [15][148/204]	Loss 0.6531 (0.5382)	
training:	Epoch: [15][149/204]	Loss 0.3732 (0.5371)	
training:	Epoch: [15][150/204]	Loss 0.4769 (0.5366)	
training:	Epoch: [15][151/204]	Loss 0.4781 (0.5363)	
training:	Epoch: [15][152/204]	Loss 0.6295 (0.5369)	
training:	Epoch: [15][153/204]	Loss 0.3482 (0.5356)	
training:	Epoch: [15][154/204]	Loss 0.4918 (0.5354)	
training:	Epoch: [15][155/204]	Loss 0.3958 (0.5345)	
training:	Epoch: [15][156/204]	Loss 0.4611 (0.5340)	
training:	Epoch: [15][157/204]	Loss 0.5493 (0.5341)	
training:	Epoch: [15][158/204]	Loss 0.4932 (0.5338)	
training:	Epoch: [15][159/204]	Loss 0.6167 (0.5343)	
training:	Epoch: [15][160/204]	Loss 0.4529 (0.5338)	
training:	Epoch: [15][161/204]	Loss 0.5016 (0.5336)	
training:	Epoch: [15][162/204]	Loss 0.5755 (0.5339)	
training:	Epoch: [15][163/204]	Loss 0.5812 (0.5342)	
training:	Epoch: [15][164/204]	Loss 0.4728 (0.5338)	
training:	Epoch: [15][165/204]	Loss 0.4999 (0.5336)	
training:	Epoch: [15][166/204]	Loss 0.4791 (0.5333)	
training:	Epoch: [15][167/204]	Loss 0.7311 (0.5345)	
training:	Epoch: [15][168/204]	Loss 0.5626 (0.5346)	
training:	Epoch: [15][169/204]	Loss 0.5118 (0.5345)	
training:	Epoch: [15][170/204]	Loss 0.4460 (0.5340)	
training:	Epoch: [15][171/204]	Loss 0.5256 (0.5339)	
training:	Epoch: [15][172/204]	Loss 0.6146 (0.5344)	
training:	Epoch: [15][173/204]	Loss 0.5132 (0.5343)	
training:	Epoch: [15][174/204]	Loss 0.5359 (0.5343)	
training:	Epoch: [15][175/204]	Loss 0.5135 (0.5342)	
training:	Epoch: [15][176/204]	Loss 0.5978 (0.5345)	
training:	Epoch: [15][177/204]	Loss 0.5250 (0.5345)	
training:	Epoch: [15][178/204]	Loss 0.4086 (0.5338)	
training:	Epoch: [15][179/204]	Loss 0.4734 (0.5334)	
training:	Epoch: [15][180/204]	Loss 0.6059 (0.5338)	
training:	Epoch: [15][181/204]	Loss 0.6703 (0.5346)	
training:	Epoch: [15][182/204]	Loss 0.4591 (0.5342)	
training:	Epoch: [15][183/204]	Loss 0.4432 (0.5337)	
training:	Epoch: [15][184/204]	Loss 0.4342 (0.5331)	
training:	Epoch: [15][185/204]	Loss 0.4028 (0.5324)	
training:	Epoch: [15][186/204]	Loss 0.7749 (0.5337)	
training:	Epoch: [15][187/204]	Loss 0.5039 (0.5336)	
training:	Epoch: [15][188/204]	Loss 0.4635 (0.5332)	
training:	Epoch: [15][189/204]	Loss 0.7804 (0.5345)	
training:	Epoch: [15][190/204]	Loss 0.5240 (0.5344)	
training:	Epoch: [15][191/204]	Loss 0.4710 (0.5341)	
training:	Epoch: [15][192/204]	Loss 0.4331 (0.5336)	
training:	Epoch: [15][193/204]	Loss 0.5714 (0.5338)	
training:	Epoch: [15][194/204]	Loss 0.6464 (0.5344)	
training:	Epoch: [15][195/204]	Loss 0.5740 (0.5346)	
training:	Epoch: [15][196/204]	Loss 0.5216 (0.5345)	
training:	Epoch: [15][197/204]	Loss 0.5069 (0.5344)	
training:	Epoch: [15][198/204]	Loss 0.5690 (0.5345)	
training:	Epoch: [15][199/204]	Loss 0.5579 (0.5347)	
training:	Epoch: [15][200/204]	Loss 0.5299 (0.5346)	
training:	Epoch: [15][201/204]	Loss 0.5412 (0.5347)	
training:	Epoch: [15][202/204]	Loss 0.4774 (0.5344)	
training:	Epoch: [15][203/204]	Loss 0.5874 (0.5346)	
training:	Epoch: [15][204/204]	Loss 0.4448 (0.5342)	
Training:	 Loss: 0.5334

Training:	 ACC: 0.7467 0.7467 0.7472 0.7462
Validation:	 ACC: 0.7272 0.7277 0.7380 0.7164
Validation:	 Best_BACC: 0.7272 0.7277 0.7380 0.7164
Validation:	 Loss: 0.5486
Pretraining:	Epoch 16/200
----------
training:	Epoch: [16][1/204]	Loss 0.5478 (0.5478)	
training:	Epoch: [16][2/204]	Loss 0.4847 (0.5163)	
training:	Epoch: [16][3/204]	Loss 0.5689 (0.5338)	
training:	Epoch: [16][4/204]	Loss 0.5324 (0.5334)	
training:	Epoch: [16][5/204]	Loss 0.3946 (0.5057)	
training:	Epoch: [16][6/204]	Loss 0.4047 (0.4888)	
training:	Epoch: [16][7/204]	Loss 0.5104 (0.4919)	
training:	Epoch: [16][8/204]	Loss 0.5812 (0.5031)	
training:	Epoch: [16][9/204]	Loss 0.6095 (0.5149)	
training:	Epoch: [16][10/204]	Loss 0.5742 (0.5208)	
training:	Epoch: [16][11/204]	Loss 0.6233 (0.5302)	
training:	Epoch: [16][12/204]	Loss 0.5182 (0.5292)	
training:	Epoch: [16][13/204]	Loss 0.5355 (0.5296)	
training:	Epoch: [16][14/204]	Loss 0.5159 (0.5287)	
training:	Epoch: [16][15/204]	Loss 0.3892 (0.5194)	
training:	Epoch: [16][16/204]	Loss 0.4972 (0.5180)	
training:	Epoch: [16][17/204]	Loss 0.4485 (0.5139)	
training:	Epoch: [16][18/204]	Loss 0.5365 (0.5151)	
training:	Epoch: [16][19/204]	Loss 0.5506 (0.5170)	
training:	Epoch: [16][20/204]	Loss 0.5345 (0.5179)	
training:	Epoch: [16][21/204]	Loss 0.5772 (0.5207)	
training:	Epoch: [16][22/204]	Loss 0.7561 (0.5314)	
training:	Epoch: [16][23/204]	Loss 0.5970 (0.5343)	
training:	Epoch: [16][24/204]	Loss 0.4174 (0.5294)	
training:	Epoch: [16][25/204]	Loss 0.5251 (0.5292)	
training:	Epoch: [16][26/204]	Loss 0.5527 (0.5301)	
training:	Epoch: [16][27/204]	Loss 0.3772 (0.5245)	
training:	Epoch: [16][28/204]	Loss 0.6828 (0.5301)	
training:	Epoch: [16][29/204]	Loss 0.5328 (0.5302)	
training:	Epoch: [16][30/204]	Loss 0.4331 (0.5270)	
training:	Epoch: [16][31/204]	Loss 0.4568 (0.5247)	
training:	Epoch: [16][32/204]	Loss 0.3678 (0.5198)	
training:	Epoch: [16][33/204]	Loss 0.4301 (0.5171)	
training:	Epoch: [16][34/204]	Loss 0.4887 (0.5162)	
training:	Epoch: [16][35/204]	Loss 0.5322 (0.5167)	
training:	Epoch: [16][36/204]	Loss 0.6268 (0.5198)	
training:	Epoch: [16][37/204]	Loss 0.4986 (0.5192)	
training:	Epoch: [16][38/204]	Loss 0.4771 (0.5181)	
training:	Epoch: [16][39/204]	Loss 0.4865 (0.5173)	
training:	Epoch: [16][40/204]	Loss 0.3236 (0.5124)	
training:	Epoch: [16][41/204]	Loss 0.5762 (0.5140)	
training:	Epoch: [16][42/204]	Loss 0.5089 (0.5139)	
training:	Epoch: [16][43/204]	Loss 0.6147 (0.5162)	
training:	Epoch: [16][44/204]	Loss 0.4428 (0.5145)	
training:	Epoch: [16][45/204]	Loss 0.6936 (0.5185)	
training:	Epoch: [16][46/204]	Loss 0.5120 (0.5184)	
training:	Epoch: [16][47/204]	Loss 0.6142 (0.5204)	
training:	Epoch: [16][48/204]	Loss 0.6745 (0.5236)	
training:	Epoch: [16][49/204]	Loss 0.6517 (0.5262)	
training:	Epoch: [16][50/204]	Loss 0.5395 (0.5265)	
training:	Epoch: [16][51/204]	Loss 0.3862 (0.5238)	
training:	Epoch: [16][52/204]	Loss 0.4859 (0.5230)	
training:	Epoch: [16][53/204]	Loss 0.4339 (0.5213)	
training:	Epoch: [16][54/204]	Loss 0.5580 (0.5220)	
training:	Epoch: [16][55/204]	Loss 0.5540 (0.5226)	
training:	Epoch: [16][56/204]	Loss 0.4178 (0.5207)	
training:	Epoch: [16][57/204]	Loss 0.5057 (0.5205)	
training:	Epoch: [16][58/204]	Loss 0.5147 (0.5204)	
training:	Epoch: [16][59/204]	Loss 0.5343 (0.5206)	
training:	Epoch: [16][60/204]	Loss 0.5583 (0.5212)	
training:	Epoch: [16][61/204]	Loss 0.4742 (0.5205)	
training:	Epoch: [16][62/204]	Loss 0.5505 (0.5209)	
training:	Epoch: [16][63/204]	Loss 0.5806 (0.5219)	
training:	Epoch: [16][64/204]	Loss 0.4764 (0.5212)	
training:	Epoch: [16][65/204]	Loss 0.4137 (0.5195)	
training:	Epoch: [16][66/204]	Loss 0.6292 (0.5212)	
training:	Epoch: [16][67/204]	Loss 0.4879 (0.5207)	
training:	Epoch: [16][68/204]	Loss 0.4988 (0.5204)	
training:	Epoch: [16][69/204]	Loss 0.7140 (0.5232)	
training:	Epoch: [16][70/204]	Loss 0.4690 (0.5224)	
training:	Epoch: [16][71/204]	Loss 0.4878 (0.5219)	
training:	Epoch: [16][72/204]	Loss 0.5755 (0.5227)	
training:	Epoch: [16][73/204]	Loss 0.4571 (0.5218)	
training:	Epoch: [16][74/204]	Loss 0.6023 (0.5229)	
training:	Epoch: [16][75/204]	Loss 0.4668 (0.5221)	
training:	Epoch: [16][76/204]	Loss 0.5354 (0.5223)	
training:	Epoch: [16][77/204]	Loss 0.4932 (0.5219)	
training:	Epoch: [16][78/204]	Loss 0.4140 (0.5205)	
training:	Epoch: [16][79/204]	Loss 0.5060 (0.5203)	
training:	Epoch: [16][80/204]	Loss 0.4765 (0.5198)	
training:	Epoch: [16][81/204]	Loss 0.6007 (0.5208)	
training:	Epoch: [16][82/204]	Loss 0.5460 (0.5211)	
training:	Epoch: [16][83/204]	Loss 0.5694 (0.5217)	
training:	Epoch: [16][84/204]	Loss 0.3793 (0.5200)	
training:	Epoch: [16][85/204]	Loss 0.4612 (0.5193)	
training:	Epoch: [16][86/204]	Loss 0.4671 (0.5187)	
training:	Epoch: [16][87/204]	Loss 0.5923 (0.5195)	
training:	Epoch: [16][88/204]	Loss 0.5211 (0.5195)	
training:	Epoch: [16][89/204]	Loss 0.6358 (0.5209)	
training:	Epoch: [16][90/204]	Loss 0.5070 (0.5207)	
training:	Epoch: [16][91/204]	Loss 0.4184 (0.5196)	
training:	Epoch: [16][92/204]	Loss 0.4791 (0.5191)	
training:	Epoch: [16][93/204]	Loss 0.5629 (0.5196)	
training:	Epoch: [16][94/204]	Loss 0.5128 (0.5195)	
training:	Epoch: [16][95/204]	Loss 0.5250 (0.5196)	
training:	Epoch: [16][96/204]	Loss 0.5223 (0.5196)	
training:	Epoch: [16][97/204]	Loss 0.6099 (0.5206)	
training:	Epoch: [16][98/204]	Loss 0.6431 (0.5218)	
training:	Epoch: [16][99/204]	Loss 0.4712 (0.5213)	
training:	Epoch: [16][100/204]	Loss 0.4857 (0.5209)	
training:	Epoch: [16][101/204]	Loss 0.5521 (0.5212)	
training:	Epoch: [16][102/204]	Loss 0.5073 (0.5211)	
training:	Epoch: [16][103/204]	Loss 0.4944 (0.5208)	
training:	Epoch: [16][104/204]	Loss 0.5337 (0.5210)	
training:	Epoch: [16][105/204]	Loss 0.6704 (0.5224)	
training:	Epoch: [16][106/204]	Loss 0.5484 (0.5226)	
training:	Epoch: [16][107/204]	Loss 0.5684 (0.5231)	
training:	Epoch: [16][108/204]	Loss 0.4266 (0.5222)	
training:	Epoch: [16][109/204]	Loss 0.5341 (0.5223)	
training:	Epoch: [16][110/204]	Loss 0.4729 (0.5218)	
training:	Epoch: [16][111/204]	Loss 0.4753 (0.5214)	
training:	Epoch: [16][112/204]	Loss 0.5977 (0.5221)	
training:	Epoch: [16][113/204]	Loss 0.4634 (0.5216)	
training:	Epoch: [16][114/204]	Loss 0.5990 (0.5223)	
training:	Epoch: [16][115/204]	Loss 0.6221 (0.5231)	
training:	Epoch: [16][116/204]	Loss 0.6357 (0.5241)	
training:	Epoch: [16][117/204]	Loss 0.5320 (0.5242)	
training:	Epoch: [16][118/204]	Loss 0.6019 (0.5248)	
training:	Epoch: [16][119/204]	Loss 0.5395 (0.5249)	
training:	Epoch: [16][120/204]	Loss 0.4909 (0.5247)	
training:	Epoch: [16][121/204]	Loss 0.4479 (0.5240)	
training:	Epoch: [16][122/204]	Loss 0.5674 (0.5244)	
training:	Epoch: [16][123/204]	Loss 0.5165 (0.5243)	
training:	Epoch: [16][124/204]	Loss 0.5944 (0.5249)	
training:	Epoch: [16][125/204]	Loss 0.4928 (0.5246)	
training:	Epoch: [16][126/204]	Loss 0.5182 (0.5246)	
training:	Epoch: [16][127/204]	Loss 0.4675 (0.5241)	
training:	Epoch: [16][128/204]	Loss 0.6245 (0.5249)	
training:	Epoch: [16][129/204]	Loss 0.6194 (0.5256)	
training:	Epoch: [16][130/204]	Loss 0.5689 (0.5260)	
training:	Epoch: [16][131/204]	Loss 0.5752 (0.5264)	
training:	Epoch: [16][132/204]	Loss 0.5091 (0.5262)	
training:	Epoch: [16][133/204]	Loss 0.6137 (0.5269)	
training:	Epoch: [16][134/204]	Loss 0.5311 (0.5269)	
training:	Epoch: [16][135/204]	Loss 0.4940 (0.5267)	
training:	Epoch: [16][136/204]	Loss 0.5716 (0.5270)	
training:	Epoch: [16][137/204]	Loss 0.4699 (0.5266)	
training:	Epoch: [16][138/204]	Loss 0.5159 (0.5265)	
training:	Epoch: [16][139/204]	Loss 0.6970 (0.5277)	
training:	Epoch: [16][140/204]	Loss 0.5026 (0.5276)	
training:	Epoch: [16][141/204]	Loss 0.5520 (0.5277)	
training:	Epoch: [16][142/204]	Loss 0.3879 (0.5267)	
training:	Epoch: [16][143/204]	Loss 0.5909 (0.5272)	
training:	Epoch: [16][144/204]	Loss 0.5876 (0.5276)	
training:	Epoch: [16][145/204]	Loss 0.5231 (0.5276)	
training:	Epoch: [16][146/204]	Loss 0.5423 (0.5277)	
training:	Epoch: [16][147/204]	Loss 0.3926 (0.5268)	
training:	Epoch: [16][148/204]	Loss 0.5657 (0.5270)	
training:	Epoch: [16][149/204]	Loss 0.5612 (0.5273)	
training:	Epoch: [16][150/204]	Loss 0.4685 (0.5269)	
training:	Epoch: [16][151/204]	Loss 0.4216 (0.5262)	
training:	Epoch: [16][152/204]	Loss 0.4597 (0.5257)	
training:	Epoch: [16][153/204]	Loss 0.4954 (0.5255)	
training:	Epoch: [16][154/204]	Loss 0.4989 (0.5254)	
training:	Epoch: [16][155/204]	Loss 0.5280 (0.5254)	
training:	Epoch: [16][156/204]	Loss 0.5030 (0.5252)	
training:	Epoch: [16][157/204]	Loss 0.6460 (0.5260)	
training:	Epoch: [16][158/204]	Loss 0.5828 (0.5264)	
training:	Epoch: [16][159/204]	Loss 0.3980 (0.5255)	
training:	Epoch: [16][160/204]	Loss 0.4515 (0.5251)	
training:	Epoch: [16][161/204]	Loss 0.6326 (0.5258)	
training:	Epoch: [16][162/204]	Loss 0.4571 (0.5253)	
training:	Epoch: [16][163/204]	Loss 0.5784 (0.5257)	
training:	Epoch: [16][164/204]	Loss 0.5654 (0.5259)	
training:	Epoch: [16][165/204]	Loss 0.5902 (0.5263)	
training:	Epoch: [16][166/204]	Loss 0.5955 (0.5267)	
training:	Epoch: [16][167/204]	Loss 0.4228 (0.5261)	
training:	Epoch: [16][168/204]	Loss 0.5052 (0.5260)	
training:	Epoch: [16][169/204]	Loss 0.5748 (0.5262)	
training:	Epoch: [16][170/204]	Loss 0.5978 (0.5267)	
training:	Epoch: [16][171/204]	Loss 0.4415 (0.5262)	
training:	Epoch: [16][172/204]	Loss 0.5790 (0.5265)	
training:	Epoch: [16][173/204]	Loss 0.5945 (0.5269)	
training:	Epoch: [16][174/204]	Loss 0.5086 (0.5268)	
training:	Epoch: [16][175/204]	Loss 0.4862 (0.5265)	
training:	Epoch: [16][176/204]	Loss 0.4794 (0.5263)	
training:	Epoch: [16][177/204]	Loss 0.4799 (0.5260)	
training:	Epoch: [16][178/204]	Loss 0.6699 (0.5268)	
training:	Epoch: [16][179/204]	Loss 0.6172 (0.5273)	
training:	Epoch: [16][180/204]	Loss 0.7179 (0.5284)	
training:	Epoch: [16][181/204]	Loss 0.4903 (0.5282)	
training:	Epoch: [16][182/204]	Loss 0.5569 (0.5283)	
training:	Epoch: [16][183/204]	Loss 0.4511 (0.5279)	
training:	Epoch: [16][184/204]	Loss 0.6053 (0.5283)	
training:	Epoch: [16][185/204]	Loss 0.4981 (0.5282)	
training:	Epoch: [16][186/204]	Loss 0.3879 (0.5274)	
training:	Epoch: [16][187/204]	Loss 0.6268 (0.5279)	
training:	Epoch: [16][188/204]	Loss 0.5619 (0.5281)	
training:	Epoch: [16][189/204]	Loss 0.4549 (0.5277)	
training:	Epoch: [16][190/204]	Loss 0.5004 (0.5276)	
training:	Epoch: [16][191/204]	Loss 0.4932 (0.5274)	
training:	Epoch: [16][192/204]	Loss 0.4852 (0.5272)	
training:	Epoch: [16][193/204]	Loss 0.6526 (0.5278)	
training:	Epoch: [16][194/204]	Loss 0.5897 (0.5282)	
training:	Epoch: [16][195/204]	Loss 0.5686 (0.5284)	
training:	Epoch: [16][196/204]	Loss 0.4706 (0.5281)	
training:	Epoch: [16][197/204]	Loss 0.5442 (0.5281)	
training:	Epoch: [16][198/204]	Loss 0.5732 (0.5284)	
training:	Epoch: [16][199/204]	Loss 0.5051 (0.5283)	
training:	Epoch: [16][200/204]	Loss 0.6862 (0.5290)	
training:	Epoch: [16][201/204]	Loss 0.4100 (0.5285)	
training:	Epoch: [16][202/204]	Loss 0.5744 (0.5287)	
training:	Epoch: [16][203/204]	Loss 0.4708 (0.5284)	
training:	Epoch: [16][204/204]	Loss 0.5263 (0.5284)	
Training:	 Loss: 0.5276

Training:	 ACC: 0.7491 0.7505 0.7840 0.7143
Validation:	 ACC: 0.7259 0.7282 0.7758 0.6760
Validation:	 Best_BACC: 0.7272 0.7277 0.7380 0.7164
Validation:	 Loss: 0.5450
Pretraining:	Epoch 17/200
----------
training:	Epoch: [17][1/204]	Loss 0.5003 (0.5003)	
training:	Epoch: [17][2/204]	Loss 0.4060 (0.4531)	
training:	Epoch: [17][3/204]	Loss 0.4859 (0.4641)	
training:	Epoch: [17][4/204]	Loss 0.4030 (0.4488)	
training:	Epoch: [17][5/204]	Loss 0.5738 (0.4738)	
training:	Epoch: [17][6/204]	Loss 0.5630 (0.4887)	
training:	Epoch: [17][7/204]	Loss 0.5661 (0.4997)	
training:	Epoch: [17][8/204]	Loss 0.3910 (0.4861)	
training:	Epoch: [17][9/204]	Loss 0.4884 (0.4864)	
training:	Epoch: [17][10/204]	Loss 0.4515 (0.4829)	
training:	Epoch: [17][11/204]	Loss 0.4652 (0.4813)	
training:	Epoch: [17][12/204]	Loss 0.5103 (0.4837)	
training:	Epoch: [17][13/204]	Loss 0.5297 (0.4872)	
training:	Epoch: [17][14/204]	Loss 0.5481 (0.4916)	
training:	Epoch: [17][15/204]	Loss 0.6464 (0.5019)	
training:	Epoch: [17][16/204]	Loss 0.4093 (0.4961)	
training:	Epoch: [17][17/204]	Loss 0.5512 (0.4994)	
training:	Epoch: [17][18/204]	Loss 0.5364 (0.5014)	
training:	Epoch: [17][19/204]	Loss 0.4357 (0.4980)	
training:	Epoch: [17][20/204]	Loss 0.4933 (0.4977)	
training:	Epoch: [17][21/204]	Loss 0.4084 (0.4935)	
training:	Epoch: [17][22/204]	Loss 0.4399 (0.4910)	
training:	Epoch: [17][23/204]	Loss 0.6088 (0.4962)	
training:	Epoch: [17][24/204]	Loss 0.5659 (0.4991)	
training:	Epoch: [17][25/204]	Loss 0.4675 (0.4978)	
training:	Epoch: [17][26/204]	Loss 0.5688 (0.5005)	
training:	Epoch: [17][27/204]	Loss 0.5197 (0.5012)	
training:	Epoch: [17][28/204]	Loss 0.5705 (0.5037)	
training:	Epoch: [17][29/204]	Loss 0.6434 (0.5085)	
training:	Epoch: [17][30/204]	Loss 0.4508 (0.5066)	
training:	Epoch: [17][31/204]	Loss 0.6826 (0.5123)	
training:	Epoch: [17][32/204]	Loss 0.5127 (0.5123)	
training:	Epoch: [17][33/204]	Loss 0.4208 (0.5095)	
training:	Epoch: [17][34/204]	Loss 0.5968 (0.5121)	
training:	Epoch: [17][35/204]	Loss 0.4213 (0.5095)	
training:	Epoch: [17][36/204]	Loss 0.6417 (0.5132)	
training:	Epoch: [17][37/204]	Loss 0.5582 (0.5144)	
training:	Epoch: [17][38/204]	Loss 0.4396 (0.5124)	
training:	Epoch: [17][39/204]	Loss 0.4836 (0.5117)	
training:	Epoch: [17][40/204]	Loss 0.4887 (0.5111)	
training:	Epoch: [17][41/204]	Loss 0.4748 (0.5102)	
training:	Epoch: [17][42/204]	Loss 0.5615 (0.5114)	
training:	Epoch: [17][43/204]	Loss 0.4886 (0.5109)	
training:	Epoch: [17][44/204]	Loss 0.4187 (0.5088)	
training:	Epoch: [17][45/204]	Loss 0.5146 (0.5089)	
training:	Epoch: [17][46/204]	Loss 0.6146 (0.5112)	
training:	Epoch: [17][47/204]	Loss 0.3620 (0.5081)	
training:	Epoch: [17][48/204]	Loss 0.5373 (0.5087)	
training:	Epoch: [17][49/204]	Loss 0.4155 (0.5068)	
training:	Epoch: [17][50/204]	Loss 0.4906 (0.5064)	
training:	Epoch: [17][51/204]	Loss 0.5821 (0.5079)	
training:	Epoch: [17][52/204]	Loss 0.6810 (0.5113)	
training:	Epoch: [17][53/204]	Loss 0.4147 (0.5094)	
training:	Epoch: [17][54/204]	Loss 0.5681 (0.5105)	
training:	Epoch: [17][55/204]	Loss 0.6178 (0.5125)	
training:	Epoch: [17][56/204]	Loss 0.4922 (0.5121)	
training:	Epoch: [17][57/204]	Loss 0.5021 (0.5119)	
training:	Epoch: [17][58/204]	Loss 0.4368 (0.5106)	
training:	Epoch: [17][59/204]	Loss 0.4309 (0.5093)	
training:	Epoch: [17][60/204]	Loss 0.4827 (0.5088)	
training:	Epoch: [17][61/204]	Loss 0.4434 (0.5078)	
training:	Epoch: [17][62/204]	Loss 0.4298 (0.5065)	
training:	Epoch: [17][63/204]	Loss 0.5383 (0.5070)	
training:	Epoch: [17][64/204]	Loss 0.6755 (0.5097)	
training:	Epoch: [17][65/204]	Loss 0.4555 (0.5088)	
training:	Epoch: [17][66/204]	Loss 0.4064 (0.5073)	
training:	Epoch: [17][67/204]	Loss 0.5593 (0.5080)	
training:	Epoch: [17][68/204]	Loss 0.5167 (0.5082)	
training:	Epoch: [17][69/204]	Loss 0.4829 (0.5078)	
training:	Epoch: [17][70/204]	Loss 0.6429 (0.5097)	
training:	Epoch: [17][71/204]	Loss 0.5332 (0.5101)	
training:	Epoch: [17][72/204]	Loss 0.5481 (0.5106)	
training:	Epoch: [17][73/204]	Loss 0.5316 (0.5109)	
training:	Epoch: [17][74/204]	Loss 0.5963 (0.5120)	
training:	Epoch: [17][75/204]	Loss 0.6133 (0.5134)	
training:	Epoch: [17][76/204]	Loss 0.5124 (0.5134)	
training:	Epoch: [17][77/204]	Loss 0.5288 (0.5136)	
training:	Epoch: [17][78/204]	Loss 0.6178 (0.5149)	
training:	Epoch: [17][79/204]	Loss 0.5233 (0.5150)	
training:	Epoch: [17][80/204]	Loss 0.4605 (0.5143)	
training:	Epoch: [17][81/204]	Loss 0.5231 (0.5144)	
training:	Epoch: [17][82/204]	Loss 0.7251 (0.5170)	
training:	Epoch: [17][83/204]	Loss 0.4830 (0.5166)	
training:	Epoch: [17][84/204]	Loss 0.5380 (0.5169)	
training:	Epoch: [17][85/204]	Loss 0.4988 (0.5166)	
training:	Epoch: [17][86/204]	Loss 0.6798 (0.5185)	
training:	Epoch: [17][87/204]	Loss 0.5220 (0.5186)	
training:	Epoch: [17][88/204]	Loss 0.4578 (0.5179)	
training:	Epoch: [17][89/204]	Loss 0.5392 (0.5181)	
training:	Epoch: [17][90/204]	Loss 0.4777 (0.5177)	
training:	Epoch: [17][91/204]	Loss 0.4856 (0.5173)	
training:	Epoch: [17][92/204]	Loss 0.6446 (0.5187)	
training:	Epoch: [17][93/204]	Loss 0.6202 (0.5198)	
training:	Epoch: [17][94/204]	Loss 0.5681 (0.5203)	
training:	Epoch: [17][95/204]	Loss 0.4302 (0.5194)	
training:	Epoch: [17][96/204]	Loss 0.6342 (0.5206)	
training:	Epoch: [17][97/204]	Loss 0.5332 (0.5207)	
training:	Epoch: [17][98/204]	Loss 0.6429 (0.5219)	
training:	Epoch: [17][99/204]	Loss 0.5847 (0.5226)	
training:	Epoch: [17][100/204]	Loss 0.5834 (0.5232)	
training:	Epoch: [17][101/204]	Loss 0.5518 (0.5235)	
training:	Epoch: [17][102/204]	Loss 0.5371 (0.5236)	
training:	Epoch: [17][103/204]	Loss 0.4755 (0.5231)	
training:	Epoch: [17][104/204]	Loss 0.5090 (0.5230)	
training:	Epoch: [17][105/204]	Loss 0.4837 (0.5226)	
training:	Epoch: [17][106/204]	Loss 0.4496 (0.5219)	
training:	Epoch: [17][107/204]	Loss 0.4369 (0.5211)	
training:	Epoch: [17][108/204]	Loss 0.5882 (0.5218)	
training:	Epoch: [17][109/204]	Loss 0.4616 (0.5212)	
training:	Epoch: [17][110/204]	Loss 0.4326 (0.5204)	
training:	Epoch: [17][111/204]	Loss 0.4687 (0.5199)	
training:	Epoch: [17][112/204]	Loss 0.5702 (0.5204)	
training:	Epoch: [17][113/204]	Loss 0.4565 (0.5198)	
training:	Epoch: [17][114/204]	Loss 0.4563 (0.5193)	
training:	Epoch: [17][115/204]	Loss 0.5618 (0.5196)	
training:	Epoch: [17][116/204]	Loss 0.5371 (0.5198)	
training:	Epoch: [17][117/204]	Loss 0.5217 (0.5198)	
training:	Epoch: [17][118/204]	Loss 0.4626 (0.5193)	
training:	Epoch: [17][119/204]	Loss 0.4759 (0.5190)	
training:	Epoch: [17][120/204]	Loss 0.5792 (0.5195)	
training:	Epoch: [17][121/204]	Loss 0.4984 (0.5193)	
training:	Epoch: [17][122/204]	Loss 0.4193 (0.5185)	
training:	Epoch: [17][123/204]	Loss 0.5473 (0.5187)	
training:	Epoch: [17][124/204]	Loss 0.4402 (0.5181)	
training:	Epoch: [17][125/204]	Loss 0.6251 (0.5189)	
training:	Epoch: [17][126/204]	Loss 0.5587 (0.5192)	
training:	Epoch: [17][127/204]	Loss 0.5268 (0.5193)	
training:	Epoch: [17][128/204]	Loss 0.6083 (0.5200)	
training:	Epoch: [17][129/204]	Loss 0.3382 (0.5186)	
training:	Epoch: [17][130/204]	Loss 0.7144 (0.5201)	
training:	Epoch: [17][131/204]	Loss 0.6375 (0.5210)	
training:	Epoch: [17][132/204]	Loss 0.5533 (0.5212)	
training:	Epoch: [17][133/204]	Loss 0.5687 (0.5216)	
training:	Epoch: [17][134/204]	Loss 0.5249 (0.5216)	
training:	Epoch: [17][135/204]	Loss 0.4505 (0.5211)	
training:	Epoch: [17][136/204]	Loss 0.6392 (0.5220)	
training:	Epoch: [17][137/204]	Loss 0.5133 (0.5219)	
training:	Epoch: [17][138/204]	Loss 0.5077 (0.5218)	
training:	Epoch: [17][139/204]	Loss 0.5340 (0.5219)	
training:	Epoch: [17][140/204]	Loss 0.5694 (0.5222)	
training:	Epoch: [17][141/204]	Loss 0.5462 (0.5224)	
training:	Epoch: [17][142/204]	Loss 0.6014 (0.5229)	
training:	Epoch: [17][143/204]	Loss 0.4468 (0.5224)	
training:	Epoch: [17][144/204]	Loss 0.5546 (0.5226)	
training:	Epoch: [17][145/204]	Loss 0.4580 (0.5222)	
training:	Epoch: [17][146/204]	Loss 0.5381 (0.5223)	
training:	Epoch: [17][147/204]	Loss 0.4473 (0.5218)	
training:	Epoch: [17][148/204]	Loss 0.3873 (0.5209)	
training:	Epoch: [17][149/204]	Loss 0.6038 (0.5214)	
training:	Epoch: [17][150/204]	Loss 0.4013 (0.5206)	
training:	Epoch: [17][151/204]	Loss 0.3855 (0.5197)	
training:	Epoch: [17][152/204]	Loss 0.6250 (0.5204)	
training:	Epoch: [17][153/204]	Loss 0.5585 (0.5207)	
training:	Epoch: [17][154/204]	Loss 0.4799 (0.5204)	
training:	Epoch: [17][155/204]	Loss 0.4445 (0.5199)	
training:	Epoch: [17][156/204]	Loss 0.4354 (0.5194)	
training:	Epoch: [17][157/204]	Loss 0.5680 (0.5197)	
training:	Epoch: [17][158/204]	Loss 0.6117 (0.5203)	
training:	Epoch: [17][159/204]	Loss 0.5799 (0.5206)	
training:	Epoch: [17][160/204]	Loss 0.5035 (0.5205)	
training:	Epoch: [17][161/204]	Loss 0.5496 (0.5207)	
training:	Epoch: [17][162/204]	Loss 0.4480 (0.5203)	
training:	Epoch: [17][163/204]	Loss 0.5178 (0.5203)	
training:	Epoch: [17][164/204]	Loss 0.4866 (0.5201)	
training:	Epoch: [17][165/204]	Loss 0.4187 (0.5194)	
training:	Epoch: [17][166/204]	Loss 0.4283 (0.5189)	
training:	Epoch: [17][167/204]	Loss 0.3907 (0.5181)	
training:	Epoch: [17][168/204]	Loss 0.7994 (0.5198)	
training:	Epoch: [17][169/204]	Loss 0.5024 (0.5197)	
training:	Epoch: [17][170/204]	Loss 0.5178 (0.5197)	
training:	Epoch: [17][171/204]	Loss 0.4567 (0.5193)	
training:	Epoch: [17][172/204]	Loss 0.5065 (0.5192)	
training:	Epoch: [17][173/204]	Loss 0.4655 (0.5189)	
training:	Epoch: [17][174/204]	Loss 0.5740 (0.5192)	
training:	Epoch: [17][175/204]	Loss 0.5128 (0.5192)	
training:	Epoch: [17][176/204]	Loss 0.4147 (0.5186)	
training:	Epoch: [17][177/204]	Loss 0.4792 (0.5184)	
training:	Epoch: [17][178/204]	Loss 0.6739 (0.5193)	
training:	Epoch: [17][179/204]	Loss 0.4970 (0.5191)	
training:	Epoch: [17][180/204]	Loss 0.4859 (0.5190)	
training:	Epoch: [17][181/204]	Loss 0.4645 (0.5187)	
training:	Epoch: [17][182/204]	Loss 0.4654 (0.5184)	
training:	Epoch: [17][183/204]	Loss 0.4843 (0.5182)	
training:	Epoch: [17][184/204]	Loss 0.6449 (0.5189)	
training:	Epoch: [17][185/204]	Loss 0.4903 (0.5187)	
training:	Epoch: [17][186/204]	Loss 0.5074 (0.5186)	
training:	Epoch: [17][187/204]	Loss 0.5155 (0.5186)	
training:	Epoch: [17][188/204]	Loss 0.7317 (0.5198)	
training:	Epoch: [17][189/204]	Loss 0.6387 (0.5204)	
training:	Epoch: [17][190/204]	Loss 0.4940 (0.5203)	
training:	Epoch: [17][191/204]	Loss 0.5234 (0.5203)	
training:	Epoch: [17][192/204]	Loss 0.4595 (0.5200)	
training:	Epoch: [17][193/204]	Loss 0.6160 (0.5205)	
training:	Epoch: [17][194/204]	Loss 0.6514 (0.5211)	
training:	Epoch: [17][195/204]	Loss 0.6594 (0.5218)	
training:	Epoch: [17][196/204]	Loss 0.4845 (0.5216)	
training:	Epoch: [17][197/204]	Loss 0.5371 (0.5217)	
training:	Epoch: [17][198/204]	Loss 0.5643 (0.5219)	
training:	Epoch: [17][199/204]	Loss 0.4418 (0.5215)	
training:	Epoch: [17][200/204]	Loss 0.5907 (0.5219)	
training:	Epoch: [17][201/204]	Loss 0.4376 (0.5215)	
training:	Epoch: [17][202/204]	Loss 0.4996 (0.5214)	
training:	Epoch: [17][203/204]	Loss 0.5767 (0.5216)	
training:	Epoch: [17][204/204]	Loss 0.4933 (0.5215)	
Training:	 Loss: 0.5207

Training:	 ACC: 0.7554 0.7570 0.7942 0.7165
Validation:	 ACC: 0.7278 0.7303 0.7840 0.6715
Validation:	 Best_BACC: 0.7278 0.7303 0.7840 0.6715
Validation:	 Loss: 0.5422
Pretraining:	Epoch 18/200
----------
training:	Epoch: [18][1/204]	Loss 0.6322 (0.6322)	
training:	Epoch: [18][2/204]	Loss 0.5549 (0.5935)	
training:	Epoch: [18][3/204]	Loss 0.5695 (0.5855)	
training:	Epoch: [18][4/204]	Loss 0.4628 (0.5548)	
training:	Epoch: [18][5/204]	Loss 0.4699 (0.5379)	
training:	Epoch: [18][6/204]	Loss 0.5267 (0.5360)	
training:	Epoch: [18][7/204]	Loss 0.4347 (0.5215)	
training:	Epoch: [18][8/204]	Loss 0.3982 (0.5061)	
training:	Epoch: [18][9/204]	Loss 0.5186 (0.5075)	
training:	Epoch: [18][10/204]	Loss 0.4983 (0.5066)	
training:	Epoch: [18][11/204]	Loss 0.4641 (0.5027)	
training:	Epoch: [18][12/204]	Loss 0.4838 (0.5011)	
training:	Epoch: [18][13/204]	Loss 0.5114 (0.5019)	
training:	Epoch: [18][14/204]	Loss 0.5857 (0.5079)	
training:	Epoch: [18][15/204]	Loss 0.5026 (0.5076)	
training:	Epoch: [18][16/204]	Loss 0.5229 (0.5085)	
training:	Epoch: [18][17/204]	Loss 0.6107 (0.5145)	
training:	Epoch: [18][18/204]	Loss 0.5285 (0.5153)	
training:	Epoch: [18][19/204]	Loss 0.5320 (0.5162)	
training:	Epoch: [18][20/204]	Loss 0.4570 (0.5132)	
training:	Epoch: [18][21/204]	Loss 0.4499 (0.5102)	
training:	Epoch: [18][22/204]	Loss 0.5113 (0.5103)	
training:	Epoch: [18][23/204]	Loss 0.5455 (0.5118)	
training:	Epoch: [18][24/204]	Loss 0.5832 (0.5148)	
training:	Epoch: [18][25/204]	Loss 0.5418 (0.5159)	
training:	Epoch: [18][26/204]	Loss 0.4519 (0.5134)	
training:	Epoch: [18][27/204]	Loss 0.5256 (0.5138)	
training:	Epoch: [18][28/204]	Loss 0.4905 (0.5130)	
training:	Epoch: [18][29/204]	Loss 0.5163 (0.5131)	
training:	Epoch: [18][30/204]	Loss 0.3514 (0.5077)	
training:	Epoch: [18][31/204]	Loss 0.4669 (0.5064)	
training:	Epoch: [18][32/204]	Loss 0.6454 (0.5108)	
training:	Epoch: [18][33/204]	Loss 0.5238 (0.5112)	
training:	Epoch: [18][34/204]	Loss 0.4461 (0.5092)	
training:	Epoch: [18][35/204]	Loss 0.3840 (0.5057)	
training:	Epoch: [18][36/204]	Loss 0.5341 (0.5065)	
training:	Epoch: [18][37/204]	Loss 0.6567 (0.5105)	
training:	Epoch: [18][38/204]	Loss 0.4992 (0.5102)	
training:	Epoch: [18][39/204]	Loss 0.6511 (0.5138)	
training:	Epoch: [18][40/204]	Loss 0.5711 (0.5153)	
training:	Epoch: [18][41/204]	Loss 0.5149 (0.5153)	
training:	Epoch: [18][42/204]	Loss 0.6503 (0.5185)	
training:	Epoch: [18][43/204]	Loss 0.4776 (0.5175)	
training:	Epoch: [18][44/204]	Loss 0.4904 (0.5169)	
training:	Epoch: [18][45/204]	Loss 0.7278 (0.5216)	
training:	Epoch: [18][46/204]	Loss 0.5516 (0.5222)	
training:	Epoch: [18][47/204]	Loss 0.7175 (0.5264)	
training:	Epoch: [18][48/204]	Loss 0.4328 (0.5244)	
training:	Epoch: [18][49/204]	Loss 0.4946 (0.5238)	
training:	Epoch: [18][50/204]	Loss 0.5360 (0.5241)	
training:	Epoch: [18][51/204]	Loss 0.5146 (0.5239)	
training:	Epoch: [18][52/204]	Loss 0.3650 (0.5208)	
training:	Epoch: [18][53/204]	Loss 0.4723 (0.5199)	
training:	Epoch: [18][54/204]	Loss 0.4702 (0.5190)	
training:	Epoch: [18][55/204]	Loss 0.5816 (0.5201)	
training:	Epoch: [18][56/204]	Loss 0.6293 (0.5221)	
training:	Epoch: [18][57/204]	Loss 0.4637 (0.5211)	
training:	Epoch: [18][58/204]	Loss 0.5765 (0.5220)	
training:	Epoch: [18][59/204]	Loss 0.3923 (0.5198)	
training:	Epoch: [18][60/204]	Loss 0.5233 (0.5199)	
training:	Epoch: [18][61/204]	Loss 0.4071 (0.5180)	
training:	Epoch: [18][62/204]	Loss 0.5306 (0.5182)	
training:	Epoch: [18][63/204]	Loss 0.4548 (0.5172)	
training:	Epoch: [18][64/204]	Loss 0.5951 (0.5184)	
training:	Epoch: [18][65/204]	Loss 0.4899 (0.5180)	
training:	Epoch: [18][66/204]	Loss 0.4951 (0.5177)	
training:	Epoch: [18][67/204]	Loss 0.7079 (0.5205)	
training:	Epoch: [18][68/204]	Loss 0.6279 (0.5221)	
training:	Epoch: [18][69/204]	Loss 0.3979 (0.5203)	
training:	Epoch: [18][70/204]	Loss 0.6680 (0.5224)	
training:	Epoch: [18][71/204]	Loss 0.5486 (0.5228)	
training:	Epoch: [18][72/204]	Loss 0.3985 (0.5210)	
training:	Epoch: [18][73/204]	Loss 0.4642 (0.5203)	
training:	Epoch: [18][74/204]	Loss 0.4509 (0.5193)	
training:	Epoch: [18][75/204]	Loss 0.4147 (0.5179)	
training:	Epoch: [18][76/204]	Loss 0.5652 (0.5185)	
training:	Epoch: [18][77/204]	Loss 0.4114 (0.5171)	
training:	Epoch: [18][78/204]	Loss 0.5031 (0.5170)	
training:	Epoch: [18][79/204]	Loss 0.6385 (0.5185)	
training:	Epoch: [18][80/204]	Loss 0.5688 (0.5191)	
training:	Epoch: [18][81/204]	Loss 0.5116 (0.5190)	
training:	Epoch: [18][82/204]	Loss 0.5138 (0.5190)	
training:	Epoch: [18][83/204]	Loss 0.3968 (0.5175)	
training:	Epoch: [18][84/204]	Loss 0.5279 (0.5176)	
training:	Epoch: [18][85/204]	Loss 0.5856 (0.5184)	
training:	Epoch: [18][86/204]	Loss 0.5203 (0.5185)	
training:	Epoch: [18][87/204]	Loss 0.5404 (0.5187)	
training:	Epoch: [18][88/204]	Loss 0.5060 (0.5186)	
training:	Epoch: [18][89/204]	Loss 0.4136 (0.5174)	
training:	Epoch: [18][90/204]	Loss 0.4969 (0.5172)	
training:	Epoch: [18][91/204]	Loss 0.5523 (0.5175)	
training:	Epoch: [18][92/204]	Loss 0.4815 (0.5171)	
training:	Epoch: [18][93/204]	Loss 0.5041 (0.5170)	
training:	Epoch: [18][94/204]	Loss 0.6947 (0.5189)	
training:	Epoch: [18][95/204]	Loss 0.4321 (0.5180)	
training:	Epoch: [18][96/204]	Loss 0.5516 (0.5183)	
training:	Epoch: [18][97/204]	Loss 0.6214 (0.5194)	
training:	Epoch: [18][98/204]	Loss 0.4742 (0.5189)	
training:	Epoch: [18][99/204]	Loss 0.4387 (0.5181)	
training:	Epoch: [18][100/204]	Loss 0.4850 (0.5178)	
training:	Epoch: [18][101/204]	Loss 0.6002 (0.5186)	
training:	Epoch: [18][102/204]	Loss 0.5636 (0.5190)	
training:	Epoch: [18][103/204]	Loss 0.6602 (0.5204)	
training:	Epoch: [18][104/204]	Loss 0.5207 (0.5204)	
training:	Epoch: [18][105/204]	Loss 0.4699 (0.5199)	
training:	Epoch: [18][106/204]	Loss 0.5574 (0.5203)	
training:	Epoch: [18][107/204]	Loss 0.6281 (0.5213)	
training:	Epoch: [18][108/204]	Loss 0.3866 (0.5201)	
training:	Epoch: [18][109/204]	Loss 0.4973 (0.5198)	
training:	Epoch: [18][110/204]	Loss 0.3493 (0.5183)	
training:	Epoch: [18][111/204]	Loss 0.4393 (0.5176)	
training:	Epoch: [18][112/204]	Loss 0.4181 (0.5167)	
training:	Epoch: [18][113/204]	Loss 0.5108 (0.5166)	
training:	Epoch: [18][114/204]	Loss 0.4704 (0.5162)	
training:	Epoch: [18][115/204]	Loss 0.5977 (0.5169)	
training:	Epoch: [18][116/204]	Loss 0.5329 (0.5171)	
training:	Epoch: [18][117/204]	Loss 0.5495 (0.5174)	
training:	Epoch: [18][118/204]	Loss 0.5051 (0.5173)	
training:	Epoch: [18][119/204]	Loss 0.5401 (0.5174)	
training:	Epoch: [18][120/204]	Loss 0.5311 (0.5176)	
training:	Epoch: [18][121/204]	Loss 0.4621 (0.5171)	
training:	Epoch: [18][122/204]	Loss 0.5013 (0.5170)	
training:	Epoch: [18][123/204]	Loss 0.4886 (0.5167)	
training:	Epoch: [18][124/204]	Loss 0.4220 (0.5160)	
training:	Epoch: [18][125/204]	Loss 0.6456 (0.5170)	
training:	Epoch: [18][126/204]	Loss 0.4908 (0.5168)	
training:	Epoch: [18][127/204]	Loss 0.4566 (0.5163)	
training:	Epoch: [18][128/204]	Loss 0.4759 (0.5160)	
training:	Epoch: [18][129/204]	Loss 0.6073 (0.5167)	
training:	Epoch: [18][130/204]	Loss 0.5128 (0.5167)	
training:	Epoch: [18][131/204]	Loss 0.5412 (0.5169)	
training:	Epoch: [18][132/204]	Loss 0.5801 (0.5174)	
training:	Epoch: [18][133/204]	Loss 0.4774 (0.5171)	
training:	Epoch: [18][134/204]	Loss 0.4661 (0.5167)	
training:	Epoch: [18][135/204]	Loss 0.3505 (0.5154)	
training:	Epoch: [18][136/204]	Loss 0.5082 (0.5154)	
training:	Epoch: [18][137/204]	Loss 0.3998 (0.5146)	
training:	Epoch: [18][138/204]	Loss 0.4113 (0.5138)	
training:	Epoch: [18][139/204]	Loss 0.5979 (0.5144)	
training:	Epoch: [18][140/204]	Loss 0.4961 (0.5143)	
training:	Epoch: [18][141/204]	Loss 0.4784 (0.5140)	
training:	Epoch: [18][142/204]	Loss 0.4733 (0.5137)	
training:	Epoch: [18][143/204]	Loss 0.6073 (0.5144)	
training:	Epoch: [18][144/204]	Loss 0.4896 (0.5142)	
training:	Epoch: [18][145/204]	Loss 0.4214 (0.5136)	
training:	Epoch: [18][146/204]	Loss 0.4854 (0.5134)	
training:	Epoch: [18][147/204]	Loss 0.4607 (0.5130)	
training:	Epoch: [18][148/204]	Loss 0.4308 (0.5125)	
training:	Epoch: [18][149/204]	Loss 0.4328 (0.5119)	
training:	Epoch: [18][150/204]	Loss 0.5452 (0.5122)	
training:	Epoch: [18][151/204]	Loss 0.4542 (0.5118)	
training:	Epoch: [18][152/204]	Loss 0.7904 (0.5136)	
training:	Epoch: [18][153/204]	Loss 0.4149 (0.5130)	
training:	Epoch: [18][154/204]	Loss 0.5707 (0.5133)	
training:	Epoch: [18][155/204]	Loss 0.7259 (0.5147)	
training:	Epoch: [18][156/204]	Loss 0.4702 (0.5144)	
training:	Epoch: [18][157/204]	Loss 0.5387 (0.5146)	
training:	Epoch: [18][158/204]	Loss 0.6252 (0.5153)	
training:	Epoch: [18][159/204]	Loss 0.5921 (0.5158)	
training:	Epoch: [18][160/204]	Loss 0.4007 (0.5150)	
training:	Epoch: [18][161/204]	Loss 0.4328 (0.5145)	
training:	Epoch: [18][162/204]	Loss 0.5881 (0.5150)	
training:	Epoch: [18][163/204]	Loss 0.4990 (0.5149)	
training:	Epoch: [18][164/204]	Loss 0.4966 (0.5148)	
training:	Epoch: [18][165/204]	Loss 0.4213 (0.5142)	
training:	Epoch: [18][166/204]	Loss 0.4842 (0.5140)	
training:	Epoch: [18][167/204]	Loss 0.6806 (0.5150)	
training:	Epoch: [18][168/204]	Loss 0.6267 (0.5157)	
training:	Epoch: [18][169/204]	Loss 0.4242 (0.5152)	
training:	Epoch: [18][170/204]	Loss 0.5020 (0.5151)	
training:	Epoch: [18][171/204]	Loss 0.5628 (0.5154)	
training:	Epoch: [18][172/204]	Loss 0.5631 (0.5156)	
training:	Epoch: [18][173/204]	Loss 0.5761 (0.5160)	
training:	Epoch: [18][174/204]	Loss 0.6279 (0.5166)	
training:	Epoch: [18][175/204]	Loss 0.5253 (0.5167)	
training:	Epoch: [18][176/204]	Loss 0.6088 (0.5172)	
training:	Epoch: [18][177/204]	Loss 0.4487 (0.5168)	
training:	Epoch: [18][178/204]	Loss 0.4845 (0.5166)	
training:	Epoch: [18][179/204]	Loss 0.5452 (0.5168)	
training:	Epoch: [18][180/204]	Loss 0.5033 (0.5167)	
training:	Epoch: [18][181/204]	Loss 0.4417 (0.5163)	
training:	Epoch: [18][182/204]	Loss 0.5621 (0.5165)	
training:	Epoch: [18][183/204]	Loss 0.6415 (0.5172)	
training:	Epoch: [18][184/204]	Loss 0.4960 (0.5171)	
training:	Epoch: [18][185/204]	Loss 0.4870 (0.5170)	
training:	Epoch: [18][186/204]	Loss 0.5301 (0.5170)	
training:	Epoch: [18][187/204]	Loss 0.4158 (0.5165)	
training:	Epoch: [18][188/204]	Loss 0.5336 (0.5166)	
training:	Epoch: [18][189/204]	Loss 0.4085 (0.5160)	
training:	Epoch: [18][190/204]	Loss 0.4147 (0.5155)	
training:	Epoch: [18][191/204]	Loss 0.3628 (0.5147)	
training:	Epoch: [18][192/204]	Loss 0.5619 (0.5149)	
training:	Epoch: [18][193/204]	Loss 0.3583 (0.5141)	
training:	Epoch: [18][194/204]	Loss 0.4233 (0.5136)	
training:	Epoch: [18][195/204]	Loss 0.5525 (0.5138)	
training:	Epoch: [18][196/204]	Loss 0.4894 (0.5137)	
training:	Epoch: [18][197/204]	Loss 0.5345 (0.5138)	
training:	Epoch: [18][198/204]	Loss 0.5762 (0.5141)	
training:	Epoch: [18][199/204]	Loss 0.4444 (0.5138)	
training:	Epoch: [18][200/204]	Loss 0.6028 (0.5142)	
training:	Epoch: [18][201/204]	Loss 0.6083 (0.5147)	
training:	Epoch: [18][202/204]	Loss 0.5300 (0.5148)	
training:	Epoch: [18][203/204]	Loss 0.4751 (0.5146)	
training:	Epoch: [18][204/204]	Loss 0.4493 (0.5143)	
Training:	 Loss: 0.5135

Training:	 ACC: 0.7613 0.7623 0.7860 0.7366
Validation:	 ACC: 0.7323 0.7341 0.7718 0.6928
Validation:	 Best_BACC: 0.7323 0.7341 0.7718 0.6928
Validation:	 Loss: 0.5366
Pretraining:	Epoch 19/200
----------
training:	Epoch: [19][1/204]	Loss 0.5539 (0.5539)	
training:	Epoch: [19][2/204]	Loss 0.4998 (0.5269)	
training:	Epoch: [19][3/204]	Loss 0.4464 (0.5000)	
training:	Epoch: [19][4/204]	Loss 0.4758 (0.4940)	
training:	Epoch: [19][5/204]	Loss 0.6004 (0.5153)	
training:	Epoch: [19][6/204]	Loss 0.4111 (0.4979)	
training:	Epoch: [19][7/204]	Loss 0.4813 (0.4955)	
training:	Epoch: [19][8/204]	Loss 0.4741 (0.4928)	
training:	Epoch: [19][9/204]	Loss 0.5550 (0.4998)	
training:	Epoch: [19][10/204]	Loss 0.3863 (0.4884)	
training:	Epoch: [19][11/204]	Loss 0.3786 (0.4784)	
training:	Epoch: [19][12/204]	Loss 0.5326 (0.4829)	
training:	Epoch: [19][13/204]	Loss 0.5156 (0.4855)	
training:	Epoch: [19][14/204]	Loss 0.4757 (0.4848)	
training:	Epoch: [19][15/204]	Loss 0.4759 (0.4842)	
training:	Epoch: [19][16/204]	Loss 0.4056 (0.4793)	
training:	Epoch: [19][17/204]	Loss 0.4148 (0.4755)	
training:	Epoch: [19][18/204]	Loss 0.4953 (0.4766)	
training:	Epoch: [19][19/204]	Loss 0.6129 (0.4837)	
training:	Epoch: [19][20/204]	Loss 0.5121 (0.4852)	
training:	Epoch: [19][21/204]	Loss 0.4653 (0.4842)	
training:	Epoch: [19][22/204]	Loss 0.4192 (0.4813)	
training:	Epoch: [19][23/204]	Loss 0.5890 (0.4859)	
training:	Epoch: [19][24/204]	Loss 0.5481 (0.4885)	
training:	Epoch: [19][25/204]	Loss 0.5236 (0.4899)	
training:	Epoch: [19][26/204]	Loss 0.6122 (0.4946)	
training:	Epoch: [19][27/204]	Loss 0.5404 (0.4963)	
training:	Epoch: [19][28/204]	Loss 0.4965 (0.4963)	
training:	Epoch: [19][29/204]	Loss 0.5437 (0.4980)	
training:	Epoch: [19][30/204]	Loss 0.4001 (0.4947)	
training:	Epoch: [19][31/204]	Loss 0.4480 (0.4932)	
training:	Epoch: [19][32/204]	Loss 0.4979 (0.4933)	
training:	Epoch: [19][33/204]	Loss 0.5560 (0.4952)	
training:	Epoch: [19][34/204]	Loss 0.5746 (0.4976)	
training:	Epoch: [19][35/204]	Loss 0.4532 (0.4963)	
training:	Epoch: [19][36/204]	Loss 0.4426 (0.4948)	
training:	Epoch: [19][37/204]	Loss 0.3717 (0.4915)	
training:	Epoch: [19][38/204]	Loss 0.5528 (0.4931)	
training:	Epoch: [19][39/204]	Loss 0.5388 (0.4943)	
training:	Epoch: [19][40/204]	Loss 0.4468 (0.4931)	
training:	Epoch: [19][41/204]	Loss 0.5156 (0.4936)	
training:	Epoch: [19][42/204]	Loss 0.3765 (0.4909)	
training:	Epoch: [19][43/204]	Loss 0.5238 (0.4916)	
training:	Epoch: [19][44/204]	Loss 0.5396 (0.4927)	
training:	Epoch: [19][45/204]	Loss 0.5119 (0.4931)	
training:	Epoch: [19][46/204]	Loss 0.5438 (0.4942)	
training:	Epoch: [19][47/204]	Loss 0.4945 (0.4942)	
training:	Epoch: [19][48/204]	Loss 0.4871 (0.4941)	
training:	Epoch: [19][49/204]	Loss 0.5244 (0.4947)	
training:	Epoch: [19][50/204]	Loss 0.5224 (0.4953)	
training:	Epoch: [19][51/204]	Loss 0.5055 (0.4955)	
training:	Epoch: [19][52/204]	Loss 0.6445 (0.4983)	
training:	Epoch: [19][53/204]	Loss 0.4053 (0.4966)	
training:	Epoch: [19][54/204]	Loss 0.6253 (0.4990)	
training:	Epoch: [19][55/204]	Loss 0.4299 (0.4977)	
training:	Epoch: [19][56/204]	Loss 0.4206 (0.4963)	
training:	Epoch: [19][57/204]	Loss 0.4911 (0.4962)	
training:	Epoch: [19][58/204]	Loss 0.5917 (0.4979)	
training:	Epoch: [19][59/204]	Loss 0.4301 (0.4967)	
training:	Epoch: [19][60/204]	Loss 0.5871 (0.4982)	
training:	Epoch: [19][61/204]	Loss 0.4675 (0.4977)	
training:	Epoch: [19][62/204]	Loss 0.3958 (0.4961)	
training:	Epoch: [19][63/204]	Loss 0.5436 (0.4968)	
training:	Epoch: [19][64/204]	Loss 0.4944 (0.4968)	
training:	Epoch: [19][65/204]	Loss 0.5248 (0.4972)	
training:	Epoch: [19][66/204]	Loss 0.4280 (0.4962)	
training:	Epoch: [19][67/204]	Loss 0.6212 (0.4981)	
training:	Epoch: [19][68/204]	Loss 0.5729 (0.4992)	
training:	Epoch: [19][69/204]	Loss 0.6365 (0.5011)	
training:	Epoch: [19][70/204]	Loss 0.5611 (0.5020)	
training:	Epoch: [19][71/204]	Loss 0.6800 (0.5045)	
training:	Epoch: [19][72/204]	Loss 0.6299 (0.5063)	
training:	Epoch: [19][73/204]	Loss 0.5464 (0.5068)	
training:	Epoch: [19][74/204]	Loss 0.4752 (0.5064)	
training:	Epoch: [19][75/204]	Loss 0.5719 (0.5072)	
training:	Epoch: [19][76/204]	Loss 0.5205 (0.5074)	
training:	Epoch: [19][77/204]	Loss 0.4200 (0.5063)	
training:	Epoch: [19][78/204]	Loss 0.4122 (0.5051)	
training:	Epoch: [19][79/204]	Loss 0.4894 (0.5049)	
training:	Epoch: [19][80/204]	Loss 0.4196 (0.5038)	
training:	Epoch: [19][81/204]	Loss 0.6406 (0.5055)	
training:	Epoch: [19][82/204]	Loss 0.5475 (0.5060)	
training:	Epoch: [19][83/204]	Loss 0.4758 (0.5057)	
training:	Epoch: [19][84/204]	Loss 0.4675 (0.5052)	
training:	Epoch: [19][85/204]	Loss 0.3837 (0.5038)	
training:	Epoch: [19][86/204]	Loss 0.7166 (0.5062)	
training:	Epoch: [19][87/204]	Loss 0.4444 (0.5055)	
training:	Epoch: [19][88/204]	Loss 0.5267 (0.5058)	
training:	Epoch: [19][89/204]	Loss 0.5796 (0.5066)	
training:	Epoch: [19][90/204]	Loss 0.4142 (0.5056)	
training:	Epoch: [19][91/204]	Loss 0.5594 (0.5062)	
training:	Epoch: [19][92/204]	Loss 0.5453 (0.5066)	
training:	Epoch: [19][93/204]	Loss 0.6096 (0.5077)	
training:	Epoch: [19][94/204]	Loss 0.3119 (0.5056)	
training:	Epoch: [19][95/204]	Loss 0.6482 (0.5071)	
training:	Epoch: [19][96/204]	Loss 0.5011 (0.5071)	
training:	Epoch: [19][97/204]	Loss 0.5914 (0.5079)	
training:	Epoch: [19][98/204]	Loss 0.5323 (0.5082)	
training:	Epoch: [19][99/204]	Loss 0.4861 (0.5080)	
training:	Epoch: [19][100/204]	Loss 0.4523 (0.5074)	
training:	Epoch: [19][101/204]	Loss 0.6362 (0.5087)	
training:	Epoch: [19][102/204]	Loss 0.5498 (0.5091)	
training:	Epoch: [19][103/204]	Loss 0.5706 (0.5097)	
training:	Epoch: [19][104/204]	Loss 0.5521 (0.5101)	
training:	Epoch: [19][105/204]	Loss 0.5089 (0.5101)	
training:	Epoch: [19][106/204]	Loss 0.3599 (0.5086)	
training:	Epoch: [19][107/204]	Loss 0.5265 (0.5088)	
training:	Epoch: [19][108/204]	Loss 0.4408 (0.5082)	
training:	Epoch: [19][109/204]	Loss 0.6444 (0.5094)	
training:	Epoch: [19][110/204]	Loss 0.5152 (0.5095)	
training:	Epoch: [19][111/204]	Loss 0.4801 (0.5092)	
training:	Epoch: [19][112/204]	Loss 0.4961 (0.5091)	
training:	Epoch: [19][113/204]	Loss 0.5403 (0.5094)	
training:	Epoch: [19][114/204]	Loss 0.4784 (0.5091)	
training:	Epoch: [19][115/204]	Loss 0.4822 (0.5089)	
training:	Epoch: [19][116/204]	Loss 0.4977 (0.5088)	
training:	Epoch: [19][117/204]	Loss 0.4843 (0.5086)	
training:	Epoch: [19][118/204]	Loss 0.4269 (0.5079)	
training:	Epoch: [19][119/204]	Loss 0.4874 (0.5077)	
training:	Epoch: [19][120/204]	Loss 0.4277 (0.5070)	
training:	Epoch: [19][121/204]	Loss 0.6534 (0.5083)	
training:	Epoch: [19][122/204]	Loss 0.4300 (0.5076)	
training:	Epoch: [19][123/204]	Loss 0.4717 (0.5073)	
training:	Epoch: [19][124/204]	Loss 0.5252 (0.5075)	
training:	Epoch: [19][125/204]	Loss 0.4693 (0.5072)	
training:	Epoch: [19][126/204]	Loss 0.4363 (0.5066)	
training:	Epoch: [19][127/204]	Loss 0.5655 (0.5071)	
training:	Epoch: [19][128/204]	Loss 0.4874 (0.5069)	
training:	Epoch: [19][129/204]	Loss 0.3617 (0.5058)	
training:	Epoch: [19][130/204]	Loss 0.5006 (0.5057)	
training:	Epoch: [19][131/204]	Loss 0.4538 (0.5053)	
training:	Epoch: [19][132/204]	Loss 0.4793 (0.5051)	
training:	Epoch: [19][133/204]	Loss 0.4916 (0.5050)	
training:	Epoch: [19][134/204]	Loss 0.5086 (0.5051)	
training:	Epoch: [19][135/204]	Loss 0.6176 (0.5059)	
training:	Epoch: [19][136/204]	Loss 0.5755 (0.5064)	
training:	Epoch: [19][137/204]	Loss 0.5384 (0.5066)	
training:	Epoch: [19][138/204]	Loss 0.6224 (0.5075)	
training:	Epoch: [19][139/204]	Loss 0.4879 (0.5073)	
training:	Epoch: [19][140/204]	Loss 0.5945 (0.5080)	
training:	Epoch: [19][141/204]	Loss 0.5921 (0.5086)	
training:	Epoch: [19][142/204]	Loss 0.5705 (0.5090)	
training:	Epoch: [19][143/204]	Loss 0.3526 (0.5079)	
training:	Epoch: [19][144/204]	Loss 0.4882 (0.5078)	
training:	Epoch: [19][145/204]	Loss 0.5108 (0.5078)	
training:	Epoch: [19][146/204]	Loss 0.4496 (0.5074)	
training:	Epoch: [19][147/204]	Loss 0.3668 (0.5064)	
training:	Epoch: [19][148/204]	Loss 0.3986 (0.5057)	
training:	Epoch: [19][149/204]	Loss 0.5232 (0.5058)	
training:	Epoch: [19][150/204]	Loss 0.3627 (0.5049)	
training:	Epoch: [19][151/204]	Loss 0.4398 (0.5044)	
training:	Epoch: [19][152/204]	Loss 0.5277 (0.5046)	
training:	Epoch: [19][153/204]	Loss 0.4564 (0.5043)	
training:	Epoch: [19][154/204]	Loss 0.4988 (0.5042)	
training:	Epoch: [19][155/204]	Loss 0.3308 (0.5031)	
training:	Epoch: [19][156/204]	Loss 0.5350 (0.5033)	
training:	Epoch: [19][157/204]	Loss 0.8364 (0.5054)	
training:	Epoch: [19][158/204]	Loss 0.5912 (0.5060)	
training:	Epoch: [19][159/204]	Loss 0.6198 (0.5067)	
training:	Epoch: [19][160/204]	Loss 0.6183 (0.5074)	
training:	Epoch: [19][161/204]	Loss 0.6476 (0.5083)	
training:	Epoch: [19][162/204]	Loss 0.4984 (0.5082)	
training:	Epoch: [19][163/204]	Loss 0.4450 (0.5078)	
training:	Epoch: [19][164/204]	Loss 0.6911 (0.5089)	
training:	Epoch: [19][165/204]	Loss 0.4610 (0.5087)	
training:	Epoch: [19][166/204]	Loss 0.4863 (0.5085)	
training:	Epoch: [19][167/204]	Loss 0.5521 (0.5088)	
training:	Epoch: [19][168/204]	Loss 0.4674 (0.5085)	
training:	Epoch: [19][169/204]	Loss 0.4443 (0.5082)	
training:	Epoch: [19][170/204]	Loss 0.5156 (0.5082)	
training:	Epoch: [19][171/204]	Loss 0.5023 (0.5082)	
training:	Epoch: [19][172/204]	Loss 0.5008 (0.5081)	
training:	Epoch: [19][173/204]	Loss 0.5333 (0.5083)	
training:	Epoch: [19][174/204]	Loss 0.6596 (0.5091)	
training:	Epoch: [19][175/204]	Loss 0.4261 (0.5087)	
training:	Epoch: [19][176/204]	Loss 0.4674 (0.5084)	
training:	Epoch: [19][177/204]	Loss 0.5756 (0.5088)	
training:	Epoch: [19][178/204]	Loss 0.5117 (0.5088)	
training:	Epoch: [19][179/204]	Loss 0.4338 (0.5084)	
training:	Epoch: [19][180/204]	Loss 0.4175 (0.5079)	
training:	Epoch: [19][181/204]	Loss 0.4367 (0.5075)	
training:	Epoch: [19][182/204]	Loss 0.4266 (0.5071)	
training:	Epoch: [19][183/204]	Loss 0.5073 (0.5071)	
training:	Epoch: [19][184/204]	Loss 0.4958 (0.5070)	
training:	Epoch: [19][185/204]	Loss 0.4572 (0.5067)	
training:	Epoch: [19][186/204]	Loss 0.5136 (0.5068)	
training:	Epoch: [19][187/204]	Loss 0.5432 (0.5070)	
training:	Epoch: [19][188/204]	Loss 0.4557 (0.5067)	
training:	Epoch: [19][189/204]	Loss 0.4223 (0.5062)	
training:	Epoch: [19][190/204]	Loss 0.5457 (0.5065)	
training:	Epoch: [19][191/204]	Loss 0.5418 (0.5066)	
training:	Epoch: [19][192/204]	Loss 0.4123 (0.5061)	
training:	Epoch: [19][193/204]	Loss 0.4391 (0.5058)	
training:	Epoch: [19][194/204]	Loss 0.6657 (0.5066)	
training:	Epoch: [19][195/204]	Loss 0.5586 (0.5069)	
training:	Epoch: [19][196/204]	Loss 0.4208 (0.5065)	
training:	Epoch: [19][197/204]	Loss 0.5651 (0.5067)	
training:	Epoch: [19][198/204]	Loss 0.5303 (0.5069)	
training:	Epoch: [19][199/204]	Loss 0.5096 (0.5069)	
training:	Epoch: [19][200/204]	Loss 0.5817 (0.5073)	
training:	Epoch: [19][201/204]	Loss 0.4427 (0.5069)	
training:	Epoch: [19][202/204]	Loss 0.4688 (0.5067)	
training:	Epoch: [19][203/204]	Loss 0.6021 (0.5072)	
training:	Epoch: [19][204/204]	Loss 0.4253 (0.5068)	
Training:	 Loss: 0.5060

Training:	 ACC: 0.7656 0.7660 0.7743 0.7570
Validation:	 ACC: 0.7327 0.7335 0.7513 0.7141
Validation:	 Best_BACC: 0.7327 0.7335 0.7513 0.7141
Validation:	 Loss: 0.5331
Pretraining:	Epoch 20/200
----------
training:	Epoch: [20][1/204]	Loss 0.4466 (0.4466)	
training:	Epoch: [20][2/204]	Loss 0.5069 (0.4767)	
training:	Epoch: [20][3/204]	Loss 0.4867 (0.4801)	
training:	Epoch: [20][4/204]	Loss 0.5205 (0.4902)	
training:	Epoch: [20][5/204]	Loss 0.4555 (0.4832)	
training:	Epoch: [20][6/204]	Loss 0.5833 (0.4999)	
training:	Epoch: [20][7/204]	Loss 0.4959 (0.4993)	
training:	Epoch: [20][8/204]	Loss 0.5315 (0.5034)	
training:	Epoch: [20][9/204]	Loss 0.4752 (0.5002)	
training:	Epoch: [20][10/204]	Loss 0.5360 (0.5038)	
training:	Epoch: [20][11/204]	Loss 0.4208 (0.4963)	
training:	Epoch: [20][12/204]	Loss 0.5801 (0.5032)	
training:	Epoch: [20][13/204]	Loss 0.5207 (0.5046)	
training:	Epoch: [20][14/204]	Loss 0.5691 (0.5092)	
training:	Epoch: [20][15/204]	Loss 0.4005 (0.5020)	
training:	Epoch: [20][16/204]	Loss 0.2845 (0.4884)	
training:	Epoch: [20][17/204]	Loss 0.4817 (0.4880)	
training:	Epoch: [20][18/204]	Loss 0.3894 (0.4825)	
training:	Epoch: [20][19/204]	Loss 0.6865 (0.4932)	
training:	Epoch: [20][20/204]	Loss 0.3957 (0.4884)	
training:	Epoch: [20][21/204]	Loss 0.4917 (0.4885)	
training:	Epoch: [20][22/204]	Loss 0.6670 (0.4966)	
training:	Epoch: [20][23/204]	Loss 0.5400 (0.4985)	
training:	Epoch: [20][24/204]	Loss 0.4666 (0.4972)	
training:	Epoch: [20][25/204]	Loss 0.5524 (0.4994)	
training:	Epoch: [20][26/204]	Loss 0.4605 (0.4979)	
training:	Epoch: [20][27/204]	Loss 0.3952 (0.4941)	
training:	Epoch: [20][28/204]	Loss 0.5602 (0.4965)	
training:	Epoch: [20][29/204]	Loss 0.4379 (0.4944)	
training:	Epoch: [20][30/204]	Loss 0.5065 (0.4948)	
training:	Epoch: [20][31/204]	Loss 0.4971 (0.4949)	
training:	Epoch: [20][32/204]	Loss 0.3490 (0.4904)	
training:	Epoch: [20][33/204]	Loss 0.5132 (0.4910)	
training:	Epoch: [20][34/204]	Loss 0.5879 (0.4939)	
training:	Epoch: [20][35/204]	Loss 0.6280 (0.4977)	
training:	Epoch: [20][36/204]	Loss 0.6639 (0.5023)	
training:	Epoch: [20][37/204]	Loss 0.3268 (0.4976)	
training:	Epoch: [20][38/204]	Loss 0.5634 (0.4993)	
training:	Epoch: [20][39/204]	Loss 0.5453 (0.5005)	
training:	Epoch: [20][40/204]	Loss 0.4501 (0.4993)	
training:	Epoch: [20][41/204]	Loss 0.5982 (0.5017)	
training:	Epoch: [20][42/204]	Loss 0.4234 (0.4998)	
training:	Epoch: [20][43/204]	Loss 0.4883 (0.4995)	
training:	Epoch: [20][44/204]	Loss 0.4889 (0.4993)	
training:	Epoch: [20][45/204]	Loss 0.5268 (0.4999)	
training:	Epoch: [20][46/204]	Loss 0.4918 (0.4997)	
training:	Epoch: [20][47/204]	Loss 0.4587 (0.4989)	
training:	Epoch: [20][48/204]	Loss 0.5788 (0.5005)	
training:	Epoch: [20][49/204]	Loss 0.4635 (0.4998)	
training:	Epoch: [20][50/204]	Loss 0.4170 (0.4981)	
training:	Epoch: [20][51/204]	Loss 0.4909 (0.4980)	
training:	Epoch: [20][52/204]	Loss 0.5865 (0.4997)	
training:	Epoch: [20][53/204]	Loss 0.5483 (0.5006)	
training:	Epoch: [20][54/204]	Loss 0.4111 (0.4989)	
training:	Epoch: [20][55/204]	Loss 0.5963 (0.5007)	
training:	Epoch: [20][56/204]	Loss 0.3879 (0.4987)	
training:	Epoch: [20][57/204]	Loss 0.6090 (0.5006)	
training:	Epoch: [20][58/204]	Loss 0.4308 (0.4994)	
training:	Epoch: [20][59/204]	Loss 0.5000 (0.4994)	
training:	Epoch: [20][60/204]	Loss 0.5619 (0.5005)	
training:	Epoch: [20][61/204]	Loss 0.3753 (0.4984)	
training:	Epoch: [20][62/204]	Loss 0.4505 (0.4976)	
training:	Epoch: [20][63/204]	Loss 0.5485 (0.4984)	
training:	Epoch: [20][64/204]	Loss 0.5484 (0.4992)	
training:	Epoch: [20][65/204]	Loss 0.5423 (0.4999)	
training:	Epoch: [20][66/204]	Loss 0.4677 (0.4994)	
training:	Epoch: [20][67/204]	Loss 0.5976 (0.5009)	
training:	Epoch: [20][68/204]	Loss 0.5196 (0.5011)	
training:	Epoch: [20][69/204]	Loss 0.5335 (0.5016)	
training:	Epoch: [20][70/204]	Loss 0.4692 (0.5011)	
training:	Epoch: [20][71/204]	Loss 0.6048 (0.5026)	
training:	Epoch: [20][72/204]	Loss 0.5066 (0.5027)	
training:	Epoch: [20][73/204]	Loss 0.3528 (0.5006)	
training:	Epoch: [20][74/204]	Loss 0.5214 (0.5009)	
training:	Epoch: [20][75/204]	Loss 0.4793 (0.5006)	
training:	Epoch: [20][76/204]	Loss 0.4040 (0.4993)	
training:	Epoch: [20][77/204]	Loss 0.5460 (0.4999)	
training:	Epoch: [20][78/204]	Loss 0.4537 (0.4993)	
training:	Epoch: [20][79/204]	Loss 0.5803 (0.5004)	
training:	Epoch: [20][80/204]	Loss 0.5292 (0.5007)	
training:	Epoch: [20][81/204]	Loss 0.5510 (0.5014)	
training:	Epoch: [20][82/204]	Loss 0.4658 (0.5009)	
training:	Epoch: [20][83/204]	Loss 0.4742 (0.5006)	
training:	Epoch: [20][84/204]	Loss 0.3682 (0.4990)	
training:	Epoch: [20][85/204]	Loss 0.6334 (0.5006)	
training:	Epoch: [20][86/204]	Loss 0.4511 (0.5000)	
training:	Epoch: [20][87/204]	Loss 0.4525 (0.4995)	
training:	Epoch: [20][88/204]	Loss 0.5867 (0.5005)	
training:	Epoch: [20][89/204]	Loss 0.4148 (0.4995)	
training:	Epoch: [20][90/204]	Loss 0.4422 (0.4989)	
training:	Epoch: [20][91/204]	Loss 0.4368 (0.4982)	
training:	Epoch: [20][92/204]	Loss 0.4626 (0.4978)	
training:	Epoch: [20][93/204]	Loss 0.3886 (0.4966)	
training:	Epoch: [20][94/204]	Loss 0.4974 (0.4966)	
training:	Epoch: [20][95/204]	Loss 0.4463 (0.4961)	
training:	Epoch: [20][96/204]	Loss 0.5232 (0.4964)	
training:	Epoch: [20][97/204]	Loss 0.4024 (0.4954)	
training:	Epoch: [20][98/204]	Loss 0.4905 (0.4954)	
training:	Epoch: [20][99/204]	Loss 0.5172 (0.4956)	
training:	Epoch: [20][100/204]	Loss 0.4487 (0.4951)	
training:	Epoch: [20][101/204]	Loss 0.4531 (0.4947)	
training:	Epoch: [20][102/204]	Loss 0.3873 (0.4937)	
training:	Epoch: [20][103/204]	Loss 0.6306 (0.4950)	
training:	Epoch: [20][104/204]	Loss 0.4949 (0.4950)	
training:	Epoch: [20][105/204]	Loss 0.6154 (0.4961)	
training:	Epoch: [20][106/204]	Loss 0.5101 (0.4963)	
training:	Epoch: [20][107/204]	Loss 0.4773 (0.4961)	
training:	Epoch: [20][108/204]	Loss 0.4454 (0.4956)	
training:	Epoch: [20][109/204]	Loss 0.5575 (0.4962)	
training:	Epoch: [20][110/204]	Loss 0.3939 (0.4953)	
training:	Epoch: [20][111/204]	Loss 0.5307 (0.4956)	
training:	Epoch: [20][112/204]	Loss 0.4426 (0.4951)	
training:	Epoch: [20][113/204]	Loss 0.4946 (0.4951)	
training:	Epoch: [20][114/204]	Loss 0.4989 (0.4951)	
training:	Epoch: [20][115/204]	Loss 0.4046 (0.4943)	
training:	Epoch: [20][116/204]	Loss 0.6062 (0.4953)	
training:	Epoch: [20][117/204]	Loss 0.4800 (0.4952)	
training:	Epoch: [20][118/204]	Loss 0.5330 (0.4955)	
training:	Epoch: [20][119/204]	Loss 0.5027 (0.4956)	
training:	Epoch: [20][120/204]	Loss 0.6878 (0.4972)	
training:	Epoch: [20][121/204]	Loss 0.4523 (0.4968)	
training:	Epoch: [20][122/204]	Loss 0.4460 (0.4964)	
training:	Epoch: [20][123/204]	Loss 0.5383 (0.4967)	
training:	Epoch: [20][124/204]	Loss 0.4675 (0.4965)	
training:	Epoch: [20][125/204]	Loss 0.4741 (0.4963)	
training:	Epoch: [20][126/204]	Loss 0.5035 (0.4964)	
training:	Epoch: [20][127/204]	Loss 0.4634 (0.4961)	
training:	Epoch: [20][128/204]	Loss 0.5074 (0.4962)	
training:	Epoch: [20][129/204]	Loss 0.5621 (0.4967)	
training:	Epoch: [20][130/204]	Loss 0.4048 (0.4960)	
training:	Epoch: [20][131/204]	Loss 0.4710 (0.4958)	
training:	Epoch: [20][132/204]	Loss 0.5425 (0.4961)	
training:	Epoch: [20][133/204]	Loss 0.6438 (0.4973)	
training:	Epoch: [20][134/204]	Loss 0.4982 (0.4973)	
training:	Epoch: [20][135/204]	Loss 0.5069 (0.4973)	
training:	Epoch: [20][136/204]	Loss 0.4949 (0.4973)	
training:	Epoch: [20][137/204]	Loss 0.5429 (0.4977)	
training:	Epoch: [20][138/204]	Loss 0.5679 (0.4982)	
training:	Epoch: [20][139/204]	Loss 0.6837 (0.4995)	
training:	Epoch: [20][140/204]	Loss 0.4564 (0.4992)	
training:	Epoch: [20][141/204]	Loss 0.6138 (0.5000)	
training:	Epoch: [20][142/204]	Loss 0.5249 (0.5002)	
training:	Epoch: [20][143/204]	Loss 0.4008 (0.4995)	
training:	Epoch: [20][144/204]	Loss 0.5464 (0.4998)	
training:	Epoch: [20][145/204]	Loss 0.5348 (0.5000)	
training:	Epoch: [20][146/204]	Loss 0.5568 (0.5004)	
training:	Epoch: [20][147/204]	Loss 0.4979 (0.5004)	
training:	Epoch: [20][148/204]	Loss 0.5171 (0.5005)	
training:	Epoch: [20][149/204]	Loss 0.7716 (0.5024)	
training:	Epoch: [20][150/204]	Loss 0.4812 (0.5022)	
training:	Epoch: [20][151/204]	Loss 0.5782 (0.5027)	
training:	Epoch: [20][152/204]	Loss 0.5155 (0.5028)	
training:	Epoch: [20][153/204]	Loss 0.4260 (0.5023)	
training:	Epoch: [20][154/204]	Loss 0.5120 (0.5024)	
training:	Epoch: [20][155/204]	Loss 0.7617 (0.5040)	
training:	Epoch: [20][156/204]	Loss 0.7026 (0.5053)	
training:	Epoch: [20][157/204]	Loss 0.4076 (0.5047)	
training:	Epoch: [20][158/204]	Loss 0.6039 (0.5053)	
training:	Epoch: [20][159/204]	Loss 0.5377 (0.5055)	
training:	Epoch: [20][160/204]	Loss 0.4530 (0.5052)	
training:	Epoch: [20][161/204]	Loss 0.5478 (0.5055)	
training:	Epoch: [20][162/204]	Loss 0.3636 (0.5046)	
training:	Epoch: [20][163/204]	Loss 0.5098 (0.5046)	
training:	Epoch: [20][164/204]	Loss 0.4617 (0.5043)	
training:	Epoch: [20][165/204]	Loss 0.4522 (0.5040)	
training:	Epoch: [20][166/204]	Loss 0.4811 (0.5039)	
training:	Epoch: [20][167/204]	Loss 0.3999 (0.5033)	
training:	Epoch: [20][168/204]	Loss 0.4915 (0.5032)	
training:	Epoch: [20][169/204]	Loss 0.6656 (0.5042)	
training:	Epoch: [20][170/204]	Loss 0.6160 (0.5048)	
training:	Epoch: [20][171/204]	Loss 0.3627 (0.5040)	
training:	Epoch: [20][172/204]	Loss 0.5361 (0.5042)	
training:	Epoch: [20][173/204]	Loss 0.3912 (0.5035)	
training:	Epoch: [20][174/204]	Loss 0.5256 (0.5036)	
training:	Epoch: [20][175/204]	Loss 0.4832 (0.5035)	
training:	Epoch: [20][176/204]	Loss 0.5402 (0.5037)	
training:	Epoch: [20][177/204]	Loss 0.5405 (0.5039)	
training:	Epoch: [20][178/204]	Loss 0.4262 (0.5035)	
training:	Epoch: [20][179/204]	Loss 0.4598 (0.5033)	
training:	Epoch: [20][180/204]	Loss 0.5829 (0.5037)	
training:	Epoch: [20][181/204]	Loss 0.4597 (0.5035)	
training:	Epoch: [20][182/204]	Loss 0.4825 (0.5034)	
training:	Epoch: [20][183/204]	Loss 0.4948 (0.5033)	
training:	Epoch: [20][184/204]	Loss 0.4030 (0.5028)	
training:	Epoch: [20][185/204]	Loss 0.4933 (0.5027)	
training:	Epoch: [20][186/204]	Loss 0.5572 (0.5030)	
training:	Epoch: [20][187/204]	Loss 0.3989 (0.5024)	
training:	Epoch: [20][188/204]	Loss 0.4241 (0.5020)	
training:	Epoch: [20][189/204]	Loss 0.6187 (0.5026)	
training:	Epoch: [20][190/204]	Loss 0.4667 (0.5025)	
training:	Epoch: [20][191/204]	Loss 0.4718 (0.5023)	
training:	Epoch: [20][192/204]	Loss 0.5494 (0.5025)	
training:	Epoch: [20][193/204]	Loss 0.4244 (0.5021)	
training:	Epoch: [20][194/204]	Loss 0.4636 (0.5019)	
training:	Epoch: [20][195/204]	Loss 0.4944 (0.5019)	
training:	Epoch: [20][196/204]	Loss 0.6814 (0.5028)	
training:	Epoch: [20][197/204]	Loss 0.3554 (0.5021)	
training:	Epoch: [20][198/204]	Loss 0.4379 (0.5017)	
training:	Epoch: [20][199/204]	Loss 0.3659 (0.5011)	
training:	Epoch: [20][200/204]	Loss 0.5025 (0.5011)	
training:	Epoch: [20][201/204]	Loss 0.4420 (0.5008)	
training:	Epoch: [20][202/204]	Loss 0.5390 (0.5010)	
training:	Epoch: [20][203/204]	Loss 0.4380 (0.5007)	
training:	Epoch: [20][204/204]	Loss 0.4062 (0.5002)	
Training:	 Loss: 0.4994

Training:	 ACC: 0.7715 0.7727 0.8010 0.7420
Validation:	 ACC: 0.7361 0.7378 0.7738 0.6984
Validation:	 Best_BACC: 0.7361 0.7378 0.7738 0.6984
Validation:	 Loss: 0.5301
Pretraining:	Epoch 21/200
----------
training:	Epoch: [21][1/204]	Loss 0.4415 (0.4415)	
training:	Epoch: [21][2/204]	Loss 0.3800 (0.4107)	
training:	Epoch: [21][3/204]	Loss 0.5578 (0.4598)	
training:	Epoch: [21][4/204]	Loss 0.4257 (0.4512)	
training:	Epoch: [21][5/204]	Loss 0.7022 (0.5014)	
training:	Epoch: [21][6/204]	Loss 0.4709 (0.4964)	
training:	Epoch: [21][7/204]	Loss 0.4600 (0.4912)	
training:	Epoch: [21][8/204]	Loss 0.5987 (0.5046)	
training:	Epoch: [21][9/204]	Loss 0.4036 (0.4934)	
training:	Epoch: [21][10/204]	Loss 0.5365 (0.4977)	
training:	Epoch: [21][11/204]	Loss 0.6534 (0.5118)	
training:	Epoch: [21][12/204]	Loss 0.5500 (0.5150)	
training:	Epoch: [21][13/204]	Loss 0.6183 (0.5230)	
training:	Epoch: [21][14/204]	Loss 0.4858 (0.5203)	
training:	Epoch: [21][15/204]	Loss 0.5729 (0.5238)	
training:	Epoch: [21][16/204]	Loss 0.4609 (0.5199)	
training:	Epoch: [21][17/204]	Loss 0.5678 (0.5227)	
training:	Epoch: [21][18/204]	Loss 0.5131 (0.5222)	
training:	Epoch: [21][19/204]	Loss 0.4971 (0.5209)	
training:	Epoch: [21][20/204]	Loss 0.3129 (0.5105)	
training:	Epoch: [21][21/204]	Loss 0.4674 (0.5084)	
training:	Epoch: [21][22/204]	Loss 0.5079 (0.5084)	
training:	Epoch: [21][23/204]	Loss 0.5425 (0.5099)	
training:	Epoch: [21][24/204]	Loss 0.5667 (0.5122)	
training:	Epoch: [21][25/204]	Loss 0.4038 (0.5079)	
training:	Epoch: [21][26/204]	Loss 0.4410 (0.5053)	
training:	Epoch: [21][27/204]	Loss 0.6491 (0.5106)	
training:	Epoch: [21][28/204]	Loss 0.3681 (0.5056)	
training:	Epoch: [21][29/204]	Loss 0.3888 (0.5015)	
training:	Epoch: [21][30/204]	Loss 0.5154 (0.5020)	
training:	Epoch: [21][31/204]	Loss 0.5433 (0.5033)	
training:	Epoch: [21][32/204]	Loss 0.4583 (0.5019)	
training:	Epoch: [21][33/204]	Loss 0.4746 (0.5011)	
training:	Epoch: [21][34/204]	Loss 0.5515 (0.5026)	
training:	Epoch: [21][35/204]	Loss 0.4514 (0.5011)	
training:	Epoch: [21][36/204]	Loss 0.4474 (0.4996)	
training:	Epoch: [21][37/204]	Loss 0.4579 (0.4985)	
training:	Epoch: [21][38/204]	Loss 0.5070 (0.4987)	
training:	Epoch: [21][39/204]	Loss 0.4626 (0.4978)	
training:	Epoch: [21][40/204]	Loss 0.4361 (0.4962)	
training:	Epoch: [21][41/204]	Loss 0.4106 (0.4942)	
training:	Epoch: [21][42/204]	Loss 0.4289 (0.4926)	
training:	Epoch: [21][43/204]	Loss 0.4128 (0.4907)	
training:	Epoch: [21][44/204]	Loss 0.4385 (0.4896)	
training:	Epoch: [21][45/204]	Loss 0.3722 (0.4869)	
training:	Epoch: [21][46/204]	Loss 0.5737 (0.4888)	
training:	Epoch: [21][47/204]	Loss 0.4001 (0.4869)	
training:	Epoch: [21][48/204]	Loss 0.5273 (0.4878)	
training:	Epoch: [21][49/204]	Loss 0.5539 (0.4891)	
training:	Epoch: [21][50/204]	Loss 0.4137 (0.4876)	
training:	Epoch: [21][51/204]	Loss 0.4905 (0.4877)	
training:	Epoch: [21][52/204]	Loss 0.6280 (0.4904)	
training:	Epoch: [21][53/204]	Loss 0.3580 (0.4879)	
training:	Epoch: [21][54/204]	Loss 0.4396 (0.4870)	
training:	Epoch: [21][55/204]	Loss 0.4812 (0.4869)	
training:	Epoch: [21][56/204]	Loss 0.4568 (0.4863)	
training:	Epoch: [21][57/204]	Loss 0.4783 (0.4862)	
training:	Epoch: [21][58/204]	Loss 0.7088 (0.4900)	
training:	Epoch: [21][59/204]	Loss 0.4287 (0.4890)	
training:	Epoch: [21][60/204]	Loss 0.2902 (0.4857)	
training:	Epoch: [21][61/204]	Loss 0.4518 (0.4851)	
training:	Epoch: [21][62/204]	Loss 0.5206 (0.4857)	
training:	Epoch: [21][63/204]	Loss 0.6675 (0.4886)	
training:	Epoch: [21][64/204]	Loss 0.3381 (0.4862)	
training:	Epoch: [21][65/204]	Loss 0.4160 (0.4852)	
training:	Epoch: [21][66/204]	Loss 0.5584 (0.4863)	
training:	Epoch: [21][67/204]	Loss 0.4759 (0.4861)	
training:	Epoch: [21][68/204]	Loss 0.4889 (0.4862)	
training:	Epoch: [21][69/204]	Loss 0.5788 (0.4875)	
training:	Epoch: [21][70/204]	Loss 0.5398 (0.4882)	
training:	Epoch: [21][71/204]	Loss 0.6771 (0.4909)	
training:	Epoch: [21][72/204]	Loss 0.4362 (0.4901)	
training:	Epoch: [21][73/204]	Loss 0.4026 (0.4889)	
training:	Epoch: [21][74/204]	Loss 0.4881 (0.4889)	
training:	Epoch: [21][75/204]	Loss 0.4276 (0.4881)	
training:	Epoch: [21][76/204]	Loss 0.4439 (0.4875)	
training:	Epoch: [21][77/204]	Loss 0.4126 (0.4866)	
training:	Epoch: [21][78/204]	Loss 0.6217 (0.4883)	
training:	Epoch: [21][79/204]	Loss 0.5480 (0.4890)	
training:	Epoch: [21][80/204]	Loss 0.4154 (0.4881)	
training:	Epoch: [21][81/204]	Loss 0.4556 (0.4877)	
training:	Epoch: [21][82/204]	Loss 0.4554 (0.4873)	
training:	Epoch: [21][83/204]	Loss 0.5772 (0.4884)	
training:	Epoch: [21][84/204]	Loss 0.5411 (0.4890)	
training:	Epoch: [21][85/204]	Loss 0.7161 (0.4917)	
training:	Epoch: [21][86/204]	Loss 0.4158 (0.4908)	
training:	Epoch: [21][87/204]	Loss 0.4001 (0.4898)	
training:	Epoch: [21][88/204]	Loss 0.4615 (0.4895)	
training:	Epoch: [21][89/204]	Loss 0.4396 (0.4889)	
training:	Epoch: [21][90/204]	Loss 0.4506 (0.4885)	
training:	Epoch: [21][91/204]	Loss 0.5888 (0.4896)	
training:	Epoch: [21][92/204]	Loss 0.5603 (0.4904)	
training:	Epoch: [21][93/204]	Loss 0.4819 (0.4903)	
training:	Epoch: [21][94/204]	Loss 0.4636 (0.4900)	
training:	Epoch: [21][95/204]	Loss 0.5969 (0.4911)	
training:	Epoch: [21][96/204]	Loss 0.4264 (0.4904)	
training:	Epoch: [21][97/204]	Loss 0.4908 (0.4904)	
training:	Epoch: [21][98/204]	Loss 0.4106 (0.4896)	
training:	Epoch: [21][99/204]	Loss 0.6599 (0.4913)	
training:	Epoch: [21][100/204]	Loss 0.5281 (0.4917)	
training:	Epoch: [21][101/204]	Loss 0.5374 (0.4922)	
training:	Epoch: [21][102/204]	Loss 0.4340 (0.4916)	
training:	Epoch: [21][103/204]	Loss 0.4199 (0.4909)	
training:	Epoch: [21][104/204]	Loss 0.4111 (0.4901)	
training:	Epoch: [21][105/204]	Loss 0.5084 (0.4903)	
training:	Epoch: [21][106/204]	Loss 0.5196 (0.4906)	
training:	Epoch: [21][107/204]	Loss 0.4476 (0.4902)	
training:	Epoch: [21][108/204]	Loss 0.6619 (0.4918)	
training:	Epoch: [21][109/204]	Loss 0.3637 (0.4906)	
training:	Epoch: [21][110/204]	Loss 0.5391 (0.4910)	
training:	Epoch: [21][111/204]	Loss 0.3752 (0.4900)	
training:	Epoch: [21][112/204]	Loss 0.5186 (0.4902)	
training:	Epoch: [21][113/204]	Loss 0.4699 (0.4901)	
training:	Epoch: [21][114/204]	Loss 0.4274 (0.4895)	
training:	Epoch: [21][115/204]	Loss 0.5473 (0.4900)	
training:	Epoch: [21][116/204]	Loss 0.5541 (0.4906)	
training:	Epoch: [21][117/204]	Loss 0.4495 (0.4902)	
training:	Epoch: [21][118/204]	Loss 0.4548 (0.4899)	
training:	Epoch: [21][119/204]	Loss 0.4349 (0.4895)	
training:	Epoch: [21][120/204]	Loss 0.5197 (0.4897)	
training:	Epoch: [21][121/204]	Loss 0.5980 (0.4906)	
training:	Epoch: [21][122/204]	Loss 0.4982 (0.4907)	
training:	Epoch: [21][123/204]	Loss 0.4458 (0.4903)	
training:	Epoch: [21][124/204]	Loss 0.2894 (0.4887)	
training:	Epoch: [21][125/204]	Loss 0.4752 (0.4886)	
training:	Epoch: [21][126/204]	Loss 0.5279 (0.4889)	
training:	Epoch: [21][127/204]	Loss 0.4120 (0.4883)	
training:	Epoch: [21][128/204]	Loss 0.5042 (0.4884)	
training:	Epoch: [21][129/204]	Loss 0.5178 (0.4886)	
training:	Epoch: [21][130/204]	Loss 0.4694 (0.4885)	
training:	Epoch: [21][131/204]	Loss 0.4692 (0.4883)	
training:	Epoch: [21][132/204]	Loss 0.5263 (0.4886)	
training:	Epoch: [21][133/204]	Loss 0.4834 (0.4886)	
training:	Epoch: [21][134/204]	Loss 0.4416 (0.4882)	
training:	Epoch: [21][135/204]	Loss 0.5370 (0.4886)	
training:	Epoch: [21][136/204]	Loss 0.4269 (0.4881)	
training:	Epoch: [21][137/204]	Loss 0.4609 (0.4879)	
training:	Epoch: [21][138/204]	Loss 0.5832 (0.4886)	
training:	Epoch: [21][139/204]	Loss 0.4160 (0.4881)	
training:	Epoch: [21][140/204]	Loss 0.4982 (0.4882)	
training:	Epoch: [21][141/204]	Loss 0.4887 (0.4882)	
training:	Epoch: [21][142/204]	Loss 0.5023 (0.4883)	
training:	Epoch: [21][143/204]	Loss 0.6804 (0.4896)	
training:	Epoch: [21][144/204]	Loss 0.7154 (0.4912)	
training:	Epoch: [21][145/204]	Loss 0.4034 (0.4906)	
training:	Epoch: [21][146/204]	Loss 0.5441 (0.4910)	
training:	Epoch: [21][147/204]	Loss 0.4797 (0.4909)	
training:	Epoch: [21][148/204]	Loss 0.4438 (0.4906)	
training:	Epoch: [21][149/204]	Loss 0.6096 (0.4914)	
training:	Epoch: [21][150/204]	Loss 0.5853 (0.4920)	
training:	Epoch: [21][151/204]	Loss 0.4596 (0.4918)	
training:	Epoch: [21][152/204]	Loss 0.5208 (0.4920)	
training:	Epoch: [21][153/204]	Loss 0.4535 (0.4917)	
training:	Epoch: [21][154/204]	Loss 0.2885 (0.4904)	
training:	Epoch: [21][155/204]	Loss 0.4754 (0.4903)	
training:	Epoch: [21][156/204]	Loss 0.5751 (0.4908)	
training:	Epoch: [21][157/204]	Loss 0.4521 (0.4906)	
training:	Epoch: [21][158/204]	Loss 0.4279 (0.4902)	
training:	Epoch: [21][159/204]	Loss 0.5681 (0.4907)	
training:	Epoch: [21][160/204]	Loss 0.4755 (0.4906)	
training:	Epoch: [21][161/204]	Loss 0.3196 (0.4895)	
training:	Epoch: [21][162/204]	Loss 0.3669 (0.4888)	
training:	Epoch: [21][163/204]	Loss 0.5514 (0.4892)	
training:	Epoch: [21][164/204]	Loss 0.3592 (0.4884)	
training:	Epoch: [21][165/204]	Loss 0.5091 (0.4885)	
training:	Epoch: [21][166/204]	Loss 0.5413 (0.4888)	
training:	Epoch: [21][167/204]	Loss 0.5313 (0.4891)	
training:	Epoch: [21][168/204]	Loss 0.6206 (0.4898)	
training:	Epoch: [21][169/204]	Loss 0.5462 (0.4902)	
training:	Epoch: [21][170/204]	Loss 0.3584 (0.4894)	
training:	Epoch: [21][171/204]	Loss 0.5181 (0.4896)	
training:	Epoch: [21][172/204]	Loss 0.4223 (0.4892)	
training:	Epoch: [21][173/204]	Loss 0.4926 (0.4892)	
training:	Epoch: [21][174/204]	Loss 0.4146 (0.4888)	
training:	Epoch: [21][175/204]	Loss 0.2854 (0.4876)	
training:	Epoch: [21][176/204]	Loss 0.6002 (0.4882)	
training:	Epoch: [21][177/204]	Loss 0.5413 (0.4885)	
training:	Epoch: [21][178/204]	Loss 0.5008 (0.4886)	
training:	Epoch: [21][179/204]	Loss 0.5966 (0.4892)	
training:	Epoch: [21][180/204]	Loss 0.4975 (0.4893)	
training:	Epoch: [21][181/204]	Loss 0.4484 (0.4890)	
training:	Epoch: [21][182/204]	Loss 0.6093 (0.4897)	
training:	Epoch: [21][183/204]	Loss 0.4845 (0.4897)	
training:	Epoch: [21][184/204]	Loss 0.4406 (0.4894)	
training:	Epoch: [21][185/204]	Loss 0.5634 (0.4898)	
training:	Epoch: [21][186/204]	Loss 0.5998 (0.4904)	
training:	Epoch: [21][187/204]	Loss 0.8396 (0.4923)	
training:	Epoch: [21][188/204]	Loss 0.4811 (0.4922)	
training:	Epoch: [21][189/204]	Loss 0.4572 (0.4920)	
training:	Epoch: [21][190/204]	Loss 0.4546 (0.4918)	
training:	Epoch: [21][191/204]	Loss 0.5091 (0.4919)	
training:	Epoch: [21][192/204]	Loss 0.5280 (0.4921)	
training:	Epoch: [21][193/204]	Loss 0.6052 (0.4927)	
training:	Epoch: [21][194/204]	Loss 0.5199 (0.4928)	
training:	Epoch: [21][195/204]	Loss 0.4863 (0.4928)	
training:	Epoch: [21][196/204]	Loss 0.3159 (0.4919)	
training:	Epoch: [21][197/204]	Loss 0.3754 (0.4913)	
training:	Epoch: [21][198/204]	Loss 0.4521 (0.4911)	
training:	Epoch: [21][199/204]	Loss 0.5722 (0.4915)	
training:	Epoch: [21][200/204]	Loss 0.5362 (0.4917)	
training:	Epoch: [21][201/204]	Loss 0.5002 (0.4918)	
training:	Epoch: [21][202/204]	Loss 0.5569 (0.4921)	
training:	Epoch: [21][203/204]	Loss 0.5240 (0.4923)	
training:	Epoch: [21][204/204]	Loss 0.6524 (0.4930)	
Training:	 Loss: 0.4923

Training:	 ACC: 0.7745 0.7742 0.7678 0.7812
Validation:	 ACC: 0.7394 0.7394 0.7400 0.7388
Validation:	 Best_BACC: 0.7394 0.7394 0.7400 0.7388
Validation:	 Loss: 0.5269
Pretraining:	Epoch 22/200
----------
training:	Epoch: [22][1/204]	Loss 0.4436 (0.4436)	
training:	Epoch: [22][2/204]	Loss 0.4830 (0.4633)	
training:	Epoch: [22][3/204]	Loss 0.7202 (0.5489)	
training:	Epoch: [22][4/204]	Loss 0.4651 (0.5280)	
training:	Epoch: [22][5/204]	Loss 0.4602 (0.5144)	
training:	Epoch: [22][6/204]	Loss 0.4625 (0.5058)	
training:	Epoch: [22][7/204]	Loss 0.4038 (0.4912)	
training:	Epoch: [22][8/204]	Loss 0.4174 (0.4820)	
training:	Epoch: [22][9/204]	Loss 0.6167 (0.4969)	
training:	Epoch: [22][10/204]	Loss 0.4787 (0.4951)	
training:	Epoch: [22][11/204]	Loss 0.5983 (0.5045)	
training:	Epoch: [22][12/204]	Loss 0.3791 (0.4940)	
training:	Epoch: [22][13/204]	Loss 0.4696 (0.4922)	
training:	Epoch: [22][14/204]	Loss 0.4474 (0.4890)	
training:	Epoch: [22][15/204]	Loss 0.4126 (0.4839)	
training:	Epoch: [22][16/204]	Loss 0.3485 (0.4754)	
training:	Epoch: [22][17/204]	Loss 0.4929 (0.4764)	
training:	Epoch: [22][18/204]	Loss 0.5248 (0.4791)	
training:	Epoch: [22][19/204]	Loss 0.4882 (0.4796)	
training:	Epoch: [22][20/204]	Loss 0.5031 (0.4808)	
training:	Epoch: [22][21/204]	Loss 0.5273 (0.4830)	
training:	Epoch: [22][22/204]	Loss 0.5249 (0.4849)	
training:	Epoch: [22][23/204]	Loss 0.4853 (0.4849)	
training:	Epoch: [22][24/204]	Loss 0.6161 (0.4904)	
training:	Epoch: [22][25/204]	Loss 0.7417 (0.5004)	
training:	Epoch: [22][26/204]	Loss 0.4024 (0.4967)	
training:	Epoch: [22][27/204]	Loss 0.4758 (0.4959)	
training:	Epoch: [22][28/204]	Loss 0.4470 (0.4941)	
training:	Epoch: [22][29/204]	Loss 0.5055 (0.4945)	
training:	Epoch: [22][30/204]	Loss 0.5170 (0.4953)	
training:	Epoch: [22][31/204]	Loss 0.3915 (0.4919)	
training:	Epoch: [22][32/204]	Loss 0.4697 (0.4912)	
training:	Epoch: [22][33/204]	Loss 0.5506 (0.4930)	
training:	Epoch: [22][34/204]	Loss 0.4185 (0.4909)	
training:	Epoch: [22][35/204]	Loss 0.5747 (0.4932)	
training:	Epoch: [22][36/204]	Loss 0.6203 (0.4968)	
training:	Epoch: [22][37/204]	Loss 0.3781 (0.4936)	
training:	Epoch: [22][38/204]	Loss 0.3724 (0.4904)	
training:	Epoch: [22][39/204]	Loss 0.5033 (0.4907)	
training:	Epoch: [22][40/204]	Loss 0.4847 (0.4906)	
training:	Epoch: [22][41/204]	Loss 0.3859 (0.4880)	
training:	Epoch: [22][42/204]	Loss 0.3125 (0.4838)	
training:	Epoch: [22][43/204]	Loss 0.5815 (0.4861)	
training:	Epoch: [22][44/204]	Loss 0.4602 (0.4855)	
training:	Epoch: [22][45/204]	Loss 0.4102 (0.4838)	
training:	Epoch: [22][46/204]	Loss 0.4530 (0.4832)	
training:	Epoch: [22][47/204]	Loss 0.5290 (0.4841)	
training:	Epoch: [22][48/204]	Loss 0.5342 (0.4852)	
training:	Epoch: [22][49/204]	Loss 0.5409 (0.4863)	
training:	Epoch: [22][50/204]	Loss 0.4827 (0.4862)	
training:	Epoch: [22][51/204]	Loss 0.4479 (0.4855)	
training:	Epoch: [22][52/204]	Loss 0.4442 (0.4847)	
training:	Epoch: [22][53/204]	Loss 0.4613 (0.4843)	
training:	Epoch: [22][54/204]	Loss 0.4693 (0.4840)	
training:	Epoch: [22][55/204]	Loss 0.5921 (0.4860)	
training:	Epoch: [22][56/204]	Loss 0.4947 (0.4861)	
training:	Epoch: [22][57/204]	Loss 0.5603 (0.4874)	
training:	Epoch: [22][58/204]	Loss 0.5303 (0.4882)	
training:	Epoch: [22][59/204]	Loss 0.5292 (0.4888)	
training:	Epoch: [22][60/204]	Loss 0.5548 (0.4899)	
training:	Epoch: [22][61/204]	Loss 0.3978 (0.4884)	
training:	Epoch: [22][62/204]	Loss 0.7062 (0.4919)	
training:	Epoch: [22][63/204]	Loss 0.4014 (0.4905)	
training:	Epoch: [22][64/204]	Loss 0.5406 (0.4913)	
training:	Epoch: [22][65/204]	Loss 0.4721 (0.4910)	
training:	Epoch: [22][66/204]	Loss 0.4382 (0.4902)	
training:	Epoch: [22][67/204]	Loss 0.5255 (0.4907)	
training:	Epoch: [22][68/204]	Loss 0.4342 (0.4899)	
training:	Epoch: [22][69/204]	Loss 0.4983 (0.4900)	
training:	Epoch: [22][70/204]	Loss 0.5082 (0.4903)	
training:	Epoch: [22][71/204]	Loss 0.5027 (0.4904)	
training:	Epoch: [22][72/204]	Loss 0.6405 (0.4925)	
training:	Epoch: [22][73/204]	Loss 0.5972 (0.4940)	
training:	Epoch: [22][74/204]	Loss 0.4252 (0.4930)	
training:	Epoch: [22][75/204]	Loss 0.5155 (0.4933)	
training:	Epoch: [22][76/204]	Loss 0.5324 (0.4939)	
training:	Epoch: [22][77/204]	Loss 0.5484 (0.4946)	
training:	Epoch: [22][78/204]	Loss 0.4053 (0.4934)	
training:	Epoch: [22][79/204]	Loss 0.4940 (0.4934)	
training:	Epoch: [22][80/204]	Loss 0.4912 (0.4934)	
training:	Epoch: [22][81/204]	Loss 0.5513 (0.4941)	
training:	Epoch: [22][82/204]	Loss 0.5030 (0.4942)	
training:	Epoch: [22][83/204]	Loss 0.3740 (0.4928)	
training:	Epoch: [22][84/204]	Loss 0.3221 (0.4907)	
training:	Epoch: [22][85/204]	Loss 0.5402 (0.4913)	
training:	Epoch: [22][86/204]	Loss 0.4083 (0.4904)	
training:	Epoch: [22][87/204]	Loss 0.4505 (0.4899)	
training:	Epoch: [22][88/204]	Loss 0.4177 (0.4891)	
training:	Epoch: [22][89/204]	Loss 0.4638 (0.4888)	
training:	Epoch: [22][90/204]	Loss 0.5100 (0.4890)	
training:	Epoch: [22][91/204]	Loss 0.4577 (0.4887)	
training:	Epoch: [22][92/204]	Loss 0.5299 (0.4891)	
training:	Epoch: [22][93/204]	Loss 0.5079 (0.4893)	
training:	Epoch: [22][94/204]	Loss 0.3889 (0.4883)	
training:	Epoch: [22][95/204]	Loss 0.4412 (0.4878)	
training:	Epoch: [22][96/204]	Loss 0.3468 (0.4863)	
training:	Epoch: [22][97/204]	Loss 0.4377 (0.4858)	
training:	Epoch: [22][98/204]	Loss 0.7202 (0.4882)	
training:	Epoch: [22][99/204]	Loss 0.4811 (0.4881)	
training:	Epoch: [22][100/204]	Loss 0.5220 (0.4885)	
training:	Epoch: [22][101/204]	Loss 0.6025 (0.4896)	
training:	Epoch: [22][102/204]	Loss 0.5612 (0.4903)	
training:	Epoch: [22][103/204]	Loss 0.4457 (0.4899)	
training:	Epoch: [22][104/204]	Loss 0.4742 (0.4897)	
training:	Epoch: [22][105/204]	Loss 0.5136 (0.4899)	
training:	Epoch: [22][106/204]	Loss 0.4272 (0.4893)	
training:	Epoch: [22][107/204]	Loss 0.5018 (0.4895)	
training:	Epoch: [22][108/204]	Loss 0.4936 (0.4895)	
training:	Epoch: [22][109/204]	Loss 0.4899 (0.4895)	
training:	Epoch: [22][110/204]	Loss 0.6182 (0.4907)	
training:	Epoch: [22][111/204]	Loss 0.4125 (0.4900)	
training:	Epoch: [22][112/204]	Loss 0.5398 (0.4904)	
training:	Epoch: [22][113/204]	Loss 0.4883 (0.4904)	
training:	Epoch: [22][114/204]	Loss 0.4043 (0.4896)	
training:	Epoch: [22][115/204]	Loss 0.4996 (0.4897)	
training:	Epoch: [22][116/204]	Loss 0.5427 (0.4902)	
training:	Epoch: [22][117/204]	Loss 0.4128 (0.4895)	
training:	Epoch: [22][118/204]	Loss 0.3979 (0.4887)	
training:	Epoch: [22][119/204]	Loss 0.5305 (0.4891)	
training:	Epoch: [22][120/204]	Loss 0.3796 (0.4882)	
training:	Epoch: [22][121/204]	Loss 0.4799 (0.4881)	
training:	Epoch: [22][122/204]	Loss 0.4211 (0.4876)	
training:	Epoch: [22][123/204]	Loss 0.4561 (0.4873)	
training:	Epoch: [22][124/204]	Loss 0.5109 (0.4875)	
training:	Epoch: [22][125/204]	Loss 0.4852 (0.4875)	
training:	Epoch: [22][126/204]	Loss 0.5227 (0.4878)	
training:	Epoch: [22][127/204]	Loss 0.5575 (0.4883)	
training:	Epoch: [22][128/204]	Loss 0.5776 (0.4890)	
training:	Epoch: [22][129/204]	Loss 0.5702 (0.4896)	
training:	Epoch: [22][130/204]	Loss 0.4759 (0.4895)	
training:	Epoch: [22][131/204]	Loss 0.5035 (0.4896)	
training:	Epoch: [22][132/204]	Loss 0.4085 (0.4890)	
training:	Epoch: [22][133/204]	Loss 0.4600 (0.4888)	
training:	Epoch: [22][134/204]	Loss 0.5135 (0.4890)	
training:	Epoch: [22][135/204]	Loss 0.4427 (0.4886)	
training:	Epoch: [22][136/204]	Loss 0.4687 (0.4885)	
training:	Epoch: [22][137/204]	Loss 0.5707 (0.4891)	
training:	Epoch: [22][138/204]	Loss 0.6140 (0.4900)	
training:	Epoch: [22][139/204]	Loss 0.3580 (0.4891)	
training:	Epoch: [22][140/204]	Loss 0.5792 (0.4897)	
training:	Epoch: [22][141/204]	Loss 0.3661 (0.4888)	
training:	Epoch: [22][142/204]	Loss 0.4132 (0.4883)	
training:	Epoch: [22][143/204]	Loss 0.5189 (0.4885)	
training:	Epoch: [22][144/204]	Loss 0.6077 (0.4893)	
training:	Epoch: [22][145/204]	Loss 0.4680 (0.4892)	
training:	Epoch: [22][146/204]	Loss 0.3720 (0.4884)	
training:	Epoch: [22][147/204]	Loss 0.4151 (0.4879)	
training:	Epoch: [22][148/204]	Loss 0.4871 (0.4879)	
training:	Epoch: [22][149/204]	Loss 0.4588 (0.4877)	
training:	Epoch: [22][150/204]	Loss 0.6085 (0.4885)	
training:	Epoch: [22][151/204]	Loss 0.5999 (0.4892)	
training:	Epoch: [22][152/204]	Loss 0.5644 (0.4897)	
training:	Epoch: [22][153/204]	Loss 0.3456 (0.4888)	
training:	Epoch: [22][154/204]	Loss 0.6074 (0.4895)	
training:	Epoch: [22][155/204]	Loss 0.5568 (0.4900)	
training:	Epoch: [22][156/204]	Loss 0.3802 (0.4893)	
training:	Epoch: [22][157/204]	Loss 0.4311 (0.4889)	
training:	Epoch: [22][158/204]	Loss 0.6309 (0.4898)	
training:	Epoch: [22][159/204]	Loss 0.4210 (0.4894)	
training:	Epoch: [22][160/204]	Loss 0.4009 (0.4888)	
training:	Epoch: [22][161/204]	Loss 0.5530 (0.4892)	
training:	Epoch: [22][162/204]	Loss 0.5670 (0.4897)	
training:	Epoch: [22][163/204]	Loss 0.4099 (0.4892)	
training:	Epoch: [22][164/204]	Loss 0.5658 (0.4897)	
training:	Epoch: [22][165/204]	Loss 0.4765 (0.4896)	
training:	Epoch: [22][166/204]	Loss 0.5906 (0.4902)	
training:	Epoch: [22][167/204]	Loss 0.5465 (0.4905)	
training:	Epoch: [22][168/204]	Loss 0.4984 (0.4906)	
training:	Epoch: [22][169/204]	Loss 0.6221 (0.4914)	
training:	Epoch: [22][170/204]	Loss 0.5235 (0.4916)	
training:	Epoch: [22][171/204]	Loss 0.5088 (0.4917)	
training:	Epoch: [22][172/204]	Loss 0.5883 (0.4922)	
training:	Epoch: [22][173/204]	Loss 0.4546 (0.4920)	
training:	Epoch: [22][174/204]	Loss 0.4146 (0.4916)	
training:	Epoch: [22][175/204]	Loss 0.3081 (0.4905)	
training:	Epoch: [22][176/204]	Loss 0.4955 (0.4905)	
training:	Epoch: [22][177/204]	Loss 0.4717 (0.4904)	
training:	Epoch: [22][178/204]	Loss 0.4564 (0.4902)	
training:	Epoch: [22][179/204]	Loss 0.4275 (0.4899)	
training:	Epoch: [22][180/204]	Loss 0.4028 (0.4894)	
training:	Epoch: [22][181/204]	Loss 0.6490 (0.4903)	
training:	Epoch: [22][182/204]	Loss 0.4980 (0.4903)	
training:	Epoch: [22][183/204]	Loss 0.5455 (0.4906)	
training:	Epoch: [22][184/204]	Loss 0.3870 (0.4901)	
training:	Epoch: [22][185/204]	Loss 0.3316 (0.4892)	
training:	Epoch: [22][186/204]	Loss 0.3607 (0.4885)	
training:	Epoch: [22][187/204]	Loss 0.5613 (0.4889)	
training:	Epoch: [22][188/204]	Loss 0.5081 (0.4890)	
training:	Epoch: [22][189/204]	Loss 0.4361 (0.4887)	
training:	Epoch: [22][190/204]	Loss 0.3247 (0.4879)	
training:	Epoch: [22][191/204]	Loss 0.4520 (0.4877)	
training:	Epoch: [22][192/204]	Loss 0.4788 (0.4876)	
training:	Epoch: [22][193/204]	Loss 0.4337 (0.4874)	
training:	Epoch: [22][194/204]	Loss 0.3850 (0.4868)	
training:	Epoch: [22][195/204]	Loss 0.4583 (0.4867)	
training:	Epoch: [22][196/204]	Loss 0.5599 (0.4871)	
training:	Epoch: [22][197/204]	Loss 0.4215 (0.4867)	
training:	Epoch: [22][198/204]	Loss 0.5003 (0.4868)	
training:	Epoch: [22][199/204]	Loss 0.4235 (0.4865)	
training:	Epoch: [22][200/204]	Loss 0.4464 (0.4863)	
training:	Epoch: [22][201/204]	Loss 0.4225 (0.4860)	
training:	Epoch: [22][202/204]	Loss 0.4031 (0.4855)	
training:	Epoch: [22][203/204]	Loss 0.4579 (0.4854)	
training:	Epoch: [22][204/204]	Loss 0.5537 (0.4857)	
Training:	 Loss: 0.4850

Training:	 ACC: 0.7798 0.7799 0.7828 0.7768
Validation:	 ACC: 0.7405 0.7410 0.7523 0.7287
Validation:	 Best_BACC: 0.7405 0.7410 0.7523 0.7287
Validation:	 Loss: 0.5237
Pretraining:	Epoch 23/200
----------
training:	Epoch: [23][1/204]	Loss 0.5848 (0.5848)	
training:	Epoch: [23][2/204]	Loss 0.4495 (0.5171)	
training:	Epoch: [23][3/204]	Loss 0.4658 (0.5000)	
training:	Epoch: [23][4/204]	Loss 0.4449 (0.4862)	
training:	Epoch: [23][5/204]	Loss 0.4331 (0.4756)	
training:	Epoch: [23][6/204]	Loss 0.5581 (0.4894)	
training:	Epoch: [23][7/204]	Loss 0.4880 (0.4892)	
training:	Epoch: [23][8/204]	Loss 0.5342 (0.4948)	
training:	Epoch: [23][9/204]	Loss 0.3965 (0.4839)	
training:	Epoch: [23][10/204]	Loss 0.4036 (0.4758)	
training:	Epoch: [23][11/204]	Loss 0.5140 (0.4793)	
training:	Epoch: [23][12/204]	Loss 0.3988 (0.4726)	
training:	Epoch: [23][13/204]	Loss 0.4514 (0.4710)	
training:	Epoch: [23][14/204]	Loss 0.5394 (0.4759)	
training:	Epoch: [23][15/204]	Loss 0.5097 (0.4781)	
training:	Epoch: [23][16/204]	Loss 0.4012 (0.4733)	
training:	Epoch: [23][17/204]	Loss 0.4958 (0.4746)	
training:	Epoch: [23][18/204]	Loss 0.2998 (0.4649)	
training:	Epoch: [23][19/204]	Loss 0.6198 (0.4731)	
training:	Epoch: [23][20/204]	Loss 0.4648 (0.4727)	
training:	Epoch: [23][21/204]	Loss 0.4441 (0.4713)	
training:	Epoch: [23][22/204]	Loss 0.5610 (0.4754)	
training:	Epoch: [23][23/204]	Loss 0.4230 (0.4731)	
training:	Epoch: [23][24/204]	Loss 0.3728 (0.4689)	
training:	Epoch: [23][25/204]	Loss 0.3803 (0.4654)	
training:	Epoch: [23][26/204]	Loss 0.6239 (0.4715)	
training:	Epoch: [23][27/204]	Loss 0.5322 (0.4737)	
training:	Epoch: [23][28/204]	Loss 0.5912 (0.4779)	
training:	Epoch: [23][29/204]	Loss 0.3620 (0.4739)	
training:	Epoch: [23][30/204]	Loss 0.4228 (0.4722)	
training:	Epoch: [23][31/204]	Loss 0.3755 (0.4691)	
training:	Epoch: [23][32/204]	Loss 0.5097 (0.4704)	
training:	Epoch: [23][33/204]	Loss 0.3555 (0.4669)	
training:	Epoch: [23][34/204]	Loss 0.5998 (0.4708)	
training:	Epoch: [23][35/204]	Loss 0.6122 (0.4748)	
training:	Epoch: [23][36/204]	Loss 0.6404 (0.4794)	
training:	Epoch: [23][37/204]	Loss 0.5931 (0.4825)	
training:	Epoch: [23][38/204]	Loss 0.4302 (0.4811)	
training:	Epoch: [23][39/204]	Loss 0.4054 (0.4792)	
training:	Epoch: [23][40/204]	Loss 0.4997 (0.4797)	
training:	Epoch: [23][41/204]	Loss 0.4767 (0.4796)	
training:	Epoch: [23][42/204]	Loss 0.4302 (0.4784)	
training:	Epoch: [23][43/204]	Loss 0.5744 (0.4807)	
training:	Epoch: [23][44/204]	Loss 0.3461 (0.4776)	
training:	Epoch: [23][45/204]	Loss 0.3882 (0.4756)	
training:	Epoch: [23][46/204]	Loss 0.6303 (0.4790)	
training:	Epoch: [23][47/204]	Loss 0.4586 (0.4786)	
training:	Epoch: [23][48/204]	Loss 0.3316 (0.4755)	
training:	Epoch: [23][49/204]	Loss 0.5157 (0.4763)	
training:	Epoch: [23][50/204]	Loss 0.3700 (0.4742)	
training:	Epoch: [23][51/204]	Loss 0.5588 (0.4758)	
training:	Epoch: [23][52/204]	Loss 0.5283 (0.4769)	
training:	Epoch: [23][53/204]	Loss 0.4934 (0.4772)	
training:	Epoch: [23][54/204]	Loss 0.5855 (0.4792)	
training:	Epoch: [23][55/204]	Loss 0.5688 (0.4808)	
training:	Epoch: [23][56/204]	Loss 0.4470 (0.4802)	
training:	Epoch: [23][57/204]	Loss 0.3787 (0.4784)	
training:	Epoch: [23][58/204]	Loss 0.4933 (0.4787)	
training:	Epoch: [23][59/204]	Loss 0.6276 (0.4812)	
training:	Epoch: [23][60/204]	Loss 0.4414 (0.4805)	
training:	Epoch: [23][61/204]	Loss 0.4952 (0.4808)	
training:	Epoch: [23][62/204]	Loss 0.4571 (0.4804)	
training:	Epoch: [23][63/204]	Loss 0.4091 (0.4793)	
training:	Epoch: [23][64/204]	Loss 0.6184 (0.4814)	
training:	Epoch: [23][65/204]	Loss 0.4743 (0.4813)	
training:	Epoch: [23][66/204]	Loss 0.5145 (0.4818)	
training:	Epoch: [23][67/204]	Loss 0.4672 (0.4816)	
training:	Epoch: [23][68/204]	Loss 0.5749 (0.4830)	
training:	Epoch: [23][69/204]	Loss 0.5078 (0.4833)	
training:	Epoch: [23][70/204]	Loss 0.4458 (0.4828)	
training:	Epoch: [23][71/204]	Loss 0.4860 (0.4829)	
training:	Epoch: [23][72/204]	Loss 0.5541 (0.4838)	
training:	Epoch: [23][73/204]	Loss 0.5492 (0.4847)	
training:	Epoch: [23][74/204]	Loss 0.3642 (0.4831)	
training:	Epoch: [23][75/204]	Loss 0.4799 (0.4831)	
training:	Epoch: [23][76/204]	Loss 0.4599 (0.4828)	
training:	Epoch: [23][77/204]	Loss 0.5326 (0.4834)	
training:	Epoch: [23][78/204]	Loss 0.5765 (0.4846)	
training:	Epoch: [23][79/204]	Loss 0.4554 (0.4842)	
training:	Epoch: [23][80/204]	Loss 0.5436 (0.4850)	
training:	Epoch: [23][81/204]	Loss 0.5271 (0.4855)	
training:	Epoch: [23][82/204]	Loss 0.3097 (0.4833)	
training:	Epoch: [23][83/204]	Loss 0.4712 (0.4832)	
training:	Epoch: [23][84/204]	Loss 0.6163 (0.4848)	
training:	Epoch: [23][85/204]	Loss 0.6170 (0.4863)	
training:	Epoch: [23][86/204]	Loss 0.4645 (0.4861)	
training:	Epoch: [23][87/204]	Loss 0.5632 (0.4870)	
training:	Epoch: [23][88/204]	Loss 0.5453 (0.4876)	
training:	Epoch: [23][89/204]	Loss 0.4900 (0.4877)	
training:	Epoch: [23][90/204]	Loss 0.5023 (0.4878)	
training:	Epoch: [23][91/204]	Loss 0.3973 (0.4868)	
training:	Epoch: [23][92/204]	Loss 0.4652 (0.4866)	
training:	Epoch: [23][93/204]	Loss 0.3302 (0.4849)	
training:	Epoch: [23][94/204]	Loss 0.5087 (0.4852)	
training:	Epoch: [23][95/204]	Loss 0.4459 (0.4848)	
training:	Epoch: [23][96/204]	Loss 0.4549 (0.4844)	
training:	Epoch: [23][97/204]	Loss 0.5349 (0.4850)	
training:	Epoch: [23][98/204]	Loss 0.5042 (0.4852)	
training:	Epoch: [23][99/204]	Loss 0.5335 (0.4856)	
training:	Epoch: [23][100/204]	Loss 0.4738 (0.4855)	
training:	Epoch: [23][101/204]	Loss 0.5373 (0.4860)	
training:	Epoch: [23][102/204]	Loss 0.4421 (0.4856)	
training:	Epoch: [23][103/204]	Loss 0.3824 (0.4846)	
training:	Epoch: [23][104/204]	Loss 0.4646 (0.4844)	
training:	Epoch: [23][105/204]	Loss 0.4905 (0.4845)	
training:	Epoch: [23][106/204]	Loss 0.4853 (0.4845)	
training:	Epoch: [23][107/204]	Loss 0.5910 (0.4855)	
training:	Epoch: [23][108/204]	Loss 0.6332 (0.4868)	
training:	Epoch: [23][109/204]	Loss 0.5623 (0.4875)	
training:	Epoch: [23][110/204]	Loss 0.4312 (0.4870)	
training:	Epoch: [23][111/204]	Loss 0.4439 (0.4866)	
training:	Epoch: [23][112/204]	Loss 0.4700 (0.4865)	
training:	Epoch: [23][113/204]	Loss 0.3831 (0.4856)	
training:	Epoch: [23][114/204]	Loss 0.3845 (0.4847)	
training:	Epoch: [23][115/204]	Loss 0.3718 (0.4837)	
training:	Epoch: [23][116/204]	Loss 0.5642 (0.4844)	
training:	Epoch: [23][117/204]	Loss 0.4384 (0.4840)	
training:	Epoch: [23][118/204]	Loss 0.5565 (0.4846)	
training:	Epoch: [23][119/204]	Loss 0.5785 (0.4854)	
training:	Epoch: [23][120/204]	Loss 0.3051 (0.4839)	
training:	Epoch: [23][121/204]	Loss 0.5099 (0.4841)	
training:	Epoch: [23][122/204]	Loss 0.5463 (0.4846)	
training:	Epoch: [23][123/204]	Loss 0.3742 (0.4837)	
training:	Epoch: [23][124/204]	Loss 0.4523 (0.4835)	
training:	Epoch: [23][125/204]	Loss 0.5200 (0.4838)	
training:	Epoch: [23][126/204]	Loss 0.4846 (0.4838)	
training:	Epoch: [23][127/204]	Loss 0.5489 (0.4843)	
training:	Epoch: [23][128/204]	Loss 0.4855 (0.4843)	
training:	Epoch: [23][129/204]	Loss 0.4220 (0.4838)	
training:	Epoch: [23][130/204]	Loss 0.4774 (0.4838)	
training:	Epoch: [23][131/204]	Loss 0.5809 (0.4845)	
training:	Epoch: [23][132/204]	Loss 0.4950 (0.4846)	
training:	Epoch: [23][133/204]	Loss 0.3997 (0.4840)	
training:	Epoch: [23][134/204]	Loss 0.4786 (0.4839)	
training:	Epoch: [23][135/204]	Loss 0.4759 (0.4839)	
training:	Epoch: [23][136/204]	Loss 0.5245 (0.4842)	
training:	Epoch: [23][137/204]	Loss 0.4700 (0.4840)	
training:	Epoch: [23][138/204]	Loss 0.4252 (0.4836)	
training:	Epoch: [23][139/204]	Loss 0.4511 (0.4834)	
training:	Epoch: [23][140/204]	Loss 0.3998 (0.4828)	
training:	Epoch: [23][141/204]	Loss 0.6079 (0.4837)	
training:	Epoch: [23][142/204]	Loss 0.5708 (0.4843)	
training:	Epoch: [23][143/204]	Loss 0.4347 (0.4839)	
training:	Epoch: [23][144/204]	Loss 0.4480 (0.4837)	
training:	Epoch: [23][145/204]	Loss 0.4210 (0.4833)	
training:	Epoch: [23][146/204]	Loss 0.4748 (0.4832)	
training:	Epoch: [23][147/204]	Loss 0.4920 (0.4833)	
training:	Epoch: [23][148/204]	Loss 0.5243 (0.4835)	
training:	Epoch: [23][149/204]	Loss 0.4727 (0.4835)	
training:	Epoch: [23][150/204]	Loss 0.4441 (0.4832)	
training:	Epoch: [23][151/204]	Loss 0.5588 (0.4837)	
training:	Epoch: [23][152/204]	Loss 0.4286 (0.4833)	
training:	Epoch: [23][153/204]	Loss 0.4430 (0.4831)	
training:	Epoch: [23][154/204]	Loss 0.5681 (0.4836)	
training:	Epoch: [23][155/204]	Loss 0.4228 (0.4832)	
training:	Epoch: [23][156/204]	Loss 0.3908 (0.4826)	
training:	Epoch: [23][157/204]	Loss 0.3832 (0.4820)	
training:	Epoch: [23][158/204]	Loss 0.4876 (0.4820)	
training:	Epoch: [23][159/204]	Loss 0.4780 (0.4820)	
training:	Epoch: [23][160/204]	Loss 0.4935 (0.4821)	
training:	Epoch: [23][161/204]	Loss 0.4772 (0.4821)	
training:	Epoch: [23][162/204]	Loss 0.4352 (0.4818)	
training:	Epoch: [23][163/204]	Loss 0.5089 (0.4819)	
training:	Epoch: [23][164/204]	Loss 0.6022 (0.4827)	
training:	Epoch: [23][165/204]	Loss 0.4424 (0.4824)	
training:	Epoch: [23][166/204]	Loss 0.3462 (0.4816)	
training:	Epoch: [23][167/204]	Loss 0.5104 (0.4818)	
training:	Epoch: [23][168/204]	Loss 0.4447 (0.4816)	
training:	Epoch: [23][169/204]	Loss 0.4281 (0.4812)	
training:	Epoch: [23][170/204]	Loss 0.3877 (0.4807)	
training:	Epoch: [23][171/204]	Loss 0.5263 (0.4810)	
training:	Epoch: [23][172/204]	Loss 0.5404 (0.4813)	
training:	Epoch: [23][173/204]	Loss 0.4547 (0.4812)	
training:	Epoch: [23][174/204]	Loss 0.4742 (0.4811)	
training:	Epoch: [23][175/204]	Loss 0.4613 (0.4810)	
training:	Epoch: [23][176/204]	Loss 0.3551 (0.4803)	
training:	Epoch: [23][177/204]	Loss 0.6116 (0.4810)	
training:	Epoch: [23][178/204]	Loss 0.5545 (0.4814)	
training:	Epoch: [23][179/204]	Loss 0.3393 (0.4806)	
training:	Epoch: [23][180/204]	Loss 0.5006 (0.4808)	
training:	Epoch: [23][181/204]	Loss 0.4505 (0.4806)	
training:	Epoch: [23][182/204]	Loss 0.5757 (0.4811)	
training:	Epoch: [23][183/204]	Loss 0.6237 (0.4819)	
training:	Epoch: [23][184/204]	Loss 0.6014 (0.4825)	
training:	Epoch: [23][185/204]	Loss 0.4824 (0.4825)	
training:	Epoch: [23][186/204]	Loss 0.4537 (0.4824)	
training:	Epoch: [23][187/204]	Loss 0.3629 (0.4817)	
training:	Epoch: [23][188/204]	Loss 0.5774 (0.4823)	
training:	Epoch: [23][189/204]	Loss 0.4477 (0.4821)	
training:	Epoch: [23][190/204]	Loss 0.5218 (0.4823)	
training:	Epoch: [23][191/204]	Loss 0.4393 (0.4821)	
training:	Epoch: [23][192/204]	Loss 0.4993 (0.4821)	
training:	Epoch: [23][193/204]	Loss 0.4755 (0.4821)	
training:	Epoch: [23][194/204]	Loss 0.6629 (0.4830)	
training:	Epoch: [23][195/204]	Loss 0.4047 (0.4826)	
training:	Epoch: [23][196/204]	Loss 0.4948 (0.4827)	
training:	Epoch: [23][197/204]	Loss 0.4931 (0.4828)	
training:	Epoch: [23][198/204]	Loss 0.4608 (0.4826)	
training:	Epoch: [23][199/204]	Loss 0.4084 (0.4823)	
training:	Epoch: [23][200/204]	Loss 0.4256 (0.4820)	
training:	Epoch: [23][201/204]	Loss 0.5140 (0.4821)	
training:	Epoch: [23][202/204]	Loss 0.4807 (0.4821)	
training:	Epoch: [23][203/204]	Loss 0.5103 (0.4823)	
training:	Epoch: [23][204/204]	Loss 0.4862 (0.4823)	
Training:	 Loss: 0.4816

Training:	 ACC: 0.7832 0.7836 0.7916 0.7749
Validation:	 ACC: 0.7449 0.7459 0.7656 0.7242
Validation:	 Best_BACC: 0.7449 0.7459 0.7656 0.7242
Validation:	 Loss: 0.5211
Pretraining:	Epoch 24/200
----------
training:	Epoch: [24][1/204]	Loss 0.4927 (0.4927)	
training:	Epoch: [24][2/204]	Loss 0.4149 (0.4538)	
training:	Epoch: [24][3/204]	Loss 0.4416 (0.4497)	
training:	Epoch: [24][4/204]	Loss 0.5829 (0.4830)	
training:	Epoch: [24][5/204]	Loss 0.4608 (0.4786)	
training:	Epoch: [24][6/204]	Loss 0.3894 (0.4637)	
training:	Epoch: [24][7/204]	Loss 0.3974 (0.4542)	
training:	Epoch: [24][8/204]	Loss 0.4182 (0.4497)	
training:	Epoch: [24][9/204]	Loss 0.5313 (0.4588)	
training:	Epoch: [24][10/204]	Loss 0.3898 (0.4519)	
training:	Epoch: [24][11/204]	Loss 0.4292 (0.4498)	
training:	Epoch: [24][12/204]	Loss 0.6152 (0.4636)	
training:	Epoch: [24][13/204]	Loss 0.5057 (0.4669)	
training:	Epoch: [24][14/204]	Loss 0.3290 (0.4570)	
training:	Epoch: [24][15/204]	Loss 0.5293 (0.4618)	
training:	Epoch: [24][16/204]	Loss 0.3357 (0.4539)	
training:	Epoch: [24][17/204]	Loss 0.4316 (0.4526)	
training:	Epoch: [24][18/204]	Loss 0.6153 (0.4617)	
training:	Epoch: [24][19/204]	Loss 0.6430 (0.4712)	
training:	Epoch: [24][20/204]	Loss 0.5198 (0.4736)	
training:	Epoch: [24][21/204]	Loss 0.5132 (0.4755)	
training:	Epoch: [24][22/204]	Loss 0.4371 (0.4738)	
training:	Epoch: [24][23/204]	Loss 0.4695 (0.4736)	
training:	Epoch: [24][24/204]	Loss 0.3999 (0.4705)	
training:	Epoch: [24][25/204]	Loss 0.4099 (0.4681)	
training:	Epoch: [24][26/204]	Loss 0.4644 (0.4680)	
training:	Epoch: [24][27/204]	Loss 0.4725 (0.4681)	
training:	Epoch: [24][28/204]	Loss 0.5789 (0.4721)	
training:	Epoch: [24][29/204]	Loss 0.5965 (0.4764)	
training:	Epoch: [24][30/204]	Loss 0.4969 (0.4771)	
training:	Epoch: [24][31/204]	Loss 0.4930 (0.4776)	
training:	Epoch: [24][32/204]	Loss 0.5840 (0.4809)	
training:	Epoch: [24][33/204]	Loss 0.5236 (0.4822)	
training:	Epoch: [24][34/204]	Loss 0.3625 (0.4787)	
training:	Epoch: [24][35/204]	Loss 0.4533 (0.4779)	
training:	Epoch: [24][36/204]	Loss 0.4044 (0.4759)	
training:	Epoch: [24][37/204]	Loss 0.5370 (0.4776)	
training:	Epoch: [24][38/204]	Loss 0.4670 (0.4773)	
training:	Epoch: [24][39/204]	Loss 0.3945 (0.4752)	
training:	Epoch: [24][40/204]	Loss 0.4448 (0.4744)	
training:	Epoch: [24][41/204]	Loss 0.4589 (0.4740)	
training:	Epoch: [24][42/204]	Loss 0.5154 (0.4750)	
training:	Epoch: [24][43/204]	Loss 0.5425 (0.4766)	
training:	Epoch: [24][44/204]	Loss 0.4132 (0.4751)	
training:	Epoch: [24][45/204]	Loss 0.5034 (0.4758)	
training:	Epoch: [24][46/204]	Loss 0.6658 (0.4799)	
training:	Epoch: [24][47/204]	Loss 0.4185 (0.4786)	
training:	Epoch: [24][48/204]	Loss 0.4758 (0.4785)	
training:	Epoch: [24][49/204]	Loss 0.5173 (0.4793)	
training:	Epoch: [24][50/204]	Loss 0.6917 (0.4836)	
training:	Epoch: [24][51/204]	Loss 0.3907 (0.4817)	
training:	Epoch: [24][52/204]	Loss 0.3872 (0.4799)	
training:	Epoch: [24][53/204]	Loss 0.5762 (0.4817)	
training:	Epoch: [24][54/204]	Loss 0.5474 (0.4830)	
training:	Epoch: [24][55/204]	Loss 0.4652 (0.4826)	
training:	Epoch: [24][56/204]	Loss 0.4527 (0.4821)	
training:	Epoch: [24][57/204]	Loss 0.4255 (0.4811)	
training:	Epoch: [24][58/204]	Loss 0.4347 (0.4803)	
training:	Epoch: [24][59/204]	Loss 0.4876 (0.4804)	
training:	Epoch: [24][60/204]	Loss 0.5779 (0.4821)	
training:	Epoch: [24][61/204]	Loss 0.4197 (0.4810)	
training:	Epoch: [24][62/204]	Loss 0.5455 (0.4821)	
training:	Epoch: [24][63/204]	Loss 0.4408 (0.4814)	
training:	Epoch: [24][64/204]	Loss 0.4944 (0.4816)	
training:	Epoch: [24][65/204]	Loss 0.5960 (0.4834)	
training:	Epoch: [24][66/204]	Loss 0.3898 (0.4820)	
training:	Epoch: [24][67/204]	Loss 0.4678 (0.4818)	
training:	Epoch: [24][68/204]	Loss 0.6083 (0.4836)	
training:	Epoch: [24][69/204]	Loss 0.5231 (0.4842)	
training:	Epoch: [24][70/204]	Loss 0.4498 (0.4837)	
training:	Epoch: [24][71/204]	Loss 0.4819 (0.4837)	
training:	Epoch: [24][72/204]	Loss 0.6613 (0.4861)	
training:	Epoch: [24][73/204]	Loss 0.3908 (0.4848)	
training:	Epoch: [24][74/204]	Loss 0.3658 (0.4832)	
training:	Epoch: [24][75/204]	Loss 0.4280 (0.4825)	
training:	Epoch: [24][76/204]	Loss 0.4933 (0.4826)	
training:	Epoch: [24][77/204]	Loss 0.4638 (0.4824)	
training:	Epoch: [24][78/204]	Loss 0.6504 (0.4845)	
training:	Epoch: [24][79/204]	Loss 0.4972 (0.4847)	
training:	Epoch: [24][80/204]	Loss 0.5217 (0.4852)	
training:	Epoch: [24][81/204]	Loss 0.5359 (0.4858)	
training:	Epoch: [24][82/204]	Loss 0.5208 (0.4862)	
training:	Epoch: [24][83/204]	Loss 0.5121 (0.4865)	
training:	Epoch: [24][84/204]	Loss 0.4591 (0.4862)	
training:	Epoch: [24][85/204]	Loss 0.5394 (0.4868)	
training:	Epoch: [24][86/204]	Loss 0.6506 (0.4887)	
training:	Epoch: [24][87/204]	Loss 0.4877 (0.4887)	
training:	Epoch: [24][88/204]	Loss 0.4703 (0.4885)	
training:	Epoch: [24][89/204]	Loss 0.5304 (0.4890)	
training:	Epoch: [24][90/204]	Loss 0.4358 (0.4884)	
training:	Epoch: [24][91/204]	Loss 0.5108 (0.4886)	
training:	Epoch: [24][92/204]	Loss 0.4638 (0.4884)	
training:	Epoch: [24][93/204]	Loss 0.4943 (0.4884)	
training:	Epoch: [24][94/204]	Loss 0.4914 (0.4885)	
training:	Epoch: [24][95/204]	Loss 0.3460 (0.4870)	
training:	Epoch: [24][96/204]	Loss 0.5991 (0.4881)	
training:	Epoch: [24][97/204]	Loss 0.4821 (0.4881)	
training:	Epoch: [24][98/204]	Loss 0.4113 (0.4873)	
training:	Epoch: [24][99/204]	Loss 0.4897 (0.4873)	
training:	Epoch: [24][100/204]	Loss 0.4765 (0.4872)	
training:	Epoch: [24][101/204]	Loss 0.4399 (0.4867)	
training:	Epoch: [24][102/204]	Loss 0.3836 (0.4857)	
training:	Epoch: [24][103/204]	Loss 0.3368 (0.4843)	
training:	Epoch: [24][104/204]	Loss 0.4074 (0.4835)	
training:	Epoch: [24][105/204]	Loss 0.3962 (0.4827)	
training:	Epoch: [24][106/204]	Loss 0.5348 (0.4832)	
training:	Epoch: [24][107/204]	Loss 0.4115 (0.4825)	
training:	Epoch: [24][108/204]	Loss 0.6012 (0.4836)	
training:	Epoch: [24][109/204]	Loss 0.5197 (0.4840)	
training:	Epoch: [24][110/204]	Loss 0.5788 (0.4848)	
training:	Epoch: [24][111/204]	Loss 0.4720 (0.4847)	
training:	Epoch: [24][112/204]	Loss 0.7046 (0.4867)	
training:	Epoch: [24][113/204]	Loss 0.4032 (0.4859)	
training:	Epoch: [24][114/204]	Loss 0.4558 (0.4857)	
training:	Epoch: [24][115/204]	Loss 0.4092 (0.4850)	
training:	Epoch: [24][116/204]	Loss 0.4678 (0.4848)	
training:	Epoch: [24][117/204]	Loss 0.4686 (0.4847)	
training:	Epoch: [24][118/204]	Loss 0.4049 (0.4840)	
training:	Epoch: [24][119/204]	Loss 0.4445 (0.4837)	
training:	Epoch: [24][120/204]	Loss 0.4912 (0.4838)	
training:	Epoch: [24][121/204]	Loss 0.4917 (0.4838)	
training:	Epoch: [24][122/204]	Loss 0.3839 (0.4830)	
training:	Epoch: [24][123/204]	Loss 0.4780 (0.4830)	
training:	Epoch: [24][124/204]	Loss 0.4423 (0.4826)	
training:	Epoch: [24][125/204]	Loss 0.5850 (0.4835)	
training:	Epoch: [24][126/204]	Loss 0.4347 (0.4831)	
training:	Epoch: [24][127/204]	Loss 0.4066 (0.4825)	
training:	Epoch: [24][128/204]	Loss 0.4361 (0.4821)	
training:	Epoch: [24][129/204]	Loss 0.4911 (0.4822)	
training:	Epoch: [24][130/204]	Loss 0.4851 (0.4822)	
training:	Epoch: [24][131/204]	Loss 0.5078 (0.4824)	
training:	Epoch: [24][132/204]	Loss 0.3514 (0.4814)	
training:	Epoch: [24][133/204]	Loss 0.5236 (0.4817)	
training:	Epoch: [24][134/204]	Loss 0.5200 (0.4820)	
training:	Epoch: [24][135/204]	Loss 0.5126 (0.4822)	
training:	Epoch: [24][136/204]	Loss 0.5540 (0.4828)	
training:	Epoch: [24][137/204]	Loss 0.6089 (0.4837)	
training:	Epoch: [24][138/204]	Loss 0.4359 (0.4833)	
training:	Epoch: [24][139/204]	Loss 0.4016 (0.4827)	
training:	Epoch: [24][140/204]	Loss 0.4993 (0.4829)	
training:	Epoch: [24][141/204]	Loss 0.3674 (0.4820)	
training:	Epoch: [24][142/204]	Loss 0.5493 (0.4825)	
training:	Epoch: [24][143/204]	Loss 0.3541 (0.4816)	
training:	Epoch: [24][144/204]	Loss 0.5307 (0.4820)	
training:	Epoch: [24][145/204]	Loss 0.3710 (0.4812)	
training:	Epoch: [24][146/204]	Loss 0.5713 (0.4818)	
training:	Epoch: [24][147/204]	Loss 0.5546 (0.4823)	
training:	Epoch: [24][148/204]	Loss 0.4008 (0.4818)	
training:	Epoch: [24][149/204]	Loss 0.3838 (0.4811)	
training:	Epoch: [24][150/204]	Loss 0.4540 (0.4809)	
training:	Epoch: [24][151/204]	Loss 0.6746 (0.4822)	
training:	Epoch: [24][152/204]	Loss 0.4319 (0.4819)	
training:	Epoch: [24][153/204]	Loss 0.5953 (0.4826)	
training:	Epoch: [24][154/204]	Loss 0.3424 (0.4817)	
training:	Epoch: [24][155/204]	Loss 0.4758 (0.4817)	
training:	Epoch: [24][156/204]	Loss 0.4769 (0.4816)	
training:	Epoch: [24][157/204]	Loss 0.3467 (0.4808)	
training:	Epoch: [24][158/204]	Loss 0.5374 (0.4811)	
training:	Epoch: [24][159/204]	Loss 0.4476 (0.4809)	
training:	Epoch: [24][160/204]	Loss 0.4194 (0.4805)	
training:	Epoch: [24][161/204]	Loss 0.3282 (0.4796)	
training:	Epoch: [24][162/204]	Loss 0.3988 (0.4791)	
training:	Epoch: [24][163/204]	Loss 0.3829 (0.4785)	
training:	Epoch: [24][164/204]	Loss 0.5463 (0.4789)	
training:	Epoch: [24][165/204]	Loss 0.5246 (0.4792)	
training:	Epoch: [24][166/204]	Loss 0.4685 (0.4791)	
training:	Epoch: [24][167/204]	Loss 0.3625 (0.4784)	
training:	Epoch: [24][168/204]	Loss 0.4569 (0.4783)	
training:	Epoch: [24][169/204]	Loss 0.4792 (0.4783)	
training:	Epoch: [24][170/204]	Loss 0.4021 (0.4779)	
training:	Epoch: [24][171/204]	Loss 0.5861 (0.4785)	
training:	Epoch: [24][172/204]	Loss 0.5444 (0.4789)	
training:	Epoch: [24][173/204]	Loss 0.4405 (0.4787)	
training:	Epoch: [24][174/204]	Loss 0.3226 (0.4778)	
training:	Epoch: [24][175/204]	Loss 0.5212 (0.4780)	
training:	Epoch: [24][176/204]	Loss 0.5584 (0.4785)	
training:	Epoch: [24][177/204]	Loss 0.5867 (0.4791)	
training:	Epoch: [24][178/204]	Loss 0.4781 (0.4791)	
training:	Epoch: [24][179/204]	Loss 0.3762 (0.4785)	
training:	Epoch: [24][180/204]	Loss 0.4281 (0.4782)	
training:	Epoch: [24][181/204]	Loss 0.5343 (0.4785)	
training:	Epoch: [24][182/204]	Loss 0.3459 (0.4778)	
training:	Epoch: [24][183/204]	Loss 0.4506 (0.4776)	
training:	Epoch: [24][184/204]	Loss 0.4050 (0.4773)	
training:	Epoch: [24][185/204]	Loss 0.4305 (0.4770)	
training:	Epoch: [24][186/204]	Loss 0.4251 (0.4767)	
training:	Epoch: [24][187/204]	Loss 0.4307 (0.4765)	
training:	Epoch: [24][188/204]	Loss 0.6100 (0.4772)	
training:	Epoch: [24][189/204]	Loss 0.4978 (0.4773)	
training:	Epoch: [24][190/204]	Loss 0.7143 (0.4785)	
training:	Epoch: [24][191/204]	Loss 0.3858 (0.4781)	
training:	Epoch: [24][192/204]	Loss 0.5027 (0.4782)	
training:	Epoch: [24][193/204]	Loss 0.4565 (0.4781)	
training:	Epoch: [24][194/204]	Loss 0.6487 (0.4789)	
training:	Epoch: [24][195/204]	Loss 0.3240 (0.4782)	
training:	Epoch: [24][196/204]	Loss 0.4495 (0.4780)	
training:	Epoch: [24][197/204]	Loss 0.3658 (0.4774)	
training:	Epoch: [24][198/204]	Loss 0.3154 (0.4766)	
training:	Epoch: [24][199/204]	Loss 0.3491 (0.4760)	
training:	Epoch: [24][200/204]	Loss 0.4452 (0.4758)	
training:	Epoch: [24][201/204]	Loss 0.4243 (0.4756)	
training:	Epoch: [24][202/204]	Loss 0.4753 (0.4756)	
training:	Epoch: [24][203/204]	Loss 0.5500 (0.4759)	
training:	Epoch: [24][204/204]	Loss 0.4337 (0.4757)	
Training:	 Loss: 0.4750

Training:	 ACC: 0.7871 0.7872 0.7907 0.7835
Validation:	 ACC: 0.7447 0.7453 0.7574 0.7321
Validation:	 Best_BACC: 0.7449 0.7459 0.7656 0.7242
Validation:	 Loss: 0.5186
Pretraining:	Epoch 25/200
----------
training:	Epoch: [25][1/204]	Loss 0.3937 (0.3937)	
training:	Epoch: [25][2/204]	Loss 0.4492 (0.4215)	
training:	Epoch: [25][3/204]	Loss 0.5499 (0.4643)	
training:	Epoch: [25][4/204]	Loss 0.4349 (0.4570)	
training:	Epoch: [25][5/204]	Loss 0.5749 (0.4806)	
training:	Epoch: [25][6/204]	Loss 0.6731 (0.5126)	
training:	Epoch: [25][7/204]	Loss 0.3631 (0.4913)	
training:	Epoch: [25][8/204]	Loss 0.5119 (0.4939)	
training:	Epoch: [25][9/204]	Loss 0.2883 (0.4710)	
training:	Epoch: [25][10/204]	Loss 0.4204 (0.4660)	
training:	Epoch: [25][11/204]	Loss 0.6246 (0.4804)	
training:	Epoch: [25][12/204]	Loss 0.4411 (0.4771)	
training:	Epoch: [25][13/204]	Loss 0.3564 (0.4678)	
training:	Epoch: [25][14/204]	Loss 0.4024 (0.4631)	
training:	Epoch: [25][15/204]	Loss 0.3636 (0.4565)	
training:	Epoch: [25][16/204]	Loss 0.3140 (0.4476)	
training:	Epoch: [25][17/204]	Loss 0.5232 (0.4521)	
training:	Epoch: [25][18/204]	Loss 0.4965 (0.4545)	
training:	Epoch: [25][19/204]	Loss 0.3641 (0.4498)	
training:	Epoch: [25][20/204]	Loss 0.4145 (0.4480)	
training:	Epoch: [25][21/204]	Loss 0.4177 (0.4466)	
training:	Epoch: [25][22/204]	Loss 0.4000 (0.4444)	
training:	Epoch: [25][23/204]	Loss 0.3743 (0.4414)	
training:	Epoch: [25][24/204]	Loss 0.6006 (0.4480)	
training:	Epoch: [25][25/204]	Loss 0.4930 (0.4498)	
training:	Epoch: [25][26/204]	Loss 0.4725 (0.4507)	
training:	Epoch: [25][27/204]	Loss 0.5131 (0.4530)	
training:	Epoch: [25][28/204]	Loss 0.5474 (0.4564)	
training:	Epoch: [25][29/204]	Loss 0.5586 (0.4599)	
training:	Epoch: [25][30/204]	Loss 0.4732 (0.4603)	
training:	Epoch: [25][31/204]	Loss 0.5592 (0.4635)	
training:	Epoch: [25][32/204]	Loss 0.3845 (0.4611)	
training:	Epoch: [25][33/204]	Loss 0.3296 (0.4571)	
training:	Epoch: [25][34/204]	Loss 0.4286 (0.4562)	
training:	Epoch: [25][35/204]	Loss 0.4205 (0.4552)	
training:	Epoch: [25][36/204]	Loss 0.4381 (0.4547)	
training:	Epoch: [25][37/204]	Loss 0.5456 (0.4572)	
training:	Epoch: [25][38/204]	Loss 0.4865 (0.4580)	
training:	Epoch: [25][39/204]	Loss 0.5569 (0.4605)	
training:	Epoch: [25][40/204]	Loss 0.3116 (0.4568)	
training:	Epoch: [25][41/204]	Loss 0.3835 (0.4550)	
training:	Epoch: [25][42/204]	Loss 0.5928 (0.4583)	
training:	Epoch: [25][43/204]	Loss 0.4880 (0.4590)	
training:	Epoch: [25][44/204]	Loss 0.2694 (0.4547)	
training:	Epoch: [25][45/204]	Loss 0.4282 (0.4541)	
training:	Epoch: [25][46/204]	Loss 0.4390 (0.4537)	
training:	Epoch: [25][47/204]	Loss 0.4658 (0.4540)	
training:	Epoch: [25][48/204]	Loss 0.4286 (0.4535)	
training:	Epoch: [25][49/204]	Loss 0.4044 (0.4525)	
training:	Epoch: [25][50/204]	Loss 0.6385 (0.4562)	
training:	Epoch: [25][51/204]	Loss 0.4004 (0.4551)	
training:	Epoch: [25][52/204]	Loss 0.3965 (0.4540)	
training:	Epoch: [25][53/204]	Loss 0.5210 (0.4552)	
training:	Epoch: [25][54/204]	Loss 0.6093 (0.4581)	
training:	Epoch: [25][55/204]	Loss 0.5781 (0.4603)	
training:	Epoch: [25][56/204]	Loss 0.4588 (0.4602)	
training:	Epoch: [25][57/204]	Loss 0.4474 (0.4600)	
training:	Epoch: [25][58/204]	Loss 0.4879 (0.4605)	
training:	Epoch: [25][59/204]	Loss 0.3906 (0.4593)	
training:	Epoch: [25][60/204]	Loss 0.5398 (0.4607)	
training:	Epoch: [25][61/204]	Loss 0.3951 (0.4596)	
training:	Epoch: [25][62/204]	Loss 0.4286 (0.4591)	
training:	Epoch: [25][63/204]	Loss 0.3403 (0.4572)	
training:	Epoch: [25][64/204]	Loss 0.5190 (0.4582)	
training:	Epoch: [25][65/204]	Loss 0.4244 (0.4576)	
training:	Epoch: [25][66/204]	Loss 0.3926 (0.4567)	
training:	Epoch: [25][67/204]	Loss 0.4039 (0.4559)	
training:	Epoch: [25][68/204]	Loss 0.5125 (0.4567)	
training:	Epoch: [25][69/204]	Loss 0.6034 (0.4588)	
training:	Epoch: [25][70/204]	Loss 0.4491 (0.4587)	
training:	Epoch: [25][71/204]	Loss 0.5007 (0.4593)	
training:	Epoch: [25][72/204]	Loss 0.6240 (0.4616)	
training:	Epoch: [25][73/204]	Loss 0.4309 (0.4611)	
training:	Epoch: [25][74/204]	Loss 0.3841 (0.4601)	
training:	Epoch: [25][75/204]	Loss 0.4449 (0.4599)	
training:	Epoch: [25][76/204]	Loss 0.3932 (0.4590)	
training:	Epoch: [25][77/204]	Loss 0.4785 (0.4593)	
training:	Epoch: [25][78/204]	Loss 0.3289 (0.4576)	
training:	Epoch: [25][79/204]	Loss 0.6749 (0.4604)	
training:	Epoch: [25][80/204]	Loss 0.5771 (0.4618)	
training:	Epoch: [25][81/204]	Loss 0.5371 (0.4627)	
training:	Epoch: [25][82/204]	Loss 0.3764 (0.4617)	
training:	Epoch: [25][83/204]	Loss 0.4343 (0.4614)	
training:	Epoch: [25][84/204]	Loss 0.5255 (0.4621)	
training:	Epoch: [25][85/204]	Loss 0.4329 (0.4618)	
training:	Epoch: [25][86/204]	Loss 0.4569 (0.4617)	
training:	Epoch: [25][87/204]	Loss 0.5217 (0.4624)	
training:	Epoch: [25][88/204]	Loss 0.4847 (0.4627)	
training:	Epoch: [25][89/204]	Loss 0.2357 (0.4601)	
training:	Epoch: [25][90/204]	Loss 0.3649 (0.4591)	
training:	Epoch: [25][91/204]	Loss 0.3850 (0.4582)	
training:	Epoch: [25][92/204]	Loss 0.5255 (0.4590)	
training:	Epoch: [25][93/204]	Loss 0.3350 (0.4576)	
training:	Epoch: [25][94/204]	Loss 0.4001 (0.4570)	
training:	Epoch: [25][95/204]	Loss 0.4832 (0.4573)	
training:	Epoch: [25][96/204]	Loss 0.5454 (0.4582)	
training:	Epoch: [25][97/204]	Loss 0.4542 (0.4582)	
training:	Epoch: [25][98/204]	Loss 0.5925 (0.4596)	
training:	Epoch: [25][99/204]	Loss 0.3460 (0.4584)	
training:	Epoch: [25][100/204]	Loss 0.5494 (0.4593)	
training:	Epoch: [25][101/204]	Loss 0.4934 (0.4597)	
training:	Epoch: [25][102/204]	Loss 0.2802 (0.4579)	
training:	Epoch: [25][103/204]	Loss 0.5039 (0.4583)	
training:	Epoch: [25][104/204]	Loss 0.6183 (0.4599)	
training:	Epoch: [25][105/204]	Loss 0.4062 (0.4594)	
training:	Epoch: [25][106/204]	Loss 0.4868 (0.4596)	
training:	Epoch: [25][107/204]	Loss 0.4867 (0.4599)	
training:	Epoch: [25][108/204]	Loss 0.5014 (0.4603)	
training:	Epoch: [25][109/204]	Loss 0.5421 (0.4610)	
training:	Epoch: [25][110/204]	Loss 0.6673 (0.4629)	
training:	Epoch: [25][111/204]	Loss 0.4598 (0.4629)	
training:	Epoch: [25][112/204]	Loss 0.2882 (0.4613)	
training:	Epoch: [25][113/204]	Loss 0.4200 (0.4609)	
training:	Epoch: [25][114/204]	Loss 0.4706 (0.4610)	
training:	Epoch: [25][115/204]	Loss 0.4788 (0.4612)	
training:	Epoch: [25][116/204]	Loss 0.5663 (0.4621)	
training:	Epoch: [25][117/204]	Loss 0.4671 (0.4621)	
training:	Epoch: [25][118/204]	Loss 0.4745 (0.4622)	
training:	Epoch: [25][119/204]	Loss 0.5142 (0.4627)	
training:	Epoch: [25][120/204]	Loss 0.4464 (0.4625)	
training:	Epoch: [25][121/204]	Loss 0.3894 (0.4619)	
training:	Epoch: [25][122/204]	Loss 0.6091 (0.4631)	
training:	Epoch: [25][123/204]	Loss 0.5754 (0.4640)	
training:	Epoch: [25][124/204]	Loss 0.4949 (0.4643)	
training:	Epoch: [25][125/204]	Loss 0.4757 (0.4644)	
training:	Epoch: [25][126/204]	Loss 0.4712 (0.4644)	
training:	Epoch: [25][127/204]	Loss 0.6420 (0.4658)	
training:	Epoch: [25][128/204]	Loss 0.5786 (0.4667)	
training:	Epoch: [25][129/204]	Loss 0.4603 (0.4667)	
training:	Epoch: [25][130/204]	Loss 0.4840 (0.4668)	
training:	Epoch: [25][131/204]	Loss 0.5365 (0.4673)	
training:	Epoch: [25][132/204]	Loss 0.3870 (0.4667)	
training:	Epoch: [25][133/204]	Loss 0.3431 (0.4658)	
training:	Epoch: [25][134/204]	Loss 0.4252 (0.4655)	
training:	Epoch: [25][135/204]	Loss 0.5575 (0.4662)	
training:	Epoch: [25][136/204]	Loss 0.5771 (0.4670)	
training:	Epoch: [25][137/204]	Loss 0.5021 (0.4672)	
training:	Epoch: [25][138/204]	Loss 0.3275 (0.4662)	
training:	Epoch: [25][139/204]	Loss 0.5533 (0.4669)	
training:	Epoch: [25][140/204]	Loss 0.5457 (0.4674)	
training:	Epoch: [25][141/204]	Loss 0.3330 (0.4665)	
training:	Epoch: [25][142/204]	Loss 0.4071 (0.4661)	
training:	Epoch: [25][143/204]	Loss 0.4098 (0.4657)	
training:	Epoch: [25][144/204]	Loss 0.5594 (0.4663)	
training:	Epoch: [25][145/204]	Loss 0.4277 (0.4660)	
training:	Epoch: [25][146/204]	Loss 0.7505 (0.4680)	
training:	Epoch: [25][147/204]	Loss 0.4091 (0.4676)	
training:	Epoch: [25][148/204]	Loss 0.4999 (0.4678)	
training:	Epoch: [25][149/204]	Loss 0.4880 (0.4679)	
training:	Epoch: [25][150/204]	Loss 0.4539 (0.4679)	
training:	Epoch: [25][151/204]	Loss 0.6696 (0.4692)	
training:	Epoch: [25][152/204]	Loss 0.4045 (0.4688)	
training:	Epoch: [25][153/204]	Loss 0.4472 (0.4686)	
training:	Epoch: [25][154/204]	Loss 0.4468 (0.4685)	
training:	Epoch: [25][155/204]	Loss 0.4249 (0.4682)	
training:	Epoch: [25][156/204]	Loss 0.4376 (0.4680)	
training:	Epoch: [25][157/204]	Loss 0.4487 (0.4679)	
training:	Epoch: [25][158/204]	Loss 0.6448 (0.4690)	
training:	Epoch: [25][159/204]	Loss 0.5613 (0.4696)	
training:	Epoch: [25][160/204]	Loss 0.4180 (0.4693)	
training:	Epoch: [25][161/204]	Loss 0.4416 (0.4691)	
training:	Epoch: [25][162/204]	Loss 0.5216 (0.4694)	
training:	Epoch: [25][163/204]	Loss 0.4448 (0.4693)	
training:	Epoch: [25][164/204]	Loss 0.3600 (0.4686)	
training:	Epoch: [25][165/204]	Loss 0.5127 (0.4689)	
training:	Epoch: [25][166/204]	Loss 0.5403 (0.4693)	
training:	Epoch: [25][167/204]	Loss 0.3825 (0.4688)	
training:	Epoch: [25][168/204]	Loss 0.5139 (0.4690)	
training:	Epoch: [25][169/204]	Loss 0.4675 (0.4690)	
training:	Epoch: [25][170/204]	Loss 0.5628 (0.4696)	
training:	Epoch: [25][171/204]	Loss 0.6243 (0.4705)	
training:	Epoch: [25][172/204]	Loss 0.3385 (0.4697)	
training:	Epoch: [25][173/204]	Loss 0.4227 (0.4694)	
training:	Epoch: [25][174/204]	Loss 0.5010 (0.4696)	
training:	Epoch: [25][175/204]	Loss 0.4445 (0.4695)	
training:	Epoch: [25][176/204]	Loss 0.4441 (0.4693)	
training:	Epoch: [25][177/204]	Loss 0.3603 (0.4687)	
training:	Epoch: [25][178/204]	Loss 0.6748 (0.4699)	
training:	Epoch: [25][179/204]	Loss 0.3816 (0.4694)	
training:	Epoch: [25][180/204]	Loss 0.4830 (0.4695)	
training:	Epoch: [25][181/204]	Loss 0.5487 (0.4699)	
training:	Epoch: [25][182/204]	Loss 0.4219 (0.4696)	
training:	Epoch: [25][183/204]	Loss 0.4580 (0.4696)	
training:	Epoch: [25][184/204]	Loss 0.3707 (0.4690)	
training:	Epoch: [25][185/204]	Loss 0.4358 (0.4689)	
training:	Epoch: [25][186/204]	Loss 0.4104 (0.4685)	
training:	Epoch: [25][187/204]	Loss 0.3621 (0.4680)	
training:	Epoch: [25][188/204]	Loss 0.5060 (0.4682)	
training:	Epoch: [25][189/204]	Loss 0.5040 (0.4684)	
training:	Epoch: [25][190/204]	Loss 0.3539 (0.4678)	
training:	Epoch: [25][191/204]	Loss 0.4586 (0.4677)	
training:	Epoch: [25][192/204]	Loss 0.4144 (0.4674)	
training:	Epoch: [25][193/204]	Loss 0.4017 (0.4671)	
training:	Epoch: [25][194/204]	Loss 0.4091 (0.4668)	
training:	Epoch: [25][195/204]	Loss 0.4591 (0.4668)	
training:	Epoch: [25][196/204]	Loss 0.4236 (0.4665)	
training:	Epoch: [25][197/204]	Loss 0.4273 (0.4663)	
training:	Epoch: [25][198/204]	Loss 0.3746 (0.4659)	
training:	Epoch: [25][199/204]	Loss 0.5813 (0.4665)	
training:	Epoch: [25][200/204]	Loss 0.5106 (0.4667)	
training:	Epoch: [25][201/204]	Loss 0.4787 (0.4667)	
training:	Epoch: [25][202/204]	Loss 0.4170 (0.4665)	
training:	Epoch: [25][203/204]	Loss 0.5816 (0.4671)	
training:	Epoch: [25][204/204]	Loss 0.4462 (0.4670)	
Training:	 Loss: 0.4662

Training:	 ACC: 0.7922 0.7931 0.8122 0.7723
Validation:	 ACC: 0.7518 0.7533 0.7861 0.7175
Validation:	 Best_BACC: 0.7518 0.7533 0.7861 0.7175
Validation:	 Loss: 0.5164
Pretraining:	Epoch 26/200
----------
training:	Epoch: [26][1/204]	Loss 0.4295 (0.4295)	
training:	Epoch: [26][2/204]	Loss 0.5080 (0.4687)	
training:	Epoch: [26][3/204]	Loss 0.4166 (0.4514)	
training:	Epoch: [26][4/204]	Loss 0.4570 (0.4528)	
training:	Epoch: [26][5/204]	Loss 0.4201 (0.4462)	
training:	Epoch: [26][6/204]	Loss 0.4515 (0.4471)	
training:	Epoch: [26][7/204]	Loss 0.5328 (0.4593)	
training:	Epoch: [26][8/204]	Loss 0.6122 (0.4784)	
training:	Epoch: [26][9/204]	Loss 0.4117 (0.4710)	
training:	Epoch: [26][10/204]	Loss 0.5497 (0.4789)	
training:	Epoch: [26][11/204]	Loss 0.4084 (0.4725)	
training:	Epoch: [26][12/204]	Loss 0.3788 (0.4647)	
training:	Epoch: [26][13/204]	Loss 0.3918 (0.4591)	
training:	Epoch: [26][14/204]	Loss 0.6083 (0.4697)	
training:	Epoch: [26][15/204]	Loss 0.4848 (0.4707)	
training:	Epoch: [26][16/204]	Loss 0.6183 (0.4800)	
training:	Epoch: [26][17/204]	Loss 0.3577 (0.4728)	
training:	Epoch: [26][18/204]	Loss 0.5157 (0.4752)	
training:	Epoch: [26][19/204]	Loss 0.4131 (0.4719)	
training:	Epoch: [26][20/204]	Loss 0.5009 (0.4733)	
training:	Epoch: [26][21/204]	Loss 0.3563 (0.4678)	
training:	Epoch: [26][22/204]	Loss 0.4041 (0.4649)	
training:	Epoch: [26][23/204]	Loss 0.3946 (0.4618)	
training:	Epoch: [26][24/204]	Loss 0.5326 (0.4648)	
training:	Epoch: [26][25/204]	Loss 0.4843 (0.4655)	
training:	Epoch: [26][26/204]	Loss 0.4630 (0.4654)	
training:	Epoch: [26][27/204]	Loss 0.6219 (0.4712)	
training:	Epoch: [26][28/204]	Loss 0.3216 (0.4659)	
training:	Epoch: [26][29/204]	Loss 0.4302 (0.4647)	
training:	Epoch: [26][30/204]	Loss 0.3336 (0.4603)	
training:	Epoch: [26][31/204]	Loss 0.3880 (0.4580)	
training:	Epoch: [26][32/204]	Loss 0.5465 (0.4607)	
training:	Epoch: [26][33/204]	Loss 0.4069 (0.4591)	
training:	Epoch: [26][34/204]	Loss 0.6005 (0.4633)	
training:	Epoch: [26][35/204]	Loss 0.4676 (0.4634)	
training:	Epoch: [26][36/204]	Loss 0.5239 (0.4651)	
training:	Epoch: [26][37/204]	Loss 0.4717 (0.4652)	
training:	Epoch: [26][38/204]	Loss 0.3990 (0.4635)	
training:	Epoch: [26][39/204]	Loss 0.5476 (0.4657)	
training:	Epoch: [26][40/204]	Loss 0.3715 (0.4633)	
training:	Epoch: [26][41/204]	Loss 0.2986 (0.4593)	
training:	Epoch: [26][42/204]	Loss 0.5954 (0.4625)	
training:	Epoch: [26][43/204]	Loss 0.5146 (0.4637)	
training:	Epoch: [26][44/204]	Loss 0.5750 (0.4663)	
training:	Epoch: [26][45/204]	Loss 0.6557 (0.4705)	
training:	Epoch: [26][46/204]	Loss 0.3504 (0.4679)	
training:	Epoch: [26][47/204]	Loss 0.5305 (0.4692)	
training:	Epoch: [26][48/204]	Loss 0.4433 (0.4687)	
training:	Epoch: [26][49/204]	Loss 0.3352 (0.4659)	
training:	Epoch: [26][50/204]	Loss 0.3881 (0.4644)	
training:	Epoch: [26][51/204]	Loss 0.4617 (0.4643)	
training:	Epoch: [26][52/204]	Loss 0.5124 (0.4653)	
training:	Epoch: [26][53/204]	Loss 0.5064 (0.4660)	
training:	Epoch: [26][54/204]	Loss 0.5054 (0.4668)	
training:	Epoch: [26][55/204]	Loss 0.4524 (0.4665)	
training:	Epoch: [26][56/204]	Loss 0.4109 (0.4655)	
training:	Epoch: [26][57/204]	Loss 0.3830 (0.4641)	
training:	Epoch: [26][58/204]	Loss 0.7355 (0.4687)	
training:	Epoch: [26][59/204]	Loss 0.3428 (0.4666)	
training:	Epoch: [26][60/204]	Loss 0.4006 (0.4655)	
training:	Epoch: [26][61/204]	Loss 0.6153 (0.4680)	
training:	Epoch: [26][62/204]	Loss 0.4443 (0.4676)	
training:	Epoch: [26][63/204]	Loss 0.6073 (0.4698)	
training:	Epoch: [26][64/204]	Loss 0.3767 (0.4683)	
training:	Epoch: [26][65/204]	Loss 0.4333 (0.4678)	
training:	Epoch: [26][66/204]	Loss 0.4736 (0.4679)	
training:	Epoch: [26][67/204]	Loss 0.4438 (0.4675)	
training:	Epoch: [26][68/204]	Loss 0.3875 (0.4664)	
training:	Epoch: [26][69/204]	Loss 0.5779 (0.4680)	
training:	Epoch: [26][70/204]	Loss 0.3514 (0.4663)	
training:	Epoch: [26][71/204]	Loss 0.5061 (0.4669)	
training:	Epoch: [26][72/204]	Loss 0.5265 (0.4677)	
training:	Epoch: [26][73/204]	Loss 0.5185 (0.4684)	
training:	Epoch: [26][74/204]	Loss 0.3555 (0.4669)	
training:	Epoch: [26][75/204]	Loss 0.3758 (0.4656)	
training:	Epoch: [26][76/204]	Loss 0.4511 (0.4655)	
training:	Epoch: [26][77/204]	Loss 0.5697 (0.4668)	
training:	Epoch: [26][78/204]	Loss 0.4583 (0.4667)	
training:	Epoch: [26][79/204]	Loss 0.4283 (0.4662)	
training:	Epoch: [26][80/204]	Loss 0.5829 (0.4677)	
training:	Epoch: [26][81/204]	Loss 0.5392 (0.4686)	
training:	Epoch: [26][82/204]	Loss 0.5176 (0.4692)	
training:	Epoch: [26][83/204]	Loss 0.4970 (0.4695)	
training:	Epoch: [26][84/204]	Loss 0.5383 (0.4703)	
training:	Epoch: [26][85/204]	Loss 0.4200 (0.4697)	
training:	Epoch: [26][86/204]	Loss 0.3585 (0.4684)	
training:	Epoch: [26][87/204]	Loss 0.3518 (0.4671)	
training:	Epoch: [26][88/204]	Loss 0.4673 (0.4671)	
training:	Epoch: [26][89/204]	Loss 0.4519 (0.4669)	
training:	Epoch: [26][90/204]	Loss 0.5222 (0.4675)	
training:	Epoch: [26][91/204]	Loss 0.4095 (0.4669)	
training:	Epoch: [26][92/204]	Loss 0.3732 (0.4659)	
training:	Epoch: [26][93/204]	Loss 0.4980 (0.4662)	
training:	Epoch: [26][94/204]	Loss 0.3536 (0.4650)	
training:	Epoch: [26][95/204]	Loss 0.5050 (0.4654)	
training:	Epoch: [26][96/204]	Loss 0.5018 (0.4658)	
training:	Epoch: [26][97/204]	Loss 0.4752 (0.4659)	
training:	Epoch: [26][98/204]	Loss 0.5325 (0.4666)	
training:	Epoch: [26][99/204]	Loss 0.4772 (0.4667)	
training:	Epoch: [26][100/204]	Loss 0.3463 (0.4655)	
training:	Epoch: [26][101/204]	Loss 0.3761 (0.4646)	
training:	Epoch: [26][102/204]	Loss 0.4765 (0.4647)	
training:	Epoch: [26][103/204]	Loss 0.4531 (0.4646)	
training:	Epoch: [26][104/204]	Loss 0.3187 (0.4632)	
training:	Epoch: [26][105/204]	Loss 0.6057 (0.4646)	
training:	Epoch: [26][106/204]	Loss 0.6268 (0.4661)	
training:	Epoch: [26][107/204]	Loss 0.4218 (0.4657)	
training:	Epoch: [26][108/204]	Loss 0.4122 (0.4652)	
training:	Epoch: [26][109/204]	Loss 0.4609 (0.4652)	
training:	Epoch: [26][110/204]	Loss 0.4100 (0.4647)	
training:	Epoch: [26][111/204]	Loss 0.3555 (0.4637)	
training:	Epoch: [26][112/204]	Loss 0.5215 (0.4642)	
training:	Epoch: [26][113/204]	Loss 0.5000 (0.4645)	
training:	Epoch: [26][114/204]	Loss 0.6438 (0.4661)	
training:	Epoch: [26][115/204]	Loss 0.5860 (0.4671)	
training:	Epoch: [26][116/204]	Loss 0.4463 (0.4669)	
training:	Epoch: [26][117/204]	Loss 0.4467 (0.4668)	
training:	Epoch: [26][118/204]	Loss 0.3767 (0.4660)	
training:	Epoch: [26][119/204]	Loss 0.4328 (0.4657)	
training:	Epoch: [26][120/204]	Loss 0.5662 (0.4666)	
training:	Epoch: [26][121/204]	Loss 0.4190 (0.4662)	
training:	Epoch: [26][122/204]	Loss 0.5441 (0.4668)	
training:	Epoch: [26][123/204]	Loss 0.5205 (0.4672)	
training:	Epoch: [26][124/204]	Loss 0.5352 (0.4678)	
training:	Epoch: [26][125/204]	Loss 0.6053 (0.4689)	
training:	Epoch: [26][126/204]	Loss 0.4278 (0.4686)	
training:	Epoch: [26][127/204]	Loss 0.3965 (0.4680)	
training:	Epoch: [26][128/204]	Loss 0.4159 (0.4676)	
training:	Epoch: [26][129/204]	Loss 0.5834 (0.4685)	
training:	Epoch: [26][130/204]	Loss 0.4166 (0.4681)	
training:	Epoch: [26][131/204]	Loss 0.4656 (0.4681)	
training:	Epoch: [26][132/204]	Loss 0.5032 (0.4683)	
training:	Epoch: [26][133/204]	Loss 0.3955 (0.4678)	
training:	Epoch: [26][134/204]	Loss 0.5289 (0.4682)	
training:	Epoch: [26][135/204]	Loss 0.5134 (0.4686)	
training:	Epoch: [26][136/204]	Loss 0.4034 (0.4681)	
training:	Epoch: [26][137/204]	Loss 0.5950 (0.4690)	
training:	Epoch: [26][138/204]	Loss 0.4951 (0.4692)	
training:	Epoch: [26][139/204]	Loss 0.4469 (0.4691)	
training:	Epoch: [26][140/204]	Loss 0.4425 (0.4689)	
training:	Epoch: [26][141/204]	Loss 0.3596 (0.4681)	
training:	Epoch: [26][142/204]	Loss 0.5814 (0.4689)	
training:	Epoch: [26][143/204]	Loss 0.3938 (0.4684)	
training:	Epoch: [26][144/204]	Loss 0.4875 (0.4685)	
training:	Epoch: [26][145/204]	Loss 0.6202 (0.4695)	
training:	Epoch: [26][146/204]	Loss 0.4389 (0.4693)	
training:	Epoch: [26][147/204]	Loss 0.7216 (0.4710)	
training:	Epoch: [26][148/204]	Loss 0.3031 (0.4699)	
training:	Epoch: [26][149/204]	Loss 0.3033 (0.4688)	
training:	Epoch: [26][150/204]	Loss 0.4164 (0.4684)	
training:	Epoch: [26][151/204]	Loss 0.4790 (0.4685)	
training:	Epoch: [26][152/204]	Loss 0.4852 (0.4686)	
training:	Epoch: [26][153/204]	Loss 0.3594 (0.4679)	
training:	Epoch: [26][154/204]	Loss 0.4512 (0.4678)	
training:	Epoch: [26][155/204]	Loss 0.3486 (0.4670)	
training:	Epoch: [26][156/204]	Loss 0.4853 (0.4672)	
training:	Epoch: [26][157/204]	Loss 0.3405 (0.4663)	
training:	Epoch: [26][158/204]	Loss 0.4642 (0.4663)	
training:	Epoch: [26][159/204]	Loss 0.5481 (0.4668)	
training:	Epoch: [26][160/204]	Loss 0.5380 (0.4673)	
training:	Epoch: [26][161/204]	Loss 0.4762 (0.4673)	
training:	Epoch: [26][162/204]	Loss 0.4585 (0.4673)	
training:	Epoch: [26][163/204]	Loss 0.6618 (0.4685)	
training:	Epoch: [26][164/204]	Loss 0.4003 (0.4681)	
training:	Epoch: [26][165/204]	Loss 0.4546 (0.4680)	
training:	Epoch: [26][166/204]	Loss 0.6082 (0.4688)	
training:	Epoch: [26][167/204]	Loss 0.4479 (0.4687)	
training:	Epoch: [26][168/204]	Loss 0.6625 (0.4699)	
training:	Epoch: [26][169/204]	Loss 0.4454 (0.4697)	
training:	Epoch: [26][170/204]	Loss 0.3774 (0.4692)	
training:	Epoch: [26][171/204]	Loss 0.4194 (0.4689)	
training:	Epoch: [26][172/204]	Loss 0.4499 (0.4688)	
training:	Epoch: [26][173/204]	Loss 0.5614 (0.4693)	
training:	Epoch: [26][174/204]	Loss 0.4977 (0.4695)	
training:	Epoch: [26][175/204]	Loss 0.4517 (0.4694)	
training:	Epoch: [26][176/204]	Loss 0.4282 (0.4691)	
training:	Epoch: [26][177/204]	Loss 0.4406 (0.4690)	
training:	Epoch: [26][178/204]	Loss 0.4488 (0.4689)	
training:	Epoch: [26][179/204]	Loss 0.5293 (0.4692)	
training:	Epoch: [26][180/204]	Loss 0.4062 (0.4688)	
training:	Epoch: [26][181/204]	Loss 0.5648 (0.4694)	
training:	Epoch: [26][182/204]	Loss 0.4744 (0.4694)	
training:	Epoch: [26][183/204]	Loss 0.4635 (0.4694)	
training:	Epoch: [26][184/204]	Loss 0.4213 (0.4691)	
training:	Epoch: [26][185/204]	Loss 0.4972 (0.4693)	
training:	Epoch: [26][186/204]	Loss 0.2858 (0.4683)	
training:	Epoch: [26][187/204]	Loss 0.5188 (0.4685)	
training:	Epoch: [26][188/204]	Loss 0.3571 (0.4680)	
training:	Epoch: [26][189/204]	Loss 0.4407 (0.4678)	
training:	Epoch: [26][190/204]	Loss 0.3810 (0.4674)	
training:	Epoch: [26][191/204]	Loss 0.3742 (0.4669)	
training:	Epoch: [26][192/204]	Loss 0.5230 (0.4672)	
training:	Epoch: [26][193/204]	Loss 0.4010 (0.4668)	
training:	Epoch: [26][194/204]	Loss 0.4516 (0.4667)	
training:	Epoch: [26][195/204]	Loss 0.5155 (0.4670)	
training:	Epoch: [26][196/204]	Loss 0.3108 (0.4662)	
training:	Epoch: [26][197/204]	Loss 0.4090 (0.4659)	
training:	Epoch: [26][198/204]	Loss 0.4274 (0.4657)	
training:	Epoch: [26][199/204]	Loss 0.4431 (0.4656)	
training:	Epoch: [26][200/204]	Loss 0.4588 (0.4656)	
training:	Epoch: [26][201/204]	Loss 0.4930 (0.4657)	
training:	Epoch: [26][202/204]	Loss 0.3447 (0.4651)	
training:	Epoch: [26][203/204]	Loss 0.3523 (0.4645)	
training:	Epoch: [26][204/204]	Loss 0.3380 (0.4639)	
Training:	 Loss: 0.4632

Training:	 ACC: 0.7966 0.7969 0.8034 0.7899
Validation:	 ACC: 0.7497 0.7507 0.7718 0.7276
Validation:	 Best_BACC: 0.7518 0.7533 0.7861 0.7175
Validation:	 Loss: 0.5139
Pretraining:	Epoch 27/200
----------
training:	Epoch: [27][1/204]	Loss 0.4226 (0.4226)	
training:	Epoch: [27][2/204]	Loss 0.5053 (0.4639)	
training:	Epoch: [27][3/204]	Loss 0.4881 (0.4720)	
training:	Epoch: [27][4/204]	Loss 0.3230 (0.4347)	
training:	Epoch: [27][5/204]	Loss 0.5448 (0.4567)	
training:	Epoch: [27][6/204]	Loss 0.3955 (0.4465)	
training:	Epoch: [27][7/204]	Loss 0.5972 (0.4681)	
training:	Epoch: [27][8/204]	Loss 0.3798 (0.4570)	
training:	Epoch: [27][9/204]	Loss 0.3711 (0.4475)	
training:	Epoch: [27][10/204]	Loss 0.5085 (0.4536)	
training:	Epoch: [27][11/204]	Loss 0.4524 (0.4535)	
training:	Epoch: [27][12/204]	Loss 0.4800 (0.4557)	
training:	Epoch: [27][13/204]	Loss 0.5793 (0.4652)	
training:	Epoch: [27][14/204]	Loss 0.2795 (0.4519)	
training:	Epoch: [27][15/204]	Loss 0.5318 (0.4573)	
training:	Epoch: [27][16/204]	Loss 0.2965 (0.4472)	
training:	Epoch: [27][17/204]	Loss 0.6084 (0.4567)	
training:	Epoch: [27][18/204]	Loss 0.5024 (0.4592)	
training:	Epoch: [27][19/204]	Loss 0.4797 (0.4603)	
training:	Epoch: [27][20/204]	Loss 0.4348 (0.4590)	
training:	Epoch: [27][21/204]	Loss 0.4882 (0.4604)	
training:	Epoch: [27][22/204]	Loss 0.4643 (0.4606)	
training:	Epoch: [27][23/204]	Loss 0.3547 (0.4560)	
training:	Epoch: [27][24/204]	Loss 0.5408 (0.4595)	
training:	Epoch: [27][25/204]	Loss 0.4034 (0.4573)	
training:	Epoch: [27][26/204]	Loss 0.5719 (0.4617)	
training:	Epoch: [27][27/204]	Loss 0.5615 (0.4654)	
training:	Epoch: [27][28/204]	Loss 0.4112 (0.4635)	
training:	Epoch: [27][29/204]	Loss 0.5026 (0.4648)	
training:	Epoch: [27][30/204]	Loss 0.4577 (0.4646)	
training:	Epoch: [27][31/204]	Loss 0.4171 (0.4630)	
training:	Epoch: [27][32/204]	Loss 0.5397 (0.4654)	
training:	Epoch: [27][33/204]	Loss 0.4817 (0.4659)	
training:	Epoch: [27][34/204]	Loss 0.7002 (0.4728)	
training:	Epoch: [27][35/204]	Loss 0.4448 (0.4720)	
training:	Epoch: [27][36/204]	Loss 0.4113 (0.4703)	
training:	Epoch: [27][37/204]	Loss 0.4233 (0.4691)	
training:	Epoch: [27][38/204]	Loss 0.6048 (0.4726)	
training:	Epoch: [27][39/204]	Loss 0.5381 (0.4743)	
training:	Epoch: [27][40/204]	Loss 0.5292 (0.4757)	
training:	Epoch: [27][41/204]	Loss 0.3792 (0.4733)	
training:	Epoch: [27][42/204]	Loss 0.3330 (0.4700)	
training:	Epoch: [27][43/204]	Loss 0.5852 (0.4727)	
training:	Epoch: [27][44/204]	Loss 0.4450 (0.4720)	
training:	Epoch: [27][45/204]	Loss 0.3326 (0.4689)	
training:	Epoch: [27][46/204]	Loss 0.4084 (0.4676)	
training:	Epoch: [27][47/204]	Loss 0.4471 (0.4672)	
training:	Epoch: [27][48/204]	Loss 0.3066 (0.4638)	
training:	Epoch: [27][49/204]	Loss 0.4667 (0.4639)	
training:	Epoch: [27][50/204]	Loss 0.3908 (0.4624)	
training:	Epoch: [27][51/204]	Loss 0.4893 (0.4630)	
training:	Epoch: [27][52/204]	Loss 0.4368 (0.4625)	
training:	Epoch: [27][53/204]	Loss 0.4161 (0.4616)	
training:	Epoch: [27][54/204]	Loss 0.3572 (0.4597)	
training:	Epoch: [27][55/204]	Loss 0.3508 (0.4577)	
training:	Epoch: [27][56/204]	Loss 0.6071 (0.4603)	
training:	Epoch: [27][57/204]	Loss 0.5304 (0.4616)	
training:	Epoch: [27][58/204]	Loss 0.5047 (0.4623)	
training:	Epoch: [27][59/204]	Loss 0.4194 (0.4616)	
training:	Epoch: [27][60/204]	Loss 0.3813 (0.4603)	
training:	Epoch: [27][61/204]	Loss 0.5271 (0.4613)	
training:	Epoch: [27][62/204]	Loss 0.5425 (0.4627)	
training:	Epoch: [27][63/204]	Loss 0.4627 (0.4627)	
training:	Epoch: [27][64/204]	Loss 0.4680 (0.4627)	
training:	Epoch: [27][65/204]	Loss 0.2862 (0.4600)	
training:	Epoch: [27][66/204]	Loss 0.3069 (0.4577)	
training:	Epoch: [27][67/204]	Loss 0.5076 (0.4584)	
training:	Epoch: [27][68/204]	Loss 0.5821 (0.4603)	
training:	Epoch: [27][69/204]	Loss 0.7031 (0.4638)	
training:	Epoch: [27][70/204]	Loss 0.3874 (0.4627)	
training:	Epoch: [27][71/204]	Loss 0.5242 (0.4636)	
training:	Epoch: [27][72/204]	Loss 0.3755 (0.4623)	
training:	Epoch: [27][73/204]	Loss 0.4656 (0.4624)	
training:	Epoch: [27][74/204]	Loss 0.3618 (0.4610)	
training:	Epoch: [27][75/204]	Loss 0.4774 (0.4612)	
training:	Epoch: [27][76/204]	Loss 0.3512 (0.4598)	
training:	Epoch: [27][77/204]	Loss 0.3987 (0.4590)	
training:	Epoch: [27][78/204]	Loss 0.5016 (0.4595)	
training:	Epoch: [27][79/204]	Loss 0.6071 (0.4614)	
training:	Epoch: [27][80/204]	Loss 0.3624 (0.4602)	
training:	Epoch: [27][81/204]	Loss 0.5983 (0.4619)	
training:	Epoch: [27][82/204]	Loss 0.4768 (0.4621)	
training:	Epoch: [27][83/204]	Loss 0.4284 (0.4617)	
training:	Epoch: [27][84/204]	Loss 0.5364 (0.4626)	
training:	Epoch: [27][85/204]	Loss 0.3913 (0.4617)	
training:	Epoch: [27][86/204]	Loss 0.4933 (0.4621)	
training:	Epoch: [27][87/204]	Loss 0.5112 (0.4626)	
training:	Epoch: [27][88/204]	Loss 0.4228 (0.4622)	
training:	Epoch: [27][89/204]	Loss 0.4485 (0.4620)	
training:	Epoch: [27][90/204]	Loss 0.2963 (0.4602)	
training:	Epoch: [27][91/204]	Loss 0.6180 (0.4619)	
training:	Epoch: [27][92/204]	Loss 0.3324 (0.4605)	
training:	Epoch: [27][93/204]	Loss 0.5184 (0.4611)	
training:	Epoch: [27][94/204]	Loss 0.3484 (0.4599)	
training:	Epoch: [27][95/204]	Loss 0.4876 (0.4602)	
training:	Epoch: [27][96/204]	Loss 0.5242 (0.4609)	
training:	Epoch: [27][97/204]	Loss 0.3886 (0.4602)	
training:	Epoch: [27][98/204]	Loss 0.2717 (0.4582)	
training:	Epoch: [27][99/204]	Loss 0.2737 (0.4564)	
training:	Epoch: [27][100/204]	Loss 0.4583 (0.4564)	
training:	Epoch: [27][101/204]	Loss 0.3653 (0.4555)	
training:	Epoch: [27][102/204]	Loss 0.5291 (0.4562)	
training:	Epoch: [27][103/204]	Loss 0.5415 (0.4570)	
training:	Epoch: [27][104/204]	Loss 0.3317 (0.4558)	
training:	Epoch: [27][105/204]	Loss 0.3393 (0.4547)	
training:	Epoch: [27][106/204]	Loss 0.4530 (0.4547)	
training:	Epoch: [27][107/204]	Loss 0.4062 (0.4543)	
training:	Epoch: [27][108/204]	Loss 0.4454 (0.4542)	
training:	Epoch: [27][109/204]	Loss 0.3941 (0.4536)	
training:	Epoch: [27][110/204]	Loss 0.6038 (0.4550)	
training:	Epoch: [27][111/204]	Loss 0.5891 (0.4562)	
training:	Epoch: [27][112/204]	Loss 0.4771 (0.4564)	
training:	Epoch: [27][113/204]	Loss 0.4419 (0.4563)	
training:	Epoch: [27][114/204]	Loss 0.4722 (0.4564)	
training:	Epoch: [27][115/204]	Loss 0.3674 (0.4556)	
training:	Epoch: [27][116/204]	Loss 0.7311 (0.4580)	
training:	Epoch: [27][117/204]	Loss 0.4313 (0.4578)	
training:	Epoch: [27][118/204]	Loss 0.5318 (0.4584)	
training:	Epoch: [27][119/204]	Loss 0.4172 (0.4580)	
training:	Epoch: [27][120/204]	Loss 0.2893 (0.4566)	
training:	Epoch: [27][121/204]	Loss 0.4452 (0.4565)	
training:	Epoch: [27][122/204]	Loss 0.3569 (0.4557)	
training:	Epoch: [27][123/204]	Loss 0.7905 (0.4584)	
training:	Epoch: [27][124/204]	Loss 0.5421 (0.4591)	
training:	Epoch: [27][125/204]	Loss 0.6451 (0.4606)	
training:	Epoch: [27][126/204]	Loss 0.5650 (0.4614)	
training:	Epoch: [27][127/204]	Loss 0.4166 (0.4611)	
training:	Epoch: [27][128/204]	Loss 0.3640 (0.4603)	
training:	Epoch: [27][129/204]	Loss 0.3940 (0.4598)	
training:	Epoch: [27][130/204]	Loss 0.3213 (0.4587)	
training:	Epoch: [27][131/204]	Loss 0.3997 (0.4583)	
training:	Epoch: [27][132/204]	Loss 0.6896 (0.4601)	
training:	Epoch: [27][133/204]	Loss 0.3805 (0.4595)	
training:	Epoch: [27][134/204]	Loss 0.5061 (0.4598)	
training:	Epoch: [27][135/204]	Loss 0.3682 (0.4591)	
training:	Epoch: [27][136/204]	Loss 0.6706 (0.4607)	
training:	Epoch: [27][137/204]	Loss 0.4235 (0.4604)	
training:	Epoch: [27][138/204]	Loss 0.4919 (0.4606)	
training:	Epoch: [27][139/204]	Loss 0.4519 (0.4606)	
training:	Epoch: [27][140/204]	Loss 0.4708 (0.4606)	
training:	Epoch: [27][141/204]	Loss 0.3885 (0.4601)	
training:	Epoch: [27][142/204]	Loss 0.3281 (0.4592)	
training:	Epoch: [27][143/204]	Loss 0.4485 (0.4591)	
training:	Epoch: [27][144/204]	Loss 0.3796 (0.4586)	
training:	Epoch: [27][145/204]	Loss 0.4368 (0.4584)	
training:	Epoch: [27][146/204]	Loss 0.3478 (0.4577)	
training:	Epoch: [27][147/204]	Loss 0.4688 (0.4577)	
training:	Epoch: [27][148/204]	Loss 0.5066 (0.4581)	
training:	Epoch: [27][149/204]	Loss 0.5121 (0.4584)	
training:	Epoch: [27][150/204]	Loss 0.3708 (0.4579)	
training:	Epoch: [27][151/204]	Loss 0.4306 (0.4577)	
training:	Epoch: [27][152/204]	Loss 0.4216 (0.4574)	
training:	Epoch: [27][153/204]	Loss 0.4442 (0.4573)	
training:	Epoch: [27][154/204]	Loss 0.4187 (0.4571)	
training:	Epoch: [27][155/204]	Loss 0.4668 (0.4572)	
training:	Epoch: [27][156/204]	Loss 0.5461 (0.4577)	
training:	Epoch: [27][157/204]	Loss 0.4382 (0.4576)	
training:	Epoch: [27][158/204]	Loss 0.3934 (0.4572)	
training:	Epoch: [27][159/204]	Loss 0.3549 (0.4566)	
training:	Epoch: [27][160/204]	Loss 0.3639 (0.4560)	
training:	Epoch: [27][161/204]	Loss 0.3430 (0.4553)	
training:	Epoch: [27][162/204]	Loss 0.3400 (0.4546)	
training:	Epoch: [27][163/204]	Loss 0.4658 (0.4546)	
training:	Epoch: [27][164/204]	Loss 0.5849 (0.4554)	
training:	Epoch: [27][165/204]	Loss 0.5197 (0.4558)	
training:	Epoch: [27][166/204]	Loss 0.4927 (0.4560)	
training:	Epoch: [27][167/204]	Loss 0.5997 (0.4569)	
training:	Epoch: [27][168/204]	Loss 0.3835 (0.4565)	
training:	Epoch: [27][169/204]	Loss 0.7158 (0.4580)	
training:	Epoch: [27][170/204]	Loss 0.3642 (0.4574)	
training:	Epoch: [27][171/204]	Loss 0.4351 (0.4573)	
training:	Epoch: [27][172/204]	Loss 0.4363 (0.4572)	
training:	Epoch: [27][173/204]	Loss 0.5463 (0.4577)	
training:	Epoch: [27][174/204]	Loss 0.4503 (0.4577)	
training:	Epoch: [27][175/204]	Loss 0.3798 (0.4572)	
training:	Epoch: [27][176/204]	Loss 0.4951 (0.4574)	
training:	Epoch: [27][177/204]	Loss 0.4885 (0.4576)	
training:	Epoch: [27][178/204]	Loss 0.4161 (0.4574)	
training:	Epoch: [27][179/204]	Loss 0.4124 (0.4571)	
training:	Epoch: [27][180/204]	Loss 0.5607 (0.4577)	
training:	Epoch: [27][181/204]	Loss 0.3759 (0.4573)	
training:	Epoch: [27][182/204]	Loss 0.5012 (0.4575)	
training:	Epoch: [27][183/204]	Loss 0.2764 (0.4565)	
training:	Epoch: [27][184/204]	Loss 0.3949 (0.4562)	
training:	Epoch: [27][185/204]	Loss 0.4616 (0.4562)	
training:	Epoch: [27][186/204]	Loss 0.6527 (0.4573)	
training:	Epoch: [27][187/204]	Loss 0.3952 (0.4569)	
training:	Epoch: [27][188/204]	Loss 0.5245 (0.4573)	
training:	Epoch: [27][189/204]	Loss 0.4168 (0.4571)	
training:	Epoch: [27][190/204]	Loss 0.4316 (0.4569)	
training:	Epoch: [27][191/204]	Loss 0.3614 (0.4564)	
training:	Epoch: [27][192/204]	Loss 0.5550 (0.4569)	
training:	Epoch: [27][193/204]	Loss 0.4795 (0.4571)	
training:	Epoch: [27][194/204]	Loss 0.5623 (0.4576)	
training:	Epoch: [27][195/204]	Loss 0.3157 (0.4569)	
training:	Epoch: [27][196/204]	Loss 0.4527 (0.4569)	
training:	Epoch: [27][197/204]	Loss 0.5839 (0.4575)	
training:	Epoch: [27][198/204]	Loss 0.3446 (0.4569)	
training:	Epoch: [27][199/204]	Loss 0.4575 (0.4569)	
training:	Epoch: [27][200/204]	Loss 0.4679 (0.4570)	
training:	Epoch: [27][201/204]	Loss 0.4569 (0.4570)	
training:	Epoch: [27][202/204]	Loss 0.4773 (0.4571)	
training:	Epoch: [27][203/204]	Loss 0.6678 (0.4581)	
training:	Epoch: [27][204/204]	Loss 0.6736 (0.4592)	
Training:	 Loss: 0.4585

Training:	 ACC: 0.7968 0.7980 0.8242 0.7695
Validation:	 ACC: 0.7525 0.7544 0.7943 0.7108
Validation:	 Best_BACC: 0.7525 0.7544 0.7943 0.7108
Validation:	 Loss: 0.5129
Pretraining:	Epoch 28/200
----------
training:	Epoch: [28][1/204]	Loss 0.5488 (0.5488)	
training:	Epoch: [28][2/204]	Loss 0.4814 (0.5151)	
training:	Epoch: [28][3/204]	Loss 0.4371 (0.4891)	
training:	Epoch: [28][4/204]	Loss 0.3622 (0.4574)	
training:	Epoch: [28][5/204]	Loss 0.4277 (0.4514)	
training:	Epoch: [28][6/204]	Loss 0.3197 (0.4295)	
training:	Epoch: [28][7/204]	Loss 0.5726 (0.4499)	
training:	Epoch: [28][8/204]	Loss 0.5441 (0.4617)	
training:	Epoch: [28][9/204]	Loss 0.4124 (0.4562)	
training:	Epoch: [28][10/204]	Loss 0.5214 (0.4627)	
training:	Epoch: [28][11/204]	Loss 0.4237 (0.4592)	
training:	Epoch: [28][12/204]	Loss 0.4333 (0.4570)	
training:	Epoch: [28][13/204]	Loss 0.4916 (0.4597)	
training:	Epoch: [28][14/204]	Loss 0.2824 (0.4470)	
training:	Epoch: [28][15/204]	Loss 0.4553 (0.4476)	
training:	Epoch: [28][16/204]	Loss 0.2679 (0.4363)	
training:	Epoch: [28][17/204]	Loss 0.3258 (0.4298)	
training:	Epoch: [28][18/204]	Loss 0.3893 (0.4276)	
training:	Epoch: [28][19/204]	Loss 0.4257 (0.4275)	
training:	Epoch: [28][20/204]	Loss 0.4333 (0.4278)	
training:	Epoch: [28][21/204]	Loss 0.4976 (0.4311)	
training:	Epoch: [28][22/204]	Loss 0.4697 (0.4329)	
training:	Epoch: [28][23/204]	Loss 0.6598 (0.4427)	
training:	Epoch: [28][24/204]	Loss 0.4334 (0.4423)	
training:	Epoch: [28][25/204]	Loss 0.5088 (0.4450)	
training:	Epoch: [28][26/204]	Loss 0.5620 (0.4495)	
training:	Epoch: [28][27/204]	Loss 0.5146 (0.4519)	
training:	Epoch: [28][28/204]	Loss 0.4471 (0.4517)	
training:	Epoch: [28][29/204]	Loss 0.4523 (0.4518)	
training:	Epoch: [28][30/204]	Loss 0.3690 (0.4490)	
training:	Epoch: [28][31/204]	Loss 0.3781 (0.4467)	
training:	Epoch: [28][32/204]	Loss 0.3356 (0.4432)	
training:	Epoch: [28][33/204]	Loss 0.5351 (0.4460)	
training:	Epoch: [28][34/204]	Loss 0.5062 (0.4478)	
training:	Epoch: [28][35/204]	Loss 0.3718 (0.4456)	
training:	Epoch: [28][36/204]	Loss 0.4341 (0.4453)	
training:	Epoch: [28][37/204]	Loss 0.5022 (0.4468)	
training:	Epoch: [28][38/204]	Loss 0.4604 (0.4472)	
training:	Epoch: [28][39/204]	Loss 0.4165 (0.4464)	
training:	Epoch: [28][40/204]	Loss 0.4140 (0.4456)	
training:	Epoch: [28][41/204]	Loss 0.3706 (0.4438)	
training:	Epoch: [28][42/204]	Loss 0.2855 (0.4400)	
training:	Epoch: [28][43/204]	Loss 0.4357 (0.4399)	
training:	Epoch: [28][44/204]	Loss 0.6728 (0.4452)	
training:	Epoch: [28][45/204]	Loss 0.3320 (0.4427)	
training:	Epoch: [28][46/204]	Loss 0.3397 (0.4404)	
training:	Epoch: [28][47/204]	Loss 0.5066 (0.4419)	
training:	Epoch: [28][48/204]	Loss 0.5594 (0.4443)	
training:	Epoch: [28][49/204]	Loss 0.4262 (0.4439)	
training:	Epoch: [28][50/204]	Loss 0.3838 (0.4427)	
training:	Epoch: [28][51/204]	Loss 0.2982 (0.4399)	
training:	Epoch: [28][52/204]	Loss 0.4020 (0.4392)	
training:	Epoch: [28][53/204]	Loss 0.5295 (0.4409)	
training:	Epoch: [28][54/204]	Loss 0.3784 (0.4397)	
training:	Epoch: [28][55/204]	Loss 0.4691 (0.4402)	
training:	Epoch: [28][56/204]	Loss 0.4053 (0.4396)	
training:	Epoch: [28][57/204]	Loss 0.4559 (0.4399)	
training:	Epoch: [28][58/204]	Loss 0.4592 (0.4402)	
training:	Epoch: [28][59/204]	Loss 0.4506 (0.4404)	
training:	Epoch: [28][60/204]	Loss 0.5834 (0.4428)	
training:	Epoch: [28][61/204]	Loss 0.4403 (0.4428)	
training:	Epoch: [28][62/204]	Loss 0.5727 (0.4449)	
training:	Epoch: [28][63/204]	Loss 0.6541 (0.4482)	
training:	Epoch: [28][64/204]	Loss 0.4789 (0.4487)	
training:	Epoch: [28][65/204]	Loss 0.3811 (0.4476)	
training:	Epoch: [28][66/204]	Loss 0.4183 (0.4472)	
training:	Epoch: [28][67/204]	Loss 0.5844 (0.4492)	
training:	Epoch: [28][68/204]	Loss 0.4460 (0.4492)	
training:	Epoch: [28][69/204]	Loss 0.4905 (0.4498)	
training:	Epoch: [28][70/204]	Loss 0.4489 (0.4498)	
training:	Epoch: [28][71/204]	Loss 0.3711 (0.4487)	
training:	Epoch: [28][72/204]	Loss 0.2648 (0.4461)	
training:	Epoch: [28][73/204]	Loss 0.3370 (0.4446)	
training:	Epoch: [28][74/204]	Loss 0.3981 (0.4440)	
training:	Epoch: [28][75/204]	Loss 0.3759 (0.4431)	
training:	Epoch: [28][76/204]	Loss 0.6210 (0.4454)	
training:	Epoch: [28][77/204]	Loss 0.3335 (0.4440)	
training:	Epoch: [28][78/204]	Loss 0.6897 (0.4471)	
training:	Epoch: [28][79/204]	Loss 0.4331 (0.4469)	
training:	Epoch: [28][80/204]	Loss 0.3284 (0.4454)	
training:	Epoch: [28][81/204]	Loss 0.4951 (0.4461)	
training:	Epoch: [28][82/204]	Loss 0.3413 (0.4448)	
training:	Epoch: [28][83/204]	Loss 0.4891 (0.4453)	
training:	Epoch: [28][84/204]	Loss 0.3861 (0.4446)	
training:	Epoch: [28][85/204]	Loss 0.4878 (0.4451)	
training:	Epoch: [28][86/204]	Loss 0.3543 (0.4441)	
training:	Epoch: [28][87/204]	Loss 0.3300 (0.4428)	
training:	Epoch: [28][88/204]	Loss 0.4239 (0.4425)	
training:	Epoch: [28][89/204]	Loss 0.3765 (0.4418)	
training:	Epoch: [28][90/204]	Loss 0.3757 (0.4411)	
training:	Epoch: [28][91/204]	Loss 0.5479 (0.4422)	
training:	Epoch: [28][92/204]	Loss 0.4649 (0.4425)	
training:	Epoch: [28][93/204]	Loss 0.4242 (0.4423)	
training:	Epoch: [28][94/204]	Loss 0.4030 (0.4419)	
training:	Epoch: [28][95/204]	Loss 0.4846 (0.4423)	
training:	Epoch: [28][96/204]	Loss 0.4085 (0.4420)	
training:	Epoch: [28][97/204]	Loss 0.5593 (0.4432)	
training:	Epoch: [28][98/204]	Loss 0.3569 (0.4423)	
training:	Epoch: [28][99/204]	Loss 0.4412 (0.4423)	
training:	Epoch: [28][100/204]	Loss 0.4170 (0.4420)	
training:	Epoch: [28][101/204]	Loss 0.4407 (0.4420)	
training:	Epoch: [28][102/204]	Loss 0.4409 (0.4420)	
training:	Epoch: [28][103/204]	Loss 0.5228 (0.4428)	
training:	Epoch: [28][104/204]	Loss 0.4150 (0.4425)	
training:	Epoch: [28][105/204]	Loss 0.5199 (0.4433)	
training:	Epoch: [28][106/204]	Loss 0.3792 (0.4427)	
training:	Epoch: [28][107/204]	Loss 0.6686 (0.4448)	
training:	Epoch: [28][108/204]	Loss 0.5860 (0.4461)	
training:	Epoch: [28][109/204]	Loss 0.3790 (0.4455)	
training:	Epoch: [28][110/204]	Loss 0.5088 (0.4460)	
training:	Epoch: [28][111/204]	Loss 0.4423 (0.4460)	
training:	Epoch: [28][112/204]	Loss 0.3593 (0.4452)	
training:	Epoch: [28][113/204]	Loss 0.4653 (0.4454)	
training:	Epoch: [28][114/204]	Loss 0.5522 (0.4463)	
training:	Epoch: [28][115/204]	Loss 0.4641 (0.4465)	
training:	Epoch: [28][116/204]	Loss 0.6636 (0.4484)	
training:	Epoch: [28][117/204]	Loss 0.5154 (0.4489)	
training:	Epoch: [28][118/204]	Loss 0.5071 (0.4494)	
training:	Epoch: [28][119/204]	Loss 0.6266 (0.4509)	
training:	Epoch: [28][120/204]	Loss 0.3352 (0.4500)	
training:	Epoch: [28][121/204]	Loss 0.4542 (0.4500)	
training:	Epoch: [28][122/204]	Loss 0.3615 (0.4493)	
training:	Epoch: [28][123/204]	Loss 0.4997 (0.4497)	
training:	Epoch: [28][124/204]	Loss 0.4335 (0.4495)	
training:	Epoch: [28][125/204]	Loss 0.3715 (0.4489)	
training:	Epoch: [28][126/204]	Loss 0.5961 (0.4501)	
training:	Epoch: [28][127/204]	Loss 0.4108 (0.4498)	
training:	Epoch: [28][128/204]	Loss 0.3733 (0.4492)	
training:	Epoch: [28][129/204]	Loss 0.3026 (0.4480)	
training:	Epoch: [28][130/204]	Loss 0.5188 (0.4486)	
training:	Epoch: [28][131/204]	Loss 0.3764 (0.4480)	
training:	Epoch: [28][132/204]	Loss 0.4706 (0.4482)	
training:	Epoch: [28][133/204]	Loss 0.4588 (0.4483)	
training:	Epoch: [28][134/204]	Loss 0.4296 (0.4482)	
training:	Epoch: [28][135/204]	Loss 0.4257 (0.4480)	
training:	Epoch: [28][136/204]	Loss 0.3886 (0.4475)	
training:	Epoch: [28][137/204]	Loss 0.4502 (0.4476)	
training:	Epoch: [28][138/204]	Loss 0.3830 (0.4471)	
training:	Epoch: [28][139/204]	Loss 0.4681 (0.4473)	
training:	Epoch: [28][140/204]	Loss 0.4065 (0.4470)	
training:	Epoch: [28][141/204]	Loss 0.5475 (0.4477)	
training:	Epoch: [28][142/204]	Loss 0.5360 (0.4483)	
training:	Epoch: [28][143/204]	Loss 0.3686 (0.4477)	
training:	Epoch: [28][144/204]	Loss 0.5945 (0.4488)	
training:	Epoch: [28][145/204]	Loss 0.4408 (0.4487)	
training:	Epoch: [28][146/204]	Loss 0.5828 (0.4496)	
training:	Epoch: [28][147/204]	Loss 0.4923 (0.4499)	
training:	Epoch: [28][148/204]	Loss 0.4931 (0.4502)	
training:	Epoch: [28][149/204]	Loss 0.3236 (0.4494)	
training:	Epoch: [28][150/204]	Loss 0.2876 (0.4483)	
training:	Epoch: [28][151/204]	Loss 0.3136 (0.4474)	
training:	Epoch: [28][152/204]	Loss 0.5949 (0.4484)	
training:	Epoch: [28][153/204]	Loss 0.5269 (0.4489)	
training:	Epoch: [28][154/204]	Loss 0.5263 (0.4494)	
training:	Epoch: [28][155/204]	Loss 0.3096 (0.4485)	
training:	Epoch: [28][156/204]	Loss 0.5679 (0.4492)	
training:	Epoch: [28][157/204]	Loss 0.4349 (0.4491)	
training:	Epoch: [28][158/204]	Loss 0.4062 (0.4489)	
training:	Epoch: [28][159/204]	Loss 0.5198 (0.4493)	
training:	Epoch: [28][160/204]	Loss 0.5497 (0.4499)	
training:	Epoch: [28][161/204]	Loss 0.4765 (0.4501)	
training:	Epoch: [28][162/204]	Loss 0.6334 (0.4512)	
training:	Epoch: [28][163/204]	Loss 0.5024 (0.4516)	
training:	Epoch: [28][164/204]	Loss 0.5239 (0.4520)	
training:	Epoch: [28][165/204]	Loss 0.3971 (0.4517)	
training:	Epoch: [28][166/204]	Loss 0.3922 (0.4513)	
training:	Epoch: [28][167/204]	Loss 0.5062 (0.4516)	
training:	Epoch: [28][168/204]	Loss 0.3870 (0.4512)	
training:	Epoch: [28][169/204]	Loss 0.4146 (0.4510)	
training:	Epoch: [28][170/204]	Loss 0.2951 (0.4501)	
training:	Epoch: [28][171/204]	Loss 0.4189 (0.4499)	
training:	Epoch: [28][172/204]	Loss 0.4075 (0.4497)	
training:	Epoch: [28][173/204]	Loss 0.4728 (0.4498)	
training:	Epoch: [28][174/204]	Loss 0.4818 (0.4500)	
training:	Epoch: [28][175/204]	Loss 0.4679 (0.4501)	
training:	Epoch: [28][176/204]	Loss 0.3814 (0.4497)	
training:	Epoch: [28][177/204]	Loss 0.4696 (0.4498)	
training:	Epoch: [28][178/204]	Loss 0.4709 (0.4499)	
training:	Epoch: [28][179/204]	Loss 0.3635 (0.4495)	
training:	Epoch: [28][180/204]	Loss 0.5223 (0.4499)	
training:	Epoch: [28][181/204]	Loss 0.5506 (0.4504)	
training:	Epoch: [28][182/204]	Loss 0.4704 (0.4505)	
training:	Epoch: [28][183/204]	Loss 0.4573 (0.4506)	
training:	Epoch: [28][184/204]	Loss 0.5563 (0.4511)	
training:	Epoch: [28][185/204]	Loss 0.3764 (0.4507)	
training:	Epoch: [28][186/204]	Loss 0.3713 (0.4503)	
training:	Epoch: [28][187/204]	Loss 0.4015 (0.4501)	
training:	Epoch: [28][188/204]	Loss 0.4823 (0.4502)	
training:	Epoch: [28][189/204]	Loss 0.4511 (0.4502)	
training:	Epoch: [28][190/204]	Loss 0.3877 (0.4499)	
training:	Epoch: [28][191/204]	Loss 0.5064 (0.4502)	
training:	Epoch: [28][192/204]	Loss 0.4206 (0.4500)	
training:	Epoch: [28][193/204]	Loss 0.5504 (0.4506)	
training:	Epoch: [28][194/204]	Loss 0.4473 (0.4505)	
training:	Epoch: [28][195/204]	Loss 0.5797 (0.4512)	
training:	Epoch: [28][196/204]	Loss 0.4113 (0.4510)	
training:	Epoch: [28][197/204]	Loss 0.6296 (0.4519)	
training:	Epoch: [28][198/204]	Loss 0.3370 (0.4513)	
training:	Epoch: [28][199/204]	Loss 0.4478 (0.4513)	
training:	Epoch: [28][200/204]	Loss 0.3598 (0.4509)	
training:	Epoch: [28][201/204]	Loss 0.6460 (0.4518)	
training:	Epoch: [28][202/204]	Loss 0.4465 (0.4518)	
training:	Epoch: [28][203/204]	Loss 0.5962 (0.4525)	
training:	Epoch: [28][204/204]	Loss 0.2892 (0.4517)	
Training:	 Loss: 0.4510

Training:	 ACC: 0.8022 0.8016 0.7881 0.8163
Validation:	 ACC: 0.7579 0.7576 0.7513 0.7646
Validation:	 Best_BACC: 0.7579 0.7576 0.7513 0.7646
Validation:	 Loss: 0.5115
Pretraining:	Epoch 29/200
----------
training:	Epoch: [29][1/204]	Loss 0.3093 (0.3093)	
training:	Epoch: [29][2/204]	Loss 0.4472 (0.3782)	
training:	Epoch: [29][3/204]	Loss 0.3526 (0.3697)	
training:	Epoch: [29][4/204]	Loss 0.4840 (0.3983)	
training:	Epoch: [29][5/204]	Loss 0.3968 (0.3980)	
training:	Epoch: [29][6/204]	Loss 0.3267 (0.3861)	
training:	Epoch: [29][7/204]	Loss 0.4537 (0.3958)	
training:	Epoch: [29][8/204]	Loss 0.5804 (0.4189)	
training:	Epoch: [29][9/204]	Loss 0.3836 (0.4149)	
training:	Epoch: [29][10/204]	Loss 0.3279 (0.4062)	
training:	Epoch: [29][11/204]	Loss 0.4493 (0.4102)	
training:	Epoch: [29][12/204]	Loss 0.6287 (0.4284)	
training:	Epoch: [29][13/204]	Loss 0.3207 (0.4201)	
training:	Epoch: [29][14/204]	Loss 0.2866 (0.4105)	
training:	Epoch: [29][15/204]	Loss 0.4694 (0.4145)	
training:	Epoch: [29][16/204]	Loss 0.3513 (0.4105)	
training:	Epoch: [29][17/204]	Loss 0.2743 (0.4025)	
training:	Epoch: [29][18/204]	Loss 0.4304 (0.4041)	
training:	Epoch: [29][19/204]	Loss 0.4106 (0.4044)	
training:	Epoch: [29][20/204]	Loss 0.3604 (0.4022)	
training:	Epoch: [29][21/204]	Loss 0.3888 (0.4016)	
training:	Epoch: [29][22/204]	Loss 0.6530 (0.4130)	
training:	Epoch: [29][23/204]	Loss 0.4381 (0.4141)	
training:	Epoch: [29][24/204]	Loss 0.5168 (0.4184)	
training:	Epoch: [29][25/204]	Loss 0.4724 (0.4205)	
training:	Epoch: [29][26/204]	Loss 0.3505 (0.4178)	
training:	Epoch: [29][27/204]	Loss 0.4605 (0.4194)	
training:	Epoch: [29][28/204]	Loss 0.3365 (0.4164)	
training:	Epoch: [29][29/204]	Loss 0.3703 (0.4149)	
training:	Epoch: [29][30/204]	Loss 0.3847 (0.4139)	
training:	Epoch: [29][31/204]	Loss 0.3895 (0.4131)	
training:	Epoch: [29][32/204]	Loss 0.5200 (0.4164)	
training:	Epoch: [29][33/204]	Loss 0.4662 (0.4179)	
training:	Epoch: [29][34/204]	Loss 0.3847 (0.4169)	
training:	Epoch: [29][35/204]	Loss 0.3749 (0.4157)	
training:	Epoch: [29][36/204]	Loss 0.5058 (0.4182)	
training:	Epoch: [29][37/204]	Loss 0.3477 (0.4163)	
training:	Epoch: [29][38/204]	Loss 0.4608 (0.4175)	
training:	Epoch: [29][39/204]	Loss 0.3430 (0.4156)	
training:	Epoch: [29][40/204]	Loss 0.4235 (0.4158)	
training:	Epoch: [29][41/204]	Loss 0.5838 (0.4199)	
training:	Epoch: [29][42/204]	Loss 0.5704 (0.4235)	
training:	Epoch: [29][43/204]	Loss 0.4718 (0.4246)	
training:	Epoch: [29][44/204]	Loss 0.4485 (0.4251)	
training:	Epoch: [29][45/204]	Loss 0.3598 (0.4237)	
training:	Epoch: [29][46/204]	Loss 0.4417 (0.4241)	
training:	Epoch: [29][47/204]	Loss 0.3960 (0.4235)	
training:	Epoch: [29][48/204]	Loss 0.6271 (0.4277)	
training:	Epoch: [29][49/204]	Loss 0.3741 (0.4266)	
training:	Epoch: [29][50/204]	Loss 0.5047 (0.4282)	
training:	Epoch: [29][51/204]	Loss 0.4539 (0.4287)	
training:	Epoch: [29][52/204]	Loss 0.5265 (0.4306)	
training:	Epoch: [29][53/204]	Loss 0.4099 (0.4302)	
training:	Epoch: [29][54/204]	Loss 0.3546 (0.4288)	
training:	Epoch: [29][55/204]	Loss 0.5354 (0.4307)	
training:	Epoch: [29][56/204]	Loss 0.6185 (0.4341)	
training:	Epoch: [29][57/204]	Loss 0.5801 (0.4366)	
training:	Epoch: [29][58/204]	Loss 0.4800 (0.4374)	
training:	Epoch: [29][59/204]	Loss 0.3831 (0.4365)	
training:	Epoch: [29][60/204]	Loss 0.4889 (0.4373)	
training:	Epoch: [29][61/204]	Loss 0.4439 (0.4374)	
training:	Epoch: [29][62/204]	Loss 0.4679 (0.4379)	
training:	Epoch: [29][63/204]	Loss 0.5964 (0.4405)	
training:	Epoch: [29][64/204]	Loss 0.5058 (0.4415)	
training:	Epoch: [29][65/204]	Loss 0.3801 (0.4405)	
training:	Epoch: [29][66/204]	Loss 0.5086 (0.4416)	
training:	Epoch: [29][67/204]	Loss 0.4675 (0.4419)	
training:	Epoch: [29][68/204]	Loss 0.3221 (0.4402)	
training:	Epoch: [29][69/204]	Loss 0.4789 (0.4407)	
training:	Epoch: [29][70/204]	Loss 0.4722 (0.4412)	
training:	Epoch: [29][71/204]	Loss 0.4379 (0.4411)	
training:	Epoch: [29][72/204]	Loss 0.4268 (0.4409)	
training:	Epoch: [29][73/204]	Loss 0.4280 (0.4408)	
training:	Epoch: [29][74/204]	Loss 0.4532 (0.4409)	
training:	Epoch: [29][75/204]	Loss 0.3373 (0.4396)	
training:	Epoch: [29][76/204]	Loss 0.5254 (0.4407)	
training:	Epoch: [29][77/204]	Loss 0.4208 (0.4404)	
training:	Epoch: [29][78/204]	Loss 0.3776 (0.4396)	
training:	Epoch: [29][79/204]	Loss 0.4085 (0.4392)	
training:	Epoch: [29][80/204]	Loss 0.4287 (0.4391)	
training:	Epoch: [29][81/204]	Loss 0.5433 (0.4404)	
training:	Epoch: [29][82/204]	Loss 0.4265 (0.4402)	
training:	Epoch: [29][83/204]	Loss 0.4053 (0.4398)	
training:	Epoch: [29][84/204]	Loss 0.5322 (0.4409)	
training:	Epoch: [29][85/204]	Loss 0.3942 (0.4403)	
training:	Epoch: [29][86/204]	Loss 0.4639 (0.4406)	
training:	Epoch: [29][87/204]	Loss 0.5309 (0.4417)	
training:	Epoch: [29][88/204]	Loss 0.4161 (0.4414)	
training:	Epoch: [29][89/204]	Loss 0.4672 (0.4417)	
training:	Epoch: [29][90/204]	Loss 0.4598 (0.4419)	
training:	Epoch: [29][91/204]	Loss 0.3893 (0.4413)	
training:	Epoch: [29][92/204]	Loss 0.4448 (0.4413)	
training:	Epoch: [29][93/204]	Loss 0.3684 (0.4405)	
training:	Epoch: [29][94/204]	Loss 0.5674 (0.4419)	
training:	Epoch: [29][95/204]	Loss 0.3480 (0.4409)	
training:	Epoch: [29][96/204]	Loss 0.4820 (0.4413)	
training:	Epoch: [29][97/204]	Loss 0.5532 (0.4425)	
training:	Epoch: [29][98/204]	Loss 0.5781 (0.4439)	
training:	Epoch: [29][99/204]	Loss 0.6727 (0.4462)	
training:	Epoch: [29][100/204]	Loss 0.4731 (0.4464)	
training:	Epoch: [29][101/204]	Loss 0.3760 (0.4457)	
training:	Epoch: [29][102/204]	Loss 0.5095 (0.4464)	
training:	Epoch: [29][103/204]	Loss 0.4002 (0.4459)	
training:	Epoch: [29][104/204]	Loss 0.5075 (0.4465)	
training:	Epoch: [29][105/204]	Loss 0.3649 (0.4457)	
training:	Epoch: [29][106/204]	Loss 0.4472 (0.4458)	
training:	Epoch: [29][107/204]	Loss 0.4775 (0.4460)	
training:	Epoch: [29][108/204]	Loss 0.3862 (0.4455)	
training:	Epoch: [29][109/204]	Loss 0.4598 (0.4456)	
training:	Epoch: [29][110/204]	Loss 0.3100 (0.4444)	
training:	Epoch: [29][111/204]	Loss 0.6084 (0.4459)	
training:	Epoch: [29][112/204]	Loss 0.6790 (0.4480)	
training:	Epoch: [29][113/204]	Loss 0.3562 (0.4471)	
training:	Epoch: [29][114/204]	Loss 0.4254 (0.4469)	
training:	Epoch: [29][115/204]	Loss 0.5067 (0.4475)	
training:	Epoch: [29][116/204]	Loss 0.4944 (0.4479)	
training:	Epoch: [29][117/204]	Loss 0.5057 (0.4484)	
training:	Epoch: [29][118/204]	Loss 0.3539 (0.4476)	
training:	Epoch: [29][119/204]	Loss 0.5788 (0.4487)	
training:	Epoch: [29][120/204]	Loss 0.4514 (0.4487)	
training:	Epoch: [29][121/204]	Loss 0.6368 (0.4502)	
training:	Epoch: [29][122/204]	Loss 0.5522 (0.4511)	
training:	Epoch: [29][123/204]	Loss 0.3955 (0.4506)	
training:	Epoch: [29][124/204]	Loss 0.4072 (0.4503)	
training:	Epoch: [29][125/204]	Loss 0.2146 (0.4484)	
training:	Epoch: [29][126/204]	Loss 0.3885 (0.4479)	
training:	Epoch: [29][127/204]	Loss 0.5138 (0.4484)	
training:	Epoch: [29][128/204]	Loss 0.3913 (0.4480)	
training:	Epoch: [29][129/204]	Loss 0.3998 (0.4476)	
training:	Epoch: [29][130/204]	Loss 0.4302 (0.4475)	
training:	Epoch: [29][131/204]	Loss 0.4741 (0.4477)	
training:	Epoch: [29][132/204]	Loss 0.3636 (0.4470)	
training:	Epoch: [29][133/204]	Loss 0.3125 (0.4460)	
training:	Epoch: [29][134/204]	Loss 0.4688 (0.4462)	
training:	Epoch: [29][135/204]	Loss 0.4073 (0.4459)	
training:	Epoch: [29][136/204]	Loss 0.4855 (0.4462)	
training:	Epoch: [29][137/204]	Loss 0.3208 (0.4453)	
training:	Epoch: [29][138/204]	Loss 0.3914 (0.4449)	
training:	Epoch: [29][139/204]	Loss 0.4509 (0.4449)	
training:	Epoch: [29][140/204]	Loss 0.3444 (0.4442)	
training:	Epoch: [29][141/204]	Loss 0.4670 (0.4444)	
training:	Epoch: [29][142/204]	Loss 0.4775 (0.4446)	
training:	Epoch: [29][143/204]	Loss 0.3575 (0.4440)	
training:	Epoch: [29][144/204]	Loss 0.3123 (0.4431)	
training:	Epoch: [29][145/204]	Loss 0.5570 (0.4439)	
training:	Epoch: [29][146/204]	Loss 0.4143 (0.4437)	
training:	Epoch: [29][147/204]	Loss 0.4475 (0.4437)	
training:	Epoch: [29][148/204]	Loss 0.5404 (0.4444)	
training:	Epoch: [29][149/204]	Loss 0.3213 (0.4435)	
training:	Epoch: [29][150/204]	Loss 0.4969 (0.4439)	
training:	Epoch: [29][151/204]	Loss 0.4541 (0.4440)	
training:	Epoch: [29][152/204]	Loss 0.5582 (0.4447)	
training:	Epoch: [29][153/204]	Loss 0.2948 (0.4437)	
training:	Epoch: [29][154/204]	Loss 0.3604 (0.4432)	
training:	Epoch: [29][155/204]	Loss 0.4200 (0.4430)	
training:	Epoch: [29][156/204]	Loss 0.3192 (0.4422)	
training:	Epoch: [29][157/204]	Loss 0.3642 (0.4417)	
training:	Epoch: [29][158/204]	Loss 0.4492 (0.4418)	
training:	Epoch: [29][159/204]	Loss 0.4044 (0.4416)	
training:	Epoch: [29][160/204]	Loss 0.4736 (0.4418)	
training:	Epoch: [29][161/204]	Loss 0.3111 (0.4410)	
training:	Epoch: [29][162/204]	Loss 0.5297 (0.4415)	
training:	Epoch: [29][163/204]	Loss 0.4933 (0.4418)	
training:	Epoch: [29][164/204]	Loss 0.4838 (0.4421)	
training:	Epoch: [29][165/204]	Loss 0.4759 (0.4423)	
training:	Epoch: [29][166/204]	Loss 0.5540 (0.4430)	
training:	Epoch: [29][167/204]	Loss 0.4085 (0.4427)	
training:	Epoch: [29][168/204]	Loss 0.4223 (0.4426)	
training:	Epoch: [29][169/204]	Loss 0.4242 (0.4425)	
training:	Epoch: [29][170/204]	Loss 0.4687 (0.4427)	
training:	Epoch: [29][171/204]	Loss 0.5842 (0.4435)	
training:	Epoch: [29][172/204]	Loss 0.3643 (0.4430)	
training:	Epoch: [29][173/204]	Loss 0.4702 (0.4432)	
training:	Epoch: [29][174/204]	Loss 0.4243 (0.4431)	
training:	Epoch: [29][175/204]	Loss 0.4490 (0.4431)	
training:	Epoch: [29][176/204]	Loss 0.3556 (0.4426)	
training:	Epoch: [29][177/204]	Loss 0.5164 (0.4430)	
training:	Epoch: [29][178/204]	Loss 0.3223 (0.4424)	
training:	Epoch: [29][179/204]	Loss 0.6827 (0.4437)	
training:	Epoch: [29][180/204]	Loss 0.5349 (0.4442)	
training:	Epoch: [29][181/204]	Loss 0.4579 (0.4443)	
training:	Epoch: [29][182/204]	Loss 0.4341 (0.4442)	
training:	Epoch: [29][183/204]	Loss 0.5012 (0.4445)	
training:	Epoch: [29][184/204]	Loss 0.6478 (0.4456)	
training:	Epoch: [29][185/204]	Loss 0.5393 (0.4461)	
training:	Epoch: [29][186/204]	Loss 0.4596 (0.4462)	
training:	Epoch: [29][187/204]	Loss 0.4997 (0.4465)	
training:	Epoch: [29][188/204]	Loss 0.5523 (0.4471)	
training:	Epoch: [29][189/204]	Loss 0.5820 (0.4478)	
training:	Epoch: [29][190/204]	Loss 0.4913 (0.4480)	
training:	Epoch: [29][191/204]	Loss 0.4679 (0.4481)	
training:	Epoch: [29][192/204]	Loss 0.4674 (0.4482)	
training:	Epoch: [29][193/204]	Loss 0.3999 (0.4480)	
training:	Epoch: [29][194/204]	Loss 0.4096 (0.4478)	
training:	Epoch: [29][195/204]	Loss 0.4004 (0.4475)	
training:	Epoch: [29][196/204]	Loss 0.4485 (0.4475)	
training:	Epoch: [29][197/204]	Loss 0.3556 (0.4471)	
training:	Epoch: [29][198/204]	Loss 0.6044 (0.4479)	
training:	Epoch: [29][199/204]	Loss 0.2416 (0.4468)	
training:	Epoch: [29][200/204]	Loss 0.4388 (0.4468)	
training:	Epoch: [29][201/204]	Loss 0.5581 (0.4473)	
training:	Epoch: [29][202/204]	Loss 0.4868 (0.4475)	
training:	Epoch: [29][203/204]	Loss 0.3528 (0.4471)	
training:	Epoch: [29][204/204]	Loss 0.5582 (0.4476)	
Training:	 Loss: 0.4469

Training:	 ACC: 0.8044 0.8053 0.8263 0.7825
Validation:	 ACC: 0.7588 0.7603 0.7912 0.7265
Validation:	 Best_BACC: 0.7588 0.7603 0.7912 0.7265
Validation:	 Loss: 0.5100
Pretraining:	Epoch 30/200
----------
training:	Epoch: [30][1/204]	Loss 0.5418 (0.5418)	
training:	Epoch: [30][2/204]	Loss 0.3565 (0.4492)	
training:	Epoch: [30][3/204]	Loss 0.4094 (0.4359)	
training:	Epoch: [30][4/204]	Loss 0.4228 (0.4326)	
training:	Epoch: [30][5/204]	Loss 0.5531 (0.4567)	
training:	Epoch: [30][6/204]	Loss 0.4187 (0.4504)	
training:	Epoch: [30][7/204]	Loss 0.4932 (0.4565)	
training:	Epoch: [30][8/204]	Loss 0.2241 (0.4274)	
training:	Epoch: [30][9/204]	Loss 0.3169 (0.4152)	
training:	Epoch: [30][10/204]	Loss 0.4283 (0.4165)	
training:	Epoch: [30][11/204]	Loss 0.3978 (0.4148)	
training:	Epoch: [30][12/204]	Loss 0.5717 (0.4279)	
training:	Epoch: [30][13/204]	Loss 0.4103 (0.4265)	
training:	Epoch: [30][14/204]	Loss 0.4355 (0.4271)	
training:	Epoch: [30][15/204]	Loss 0.5020 (0.4321)	
training:	Epoch: [30][16/204]	Loss 0.4710 (0.4346)	
training:	Epoch: [30][17/204]	Loss 0.4114 (0.4332)	
training:	Epoch: [30][18/204]	Loss 0.3612 (0.4292)	
training:	Epoch: [30][19/204]	Loss 0.3853 (0.4269)	
training:	Epoch: [30][20/204]	Loss 0.5329 (0.4322)	
training:	Epoch: [30][21/204]	Loss 0.5176 (0.4363)	
training:	Epoch: [30][22/204]	Loss 0.4243 (0.4357)	
training:	Epoch: [30][23/204]	Loss 0.4263 (0.4353)	
training:	Epoch: [30][24/204]	Loss 0.4393 (0.4355)	
training:	Epoch: [30][25/204]	Loss 0.4795 (0.4372)	
training:	Epoch: [30][26/204]	Loss 0.4088 (0.4361)	
training:	Epoch: [30][27/204]	Loss 0.2919 (0.4308)	
training:	Epoch: [30][28/204]	Loss 0.4112 (0.4301)	
training:	Epoch: [30][29/204]	Loss 0.3784 (0.4283)	
training:	Epoch: [30][30/204]	Loss 0.3714 (0.4264)	
training:	Epoch: [30][31/204]	Loss 0.3112 (0.4227)	
training:	Epoch: [30][32/204]	Loss 0.5013 (0.4252)	
training:	Epoch: [30][33/204]	Loss 0.5100 (0.4277)	
training:	Epoch: [30][34/204]	Loss 0.5760 (0.4321)	
training:	Epoch: [30][35/204]	Loss 0.6126 (0.4372)	
training:	Epoch: [30][36/204]	Loss 0.3495 (0.4348)	
training:	Epoch: [30][37/204]	Loss 0.4243 (0.4345)	
training:	Epoch: [30][38/204]	Loss 0.4968 (0.4362)	
training:	Epoch: [30][39/204]	Loss 0.3790 (0.4347)	
training:	Epoch: [30][40/204]	Loss 0.3958 (0.4337)	
training:	Epoch: [30][41/204]	Loss 0.4815 (0.4349)	
training:	Epoch: [30][42/204]	Loss 0.4879 (0.4362)	
training:	Epoch: [30][43/204]	Loss 0.4596 (0.4367)	
training:	Epoch: [30][44/204]	Loss 0.5599 (0.4395)	
training:	Epoch: [30][45/204]	Loss 0.5477 (0.4419)	
training:	Epoch: [30][46/204]	Loss 0.4147 (0.4413)	
training:	Epoch: [30][47/204]	Loss 0.2828 (0.4379)	
training:	Epoch: [30][48/204]	Loss 0.4106 (0.4374)	
training:	Epoch: [30][49/204]	Loss 0.3600 (0.4358)	
training:	Epoch: [30][50/204]	Loss 0.4426 (0.4359)	
training:	Epoch: [30][51/204]	Loss 0.3956 (0.4351)	
training:	Epoch: [30][52/204]	Loss 0.5220 (0.4368)	
training:	Epoch: [30][53/204]	Loss 0.4959 (0.4379)	
training:	Epoch: [30][54/204]	Loss 0.4412 (0.4380)	
training:	Epoch: [30][55/204]	Loss 0.5703 (0.4404)	
training:	Epoch: [30][56/204]	Loss 0.4097 (0.4398)	
training:	Epoch: [30][57/204]	Loss 0.5473 (0.4417)	
training:	Epoch: [30][58/204]	Loss 0.3583 (0.4403)	
training:	Epoch: [30][59/204]	Loss 0.3419 (0.4386)	
training:	Epoch: [30][60/204]	Loss 0.6112 (0.4415)	
training:	Epoch: [30][61/204]	Loss 0.3733 (0.4404)	
training:	Epoch: [30][62/204]	Loss 0.4107 (0.4399)	
training:	Epoch: [30][63/204]	Loss 0.3858 (0.4390)	
training:	Epoch: [30][64/204]	Loss 0.4809 (0.4397)	
training:	Epoch: [30][65/204]	Loss 0.5748 (0.4418)	
training:	Epoch: [30][66/204]	Loss 0.3685 (0.4407)	
training:	Epoch: [30][67/204]	Loss 0.4568 (0.4409)	
training:	Epoch: [30][68/204]	Loss 0.4694 (0.4413)	
training:	Epoch: [30][69/204]	Loss 0.3764 (0.4404)	
training:	Epoch: [30][70/204]	Loss 0.6200 (0.4430)	
training:	Epoch: [30][71/204]	Loss 0.3135 (0.4411)	
training:	Epoch: [30][72/204]	Loss 0.3199 (0.4394)	
training:	Epoch: [30][73/204]	Loss 0.4881 (0.4401)	
training:	Epoch: [30][74/204]	Loss 0.4406 (0.4401)	
training:	Epoch: [30][75/204]	Loss 0.4587 (0.4404)	
training:	Epoch: [30][76/204]	Loss 0.3809 (0.4396)	
training:	Epoch: [30][77/204]	Loss 0.4124 (0.4392)	
training:	Epoch: [30][78/204]	Loss 0.3722 (0.4384)	
training:	Epoch: [30][79/204]	Loss 0.2327 (0.4358)	
training:	Epoch: [30][80/204]	Loss 0.3649 (0.4349)	
training:	Epoch: [30][81/204]	Loss 0.4247 (0.4348)	
training:	Epoch: [30][82/204]	Loss 0.3534 (0.4338)	
training:	Epoch: [30][83/204]	Loss 0.4917 (0.4345)	
training:	Epoch: [30][84/204]	Loss 0.5483 (0.4358)	
training:	Epoch: [30][85/204]	Loss 0.4229 (0.4357)	
training:	Epoch: [30][86/204]	Loss 0.4763 (0.4361)	
training:	Epoch: [30][87/204]	Loss 0.4398 (0.4362)	
training:	Epoch: [30][88/204]	Loss 0.4962 (0.4369)	
training:	Epoch: [30][89/204]	Loss 0.5919 (0.4386)	
training:	Epoch: [30][90/204]	Loss 0.5105 (0.4394)	
training:	Epoch: [30][91/204]	Loss 0.4573 (0.4396)	
training:	Epoch: [30][92/204]	Loss 0.4978 (0.4402)	
training:	Epoch: [30][93/204]	Loss 0.5203 (0.4411)	
training:	Epoch: [30][94/204]	Loss 0.3469 (0.4401)	
training:	Epoch: [30][95/204]	Loss 0.4636 (0.4403)	
training:	Epoch: [30][96/204]	Loss 0.4738 (0.4407)	
training:	Epoch: [30][97/204]	Loss 0.5936 (0.4423)	
training:	Epoch: [30][98/204]	Loss 0.5929 (0.4438)	
training:	Epoch: [30][99/204]	Loss 0.6881 (0.4463)	
training:	Epoch: [30][100/204]	Loss 0.3743 (0.4455)	
training:	Epoch: [30][101/204]	Loss 0.4030 (0.4451)	
training:	Epoch: [30][102/204]	Loss 0.3608 (0.4443)	
training:	Epoch: [30][103/204]	Loss 0.4133 (0.4440)	
training:	Epoch: [30][104/204]	Loss 0.4066 (0.4436)	
training:	Epoch: [30][105/204]	Loss 0.3363 (0.4426)	
training:	Epoch: [30][106/204]	Loss 0.4481 (0.4427)	
training:	Epoch: [30][107/204]	Loss 0.3246 (0.4416)	
training:	Epoch: [30][108/204]	Loss 0.3372 (0.4406)	
training:	Epoch: [30][109/204]	Loss 0.3730 (0.4400)	
training:	Epoch: [30][110/204]	Loss 0.4219 (0.4398)	
training:	Epoch: [30][111/204]	Loss 0.3970 (0.4394)	
training:	Epoch: [30][112/204]	Loss 0.3533 (0.4387)	
training:	Epoch: [30][113/204]	Loss 0.4762 (0.4390)	
training:	Epoch: [30][114/204]	Loss 0.2723 (0.4375)	
training:	Epoch: [30][115/204]	Loss 0.5192 (0.4382)	
training:	Epoch: [30][116/204]	Loss 0.4974 (0.4388)	
training:	Epoch: [30][117/204]	Loss 0.2680 (0.4373)	
training:	Epoch: [30][118/204]	Loss 0.4556 (0.4374)	
training:	Epoch: [30][119/204]	Loss 0.4988 (0.4380)	
training:	Epoch: [30][120/204]	Loss 0.4459 (0.4380)	
training:	Epoch: [30][121/204]	Loss 0.5335 (0.4388)	
training:	Epoch: [30][122/204]	Loss 0.4184 (0.4387)	
training:	Epoch: [30][123/204]	Loss 0.3381 (0.4378)	
training:	Epoch: [30][124/204]	Loss 0.6095 (0.4392)	
training:	Epoch: [30][125/204]	Loss 0.3167 (0.4382)	
training:	Epoch: [30][126/204]	Loss 0.3803 (0.4378)	
training:	Epoch: [30][127/204]	Loss 0.5123 (0.4384)	
training:	Epoch: [30][128/204]	Loss 0.5723 (0.4394)	
training:	Epoch: [30][129/204]	Loss 0.4343 (0.4394)	
training:	Epoch: [30][130/204]	Loss 0.5077 (0.4399)	
training:	Epoch: [30][131/204]	Loss 0.3245 (0.4390)	
training:	Epoch: [30][132/204]	Loss 0.4186 (0.4389)	
training:	Epoch: [30][133/204]	Loss 0.3894 (0.4385)	
training:	Epoch: [30][134/204]	Loss 0.5072 (0.4390)	
training:	Epoch: [30][135/204]	Loss 0.4047 (0.4387)	
training:	Epoch: [30][136/204]	Loss 0.4941 (0.4392)	
training:	Epoch: [30][137/204]	Loss 0.3629 (0.4386)	
training:	Epoch: [30][138/204]	Loss 0.5780 (0.4396)	
training:	Epoch: [30][139/204]	Loss 0.4193 (0.4395)	
training:	Epoch: [30][140/204]	Loss 0.5994 (0.4406)	
training:	Epoch: [30][141/204]	Loss 0.4994 (0.4410)	
training:	Epoch: [30][142/204]	Loss 0.5781 (0.4420)	
training:	Epoch: [30][143/204]	Loss 0.5754 (0.4429)	
training:	Epoch: [30][144/204]	Loss 0.4432 (0.4429)	
training:	Epoch: [30][145/204]	Loss 0.3129 (0.4420)	
training:	Epoch: [30][146/204]	Loss 0.4411 (0.4420)	
training:	Epoch: [30][147/204]	Loss 0.5039 (0.4424)	
training:	Epoch: [30][148/204]	Loss 0.5907 (0.4434)	
training:	Epoch: [30][149/204]	Loss 0.6736 (0.4450)	
training:	Epoch: [30][150/204]	Loss 0.4020 (0.4447)	
training:	Epoch: [30][151/204]	Loss 0.6181 (0.4459)	
training:	Epoch: [30][152/204]	Loss 0.3486 (0.4452)	
training:	Epoch: [30][153/204]	Loss 0.3652 (0.4447)	
training:	Epoch: [30][154/204]	Loss 0.4369 (0.4446)	
training:	Epoch: [30][155/204]	Loss 0.4095 (0.4444)	
training:	Epoch: [30][156/204]	Loss 0.4594 (0.4445)	
training:	Epoch: [30][157/204]	Loss 0.3465 (0.4439)	
training:	Epoch: [30][158/204]	Loss 0.4834 (0.4441)	
training:	Epoch: [30][159/204]	Loss 0.4004 (0.4439)	
training:	Epoch: [30][160/204]	Loss 0.3644 (0.4434)	
