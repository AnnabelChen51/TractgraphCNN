Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['all'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, epochs=400, tensorboard=True, net_architecture='DGCNN', batch_size=16, rate=0.0001, weight=0.0, sched_step=200, sched_gamma=0.1, printing_frequency=1, seed=0, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=22, fl=64, nh=1)
Training the 'DGCNN' architecture
Batch size:	16
Number of workers:	0
Learning rate:	0.0001
Weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Number of epochs of training:	400
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0


Total number of parameters:7824898  Trainable:7824898
Pretraining:	Epoch 1/400
----------
training:	Epoch: [1][1/408]	Loss 0.6396 (0.6396)	
training:	Epoch: [1][2/408]	Loss 0.7078 (0.6737)	
training:	Epoch: [1][3/408]	Loss 0.7778 (0.7084)	
training:	Epoch: [1][4/408]	Loss 0.5616 (0.6717)	
training:	Epoch: [1][5/408]	Loss 0.8196 (0.7013)	
training:	Epoch: [1][6/408]	Loss 0.6623 (0.6948)	
training:	Epoch: [1][7/408]	Loss 0.7822 (0.7073)	
training:	Epoch: [1][8/408]	Loss 1.0655 (0.7520)	
training:	Epoch: [1][9/408]	Loss 0.9383 (0.7727)	
training:	Epoch: [1][10/408]	Loss 0.7753 (0.7730)	
training:	Epoch: [1][11/408]	Loss 0.6909 (0.7655)	
training:	Epoch: [1][12/408]	Loss 0.7559 (0.7647)	
training:	Epoch: [1][13/408]	Loss 0.5737 (0.7500)	
training:	Epoch: [1][14/408]	Loss 0.5267 (0.7341)	
training:	Epoch: [1][15/408]	Loss 0.6915 (0.7312)	
training:	Epoch: [1][16/408]	Loss 0.5974 (0.7229)	
training:	Epoch: [1][17/408]	Loss 0.5052 (0.7101)	
training:	Epoch: [1][18/408]	Loss 0.5541 (0.7014)	
training:	Epoch: [1][19/408]	Loss 0.9159 (0.7127)	
training:	Epoch: [1][20/408]	Loss 0.6357 (0.7088)	
training:	Epoch: [1][21/408]	Loss 0.6665 (0.7068)	
training:	Epoch: [1][22/408]	Loss 0.5814 (0.7011)	
training:	Epoch: [1][23/408]	Loss 0.5413 (0.6942)	
training:	Epoch: [1][24/408]	Loss 0.6027 (0.6904)	
training:	Epoch: [1][25/408]	Loss 0.6033 (0.6869)	
training:	Epoch: [1][26/408]	Loss 0.4211 (0.6767)	
training:	Epoch: [1][27/408]	Loss 0.6824 (0.6769)	
training:	Epoch: [1][28/408]	Loss 0.4790 (0.6698)	
training:	Epoch: [1][29/408]	Loss 0.7087 (0.6712)	
training:	Epoch: [1][30/408]	Loss 0.5469 (0.6670)	
training:	Epoch: [1][31/408]	Loss 0.5776 (0.6641)	
training:	Epoch: [1][32/408]	Loss 0.6436 (0.6635)	
training:	Epoch: [1][33/408]	Loss 0.6794 (0.6640)	
training:	Epoch: [1][34/408]	Loss 0.4895 (0.6588)	
training:	Epoch: [1][35/408]	Loss 0.5443 (0.6556)	
training:	Epoch: [1][36/408]	Loss 0.5156 (0.6517)	
training:	Epoch: [1][37/408]	Loss 0.5863 (0.6499)	
training:	Epoch: [1][38/408]	Loss 0.4954 (0.6458)	
training:	Epoch: [1][39/408]	Loss 0.9182 (0.6528)	
training:	Epoch: [1][40/408]	Loss 0.8551 (0.6579)	
training:	Epoch: [1][41/408]	Loss 0.5668 (0.6557)	
training:	Epoch: [1][42/408]	Loss 0.7319 (0.6575)	
training:	Epoch: [1][43/408]	Loss 0.6687 (0.6577)	
training:	Epoch: [1][44/408]	Loss 0.4060 (0.6520)	
training:	Epoch: [1][45/408]	Loss 0.5443 (0.6496)	
training:	Epoch: [1][46/408]	Loss 0.3960 (0.6441)	
training:	Epoch: [1][47/408]	Loss 0.3849 (0.6386)	
training:	Epoch: [1][48/408]	Loss 0.6307 (0.6384)	
training:	Epoch: [1][49/408]	Loss 0.4578 (0.6347)	
training:	Epoch: [1][50/408]	Loss 0.4721 (0.6315)	
training:	Epoch: [1][51/408]	Loss 0.6577 (0.6320)	
training:	Epoch: [1][52/408]	Loss 0.7806 (0.6349)	
training:	Epoch: [1][53/408]	Loss 0.3562 (0.6296)	
training:	Epoch: [1][54/408]	Loss 0.3626 (0.6247)	
training:	Epoch: [1][55/408]	Loss 0.4685 (0.6218)	
training:	Epoch: [1][56/408]	Loss 0.4623 (0.6190)	
training:	Epoch: [1][57/408]	Loss 0.6570 (0.6196)	
training:	Epoch: [1][58/408]	Loss 0.5277 (0.6181)	
training:	Epoch: [1][59/408]	Loss 0.4035 (0.6144)	
training:	Epoch: [1][60/408]	Loss 0.4622 (0.6119)	
training:	Epoch: [1][61/408]	Loss 0.5122 (0.6103)	
training:	Epoch: [1][62/408]	Loss 0.5267 (0.6089)	
training:	Epoch: [1][63/408]	Loss 0.6636 (0.6098)	
training:	Epoch: [1][64/408]	Loss 0.7207 (0.6115)	
training:	Epoch: [1][65/408]	Loss 0.5414 (0.6104)	
training:	Epoch: [1][66/408]	Loss 0.2824 (0.6055)	
training:	Epoch: [1][67/408]	Loss 0.3749 (0.6020)	
training:	Epoch: [1][68/408]	Loss 0.6181 (0.6023)	
training:	Epoch: [1][69/408]	Loss 0.5513 (0.6015)	
training:	Epoch: [1][70/408]	Loss 0.5486 (0.6008)	
training:	Epoch: [1][71/408]	Loss 0.4137 (0.5981)	
training:	Epoch: [1][72/408]	Loss 0.5439 (0.5974)	
training:	Epoch: [1][73/408]	Loss 0.5710 (0.5970)	
training:	Epoch: [1][74/408]	Loss 0.7746 (0.5994)	
training:	Epoch: [1][75/408]	Loss 0.5014 (0.5981)	
training:	Epoch: [1][76/408]	Loss 0.6277 (0.5985)	
training:	Epoch: [1][77/408]	Loss 0.4833 (0.5970)	
training:	Epoch: [1][78/408]	Loss 0.4860 (0.5956)	
training:	Epoch: [1][79/408]	Loss 0.3002 (0.5918)	
training:	Epoch: [1][80/408]	Loss 0.3321 (0.5886)	
training:	Epoch: [1][81/408]	Loss 0.7582 (0.5907)	
training:	Epoch: [1][82/408]	Loss 0.6475 (0.5914)	
training:	Epoch: [1][83/408]	Loss 0.4923 (0.5902)	
training:	Epoch: [1][84/408]	Loss 0.6661 (0.5911)	
training:	Epoch: [1][85/408]	Loss 0.4367 (0.5893)	
training:	Epoch: [1][86/408]	Loss 0.6587 (0.5901)	
training:	Epoch: [1][87/408]	Loss 0.3350 (0.5871)	
training:	Epoch: [1][88/408]	Loss 0.3403 (0.5843)	
training:	Epoch: [1][89/408]	Loss 0.4640 (0.5830)	
training:	Epoch: [1][90/408]	Loss 0.4674 (0.5817)	
training:	Epoch: [1][91/408]	Loss 0.6713 (0.5827)	
training:	Epoch: [1][92/408]	Loss 0.5631 (0.5825)	
training:	Epoch: [1][93/408]	Loss 0.6997 (0.5837)	
training:	Epoch: [1][94/408]	Loss 0.3278 (0.5810)	
training:	Epoch: [1][95/408]	Loss 0.5279 (0.5805)	
training:	Epoch: [1][96/408]	Loss 0.4645 (0.5792)	
training:	Epoch: [1][97/408]	Loss 0.4374 (0.5778)	
training:	Epoch: [1][98/408]	Loss 0.6214 (0.5782)	
training:	Epoch: [1][99/408]	Loss 0.4548 (0.5770)	
training:	Epoch: [1][100/408]	Loss 0.2869 (0.5741)	
training:	Epoch: [1][101/408]	Loss 0.4674 (0.5730)	
training:	Epoch: [1][102/408]	Loss 0.3333 (0.5707)	
training:	Epoch: [1][103/408]	Loss 0.3955 (0.5690)	
training:	Epoch: [1][104/408]	Loss 0.3041 (0.5664)	
training:	Epoch: [1][105/408]	Loss 0.4229 (0.5651)	
training:	Epoch: [1][106/408]	Loss 0.3045 (0.5626)	
training:	Epoch: [1][107/408]	Loss 0.5267 (0.5623)	
training:	Epoch: [1][108/408]	Loss 0.3821 (0.5606)	
training:	Epoch: [1][109/408]	Loss 0.5217 (0.5602)	
training:	Epoch: [1][110/408]	Loss 0.3848 (0.5586)	
training:	Epoch: [1][111/408]	Loss 0.4659 (0.5578)	
training:	Epoch: [1][112/408]	Loss 0.5940 (0.5581)	
training:	Epoch: [1][113/408]	Loss 0.3886 (0.5566)	
training:	Epoch: [1][114/408]	Loss 0.7650 (0.5585)	
training:	Epoch: [1][115/408]	Loss 0.3983 (0.5571)	
training:	Epoch: [1][116/408]	Loss 0.3982 (0.5557)	
training:	Epoch: [1][117/408]	Loss 0.4724 (0.5550)	
training:	Epoch: [1][118/408]	Loss 0.6684 (0.5559)	
training:	Epoch: [1][119/408]	Loss 0.4640 (0.5552)	
training:	Epoch: [1][120/408]	Loss 0.3611 (0.5536)	
training:	Epoch: [1][121/408]	Loss 0.4782 (0.5529)	
training:	Epoch: [1][122/408]	Loss 0.3744 (0.5515)	
training:	Epoch: [1][123/408]	Loss 0.3405 (0.5498)	
training:	Epoch: [1][124/408]	Loss 0.5548 (0.5498)	
training:	Epoch: [1][125/408]	Loss 0.3340 (0.5481)	
training:	Epoch: [1][126/408]	Loss 0.4718 (0.5475)	
training:	Epoch: [1][127/408]	Loss 0.4575 (0.5468)	
training:	Epoch: [1][128/408]	Loss 0.3343 (0.5451)	
training:	Epoch: [1][129/408]	Loss 0.4552 (0.5444)	
training:	Epoch: [1][130/408]	Loss 0.3552 (0.5429)	
training:	Epoch: [1][131/408]	Loss 0.6137 (0.5435)	
training:	Epoch: [1][132/408]	Loss 0.6180 (0.5440)	
training:	Epoch: [1][133/408]	Loss 0.5667 (0.5442)	
training:	Epoch: [1][134/408]	Loss 0.4499 (0.5435)	
training:	Epoch: [1][135/408]	Loss 0.4272 (0.5427)	
training:	Epoch: [1][136/408]	Loss 0.4086 (0.5417)	
training:	Epoch: [1][137/408]	Loss 0.3083 (0.5400)	
training:	Epoch: [1][138/408]	Loss 0.4633 (0.5394)	
training:	Epoch: [1][139/408]	Loss 0.6723 (0.5404)	
training:	Epoch: [1][140/408]	Loss 0.3970 (0.5393)	
training:	Epoch: [1][141/408]	Loss 0.4618 (0.5388)	
training:	Epoch: [1][142/408]	Loss 0.5155 (0.5386)	
training:	Epoch: [1][143/408]	Loss 0.6487 (0.5394)	
training:	Epoch: [1][144/408]	Loss 0.6505 (0.5402)	
training:	Epoch: [1][145/408]	Loss 0.5238 (0.5401)	
training:	Epoch: [1][146/408]	Loss 0.3150 (0.5385)	
training:	Epoch: [1][147/408]	Loss 0.4081 (0.5376)	
training:	Epoch: [1][148/408]	Loss 0.5107 (0.5374)	
training:	Epoch: [1][149/408]	Loss 0.4241 (0.5367)	
training:	Epoch: [1][150/408]	Loss 0.2783 (0.5350)	
training:	Epoch: [1][151/408]	Loss 0.6156 (0.5355)	
training:	Epoch: [1][152/408]	Loss 0.5224 (0.5354)	
training:	Epoch: [1][153/408]	Loss 0.3198 (0.5340)	
training:	Epoch: [1][154/408]	Loss 0.3651 (0.5329)	
training:	Epoch: [1][155/408]	Loss 0.4790 (0.5326)	
training:	Epoch: [1][156/408]	Loss 0.5061 (0.5324)	
training:	Epoch: [1][157/408]	Loss 0.5009 (0.5322)	
training:	Epoch: [1][158/408]	Loss 0.4785 (0.5318)	
training:	Epoch: [1][159/408]	Loss 0.4813 (0.5315)	
training:	Epoch: [1][160/408]	Loss 0.4399 (0.5310)	
training:	Epoch: [1][161/408]	Loss 0.3938 (0.5301)	
training:	Epoch: [1][162/408]	Loss 0.5584 (0.5303)	
training:	Epoch: [1][163/408]	Loss 0.3439 (0.5291)	
training:	Epoch: [1][164/408]	Loss 0.2388 (0.5274)	
training:	Epoch: [1][165/408]	Loss 0.4714 (0.5270)	
training:	Epoch: [1][166/408]	Loss 0.7192 (0.5282)	
training:	Epoch: [1][167/408]	Loss 0.3332 (0.5270)	
training:	Epoch: [1][168/408]	Loss 0.8120 (0.5287)	
training:	Epoch: [1][169/408]	Loss 0.6138 (0.5292)	
training:	Epoch: [1][170/408]	Loss 0.5427 (0.5293)	
training:	Epoch: [1][171/408]	Loss 0.4628 (0.5289)	
training:	Epoch: [1][172/408]	Loss 0.3697 (0.5280)	
training:	Epoch: [1][173/408]	Loss 0.3938 (0.5272)	
training:	Epoch: [1][174/408]	Loss 0.7319 (0.5284)	
training:	Epoch: [1][175/408]	Loss 0.4341 (0.5278)	
training:	Epoch: [1][176/408]	Loss 0.2786 (0.5264)	
training:	Epoch: [1][177/408]	Loss 0.6913 (0.5274)	
training:	Epoch: [1][178/408]	Loss 0.3609 (0.5264)	
training:	Epoch: [1][179/408]	Loss 0.5251 (0.5264)	
training:	Epoch: [1][180/408]	Loss 0.4147 (0.5258)	
training:	Epoch: [1][181/408]	Loss 0.4223 (0.5252)	
training:	Epoch: [1][182/408]	Loss 0.4077 (0.5246)	
training:	Epoch: [1][183/408]	Loss 0.3215 (0.5235)	
training:	Epoch: [1][184/408]	Loss 0.3921 (0.5228)	
training:	Epoch: [1][185/408]	Loss 0.3878 (0.5220)	
training:	Epoch: [1][186/408]	Loss 0.5337 (0.5221)	
training:	Epoch: [1][187/408]	Loss 0.4478 (0.5217)	
training:	Epoch: [1][188/408]	Loss 0.3353 (0.5207)	
training:	Epoch: [1][189/408]	Loss 0.3562 (0.5198)	
training:	Epoch: [1][190/408]	Loss 0.4588 (0.5195)	
training:	Epoch: [1][191/408]	Loss 0.3514 (0.5186)	
training:	Epoch: [1][192/408]	Loss 0.4108 (0.5181)	
training:	Epoch: [1][193/408]	Loss 0.3356 (0.5171)	
training:	Epoch: [1][194/408]	Loss 0.3701 (0.5164)	
training:	Epoch: [1][195/408]	Loss 0.3976 (0.5158)	
training:	Epoch: [1][196/408]	Loss 0.2816 (0.5146)	
training:	Epoch: [1][197/408]	Loss 0.7214 (0.5156)	
training:	Epoch: [1][198/408]	Loss 0.3564 (0.5148)	
training:	Epoch: [1][199/408]	Loss 0.6700 (0.5156)	
training:	Epoch: [1][200/408]	Loss 0.3187 (0.5146)	
training:	Epoch: [1][201/408]	Loss 0.4089 (0.5141)	
training:	Epoch: [1][202/408]	Loss 0.4994 (0.5140)	
training:	Epoch: [1][203/408]	Loss 0.5101 (0.5140)	
training:	Epoch: [1][204/408]	Loss 0.3439 (0.5131)	
training:	Epoch: [1][205/408]	Loss 0.4237 (0.5127)	
training:	Epoch: [1][206/408]	Loss 0.4370 (0.5123)	
training:	Epoch: [1][207/408]	Loss 0.3612 (0.5116)	
training:	Epoch: [1][208/408]	Loss 0.7614 (0.5128)	
training:	Epoch: [1][209/408]	Loss 0.2587 (0.5116)	
training:	Epoch: [1][210/408]	Loss 0.6044 (0.5120)	
training:	Epoch: [1][211/408]	Loss 0.4218 (0.5116)	
training:	Epoch: [1][212/408]	Loss 0.3943 (0.5111)	
training:	Epoch: [1][213/408]	Loss 0.7281 (0.5121)	
training:	Epoch: [1][214/408]	Loss 0.3820 (0.5115)	
training:	Epoch: [1][215/408]	Loss 0.6796 (0.5123)	
training:	Epoch: [1][216/408]	Loss 0.4232 (0.5118)	
training:	Epoch: [1][217/408]	Loss 0.3873 (0.5113)	
training:	Epoch: [1][218/408]	Loss 0.6839 (0.5121)	
training:	Epoch: [1][219/408]	Loss 0.3300 (0.5112)	
training:	Epoch: [1][220/408]	Loss 0.5643 (0.5115)	
training:	Epoch: [1][221/408]	Loss 0.3649 (0.5108)	
training:	Epoch: [1][222/408]	Loss 0.4008 (0.5103)	
training:	Epoch: [1][223/408]	Loss 0.6633 (0.5110)	
training:	Epoch: [1][224/408]	Loss 0.3665 (0.5104)	
training:	Epoch: [1][225/408]	Loss 0.3820 (0.5098)	
training:	Epoch: [1][226/408]	Loss 0.5062 (0.5098)	
training:	Epoch: [1][227/408]	Loss 0.4717 (0.5096)	
training:	Epoch: [1][228/408]	Loss 0.3802 (0.5090)	
training:	Epoch: [1][229/408]	Loss 0.4989 (0.5090)	
training:	Epoch: [1][230/408]	Loss 0.4374 (0.5087)	
training:	Epoch: [1][231/408]	Loss 0.5684 (0.5089)	
training:	Epoch: [1][232/408]	Loss 0.3684 (0.5083)	
training:	Epoch: [1][233/408]	Loss 0.5250 (0.5084)	
training:	Epoch: [1][234/408]	Loss 0.4842 (0.5083)	
training:	Epoch: [1][235/408]	Loss 0.4338 (0.5080)	
training:	Epoch: [1][236/408]	Loss 0.3096 (0.5071)	
training:	Epoch: [1][237/408]	Loss 0.6471 (0.5077)	
training:	Epoch: [1][238/408]	Loss 0.5440 (0.5079)	
training:	Epoch: [1][239/408]	Loss 0.4320 (0.5076)	
training:	Epoch: [1][240/408]	Loss 0.4285 (0.5072)	
training:	Epoch: [1][241/408]	Loss 0.5179 (0.5073)	
training:	Epoch: [1][242/408]	Loss 0.5358 (0.5074)	
training:	Epoch: [1][243/408]	Loss 0.4995 (0.5074)	
training:	Epoch: [1][244/408]	Loss 0.4773 (0.5072)	
training:	Epoch: [1][245/408]	Loss 0.4852 (0.5072)	
training:	Epoch: [1][246/408]	Loss 0.5032 (0.5071)	
training:	Epoch: [1][247/408]	Loss 0.2527 (0.5061)	
training:	Epoch: [1][248/408]	Loss 0.4977 (0.5061)	
training:	Epoch: [1][249/408]	Loss 0.4528 (0.5059)	
training:	Epoch: [1][250/408]	Loss 0.4212 (0.5055)	
training:	Epoch: [1][251/408]	Loss 0.3294 (0.5048)	
training:	Epoch: [1][252/408]	Loss 0.3151 (0.5041)	
training:	Epoch: [1][253/408]	Loss 0.3659 (0.5035)	
training:	Epoch: [1][254/408]	Loss 0.3814 (0.5030)	
training:	Epoch: [1][255/408]	Loss 0.3976 (0.5026)	
training:	Epoch: [1][256/408]	Loss 0.4176 (0.5023)	
training:	Epoch: [1][257/408]	Loss 0.5519 (0.5025)	
training:	Epoch: [1][258/408]	Loss 0.3410 (0.5019)	
training:	Epoch: [1][259/408]	Loss 0.5540 (0.5021)	
training:	Epoch: [1][260/408]	Loss 0.6150 (0.5025)	
training:	Epoch: [1][261/408]	Loss 0.4136 (0.5022)	
training:	Epoch: [1][262/408]	Loss 0.4477 (0.5019)	
training:	Epoch: [1][263/408]	Loss 0.2657 (0.5010)	
training:	Epoch: [1][264/408]	Loss 0.6776 (0.5017)	
training:	Epoch: [1][265/408]	Loss 0.3407 (0.5011)	
training:	Epoch: [1][266/408]	Loss 0.2539 (0.5002)	
training:	Epoch: [1][267/408]	Loss 0.6727 (0.5008)	
training:	Epoch: [1][268/408]	Loss 0.3859 (0.5004)	
training:	Epoch: [1][269/408]	Loss 0.2331 (0.4994)	
training:	Epoch: [1][270/408]	Loss 0.4739 (0.4993)	
training:	Epoch: [1][271/408]	Loss 0.3502 (0.4988)	
training:	Epoch: [1][272/408]	Loss 0.3584 (0.4982)	
training:	Epoch: [1][273/408]	Loss 0.4513 (0.4981)	
training:	Epoch: [1][274/408]	Loss 0.3191 (0.4974)	
training:	Epoch: [1][275/408]	Loss 0.3274 (0.4968)	
training:	Epoch: [1][276/408]	Loss 0.7993 (0.4979)	
training:	Epoch: [1][277/408]	Loss 0.5406 (0.4980)	
training:	Epoch: [1][278/408]	Loss 0.5924 (0.4984)	
training:	Epoch: [1][279/408]	Loss 0.3772 (0.4980)	
training:	Epoch: [1][280/408]	Loss 0.3351 (0.4974)	
training:	Epoch: [1][281/408]	Loss 0.2981 (0.4967)	
training:	Epoch: [1][282/408]	Loss 0.5302 (0.4968)	
training:	Epoch: [1][283/408]	Loss 0.5278 (0.4969)	
training:	Epoch: [1][284/408]	Loss 0.4031 (0.4966)	
training:	Epoch: [1][285/408]	Loss 0.3425 (0.4960)	
training:	Epoch: [1][286/408]	Loss 0.3223 (0.4954)	
training:	Epoch: [1][287/408]	Loss 0.6148 (0.4958)	
training:	Epoch: [1][288/408]	Loss 0.5371 (0.4960)	
training:	Epoch: [1][289/408]	Loss 0.4513 (0.4958)	
training:	Epoch: [1][290/408]	Loss 0.4109 (0.4955)	
training:	Epoch: [1][291/408]	Loss 0.4376 (0.4953)	
training:	Epoch: [1][292/408]	Loss 0.8199 (0.4964)	
training:	Epoch: [1][293/408]	Loss 0.3405 (0.4959)	
training:	Epoch: [1][294/408]	Loss 0.4478 (0.4957)	
training:	Epoch: [1][295/408]	Loss 0.5062 (0.4958)	
training:	Epoch: [1][296/408]	Loss 0.5248 (0.4959)	
training:	Epoch: [1][297/408]	Loss 0.4500 (0.4957)	
training:	Epoch: [1][298/408]	Loss 0.3357 (0.4952)	
training:	Epoch: [1][299/408]	Loss 0.2454 (0.4943)	
training:	Epoch: [1][300/408]	Loss 0.5871 (0.4947)	
training:	Epoch: [1][301/408]	Loss 0.4582 (0.4945)	
training:	Epoch: [1][302/408]	Loss 0.4220 (0.4943)	
training:	Epoch: [1][303/408]	Loss 0.2976 (0.4936)	
training:	Epoch: [1][304/408]	Loss 0.2929 (0.4930)	
training:	Epoch: [1][305/408]	Loss 0.3641 (0.4926)	
training:	Epoch: [1][306/408]	Loss 0.5041 (0.4926)	
training:	Epoch: [1][307/408]	Loss 0.5569 (0.4928)	
training:	Epoch: [1][308/408]	Loss 0.3745 (0.4924)	
training:	Epoch: [1][309/408]	Loss 0.3913 (0.4921)	
training:	Epoch: [1][310/408]	Loss 0.3480 (0.4916)	
training:	Epoch: [1][311/408]	Loss 0.3666 (0.4912)	
training:	Epoch: [1][312/408]	Loss 0.5526 (0.4914)	
training:	Epoch: [1][313/408]	Loss 0.2299 (0.4906)	
training:	Epoch: [1][314/408]	Loss 0.3697 (0.4902)	
training:	Epoch: [1][315/408]	Loss 0.2868 (0.4896)	
training:	Epoch: [1][316/408]	Loss 0.2337 (0.4888)	
training:	Epoch: [1][317/408]	Loss 0.2845 (0.4881)	
training:	Epoch: [1][318/408]	Loss 0.2638 (0.4874)	
training:	Epoch: [1][319/408]	Loss 0.4027 (0.4871)	
training:	Epoch: [1][320/408]	Loss 0.5104 (0.4872)	
training:	Epoch: [1][321/408]	Loss 0.6283 (0.4877)	
training:	Epoch: [1][322/408]	Loss 0.6735 (0.4882)	
training:	Epoch: [1][323/408]	Loss 0.7033 (0.4889)	
training:	Epoch: [1][324/408]	Loss 0.3270 (0.4884)	
training:	Epoch: [1][325/408]	Loss 0.4223 (0.4882)	
training:	Epoch: [1][326/408]	Loss 0.4016 (0.4879)	
training:	Epoch: [1][327/408]	Loss 0.3671 (0.4876)	
training:	Epoch: [1][328/408]	Loss 0.3330 (0.4871)	
training:	Epoch: [1][329/408]	Loss 0.4109 (0.4869)	
training:	Epoch: [1][330/408]	Loss 0.5822 (0.4871)	
training:	Epoch: [1][331/408]	Loss 0.4190 (0.4869)	
training:	Epoch: [1][332/408]	Loss 0.5620 (0.4872)	
training:	Epoch: [1][333/408]	Loss 0.2424 (0.4864)	
training:	Epoch: [1][334/408]	Loss 0.4110 (0.4862)	
training:	Epoch: [1][335/408]	Loss 0.2523 (0.4855)	
training:	Epoch: [1][336/408]	Loss 0.3409 (0.4851)	
training:	Epoch: [1][337/408]	Loss 0.7308 (0.4858)	
training:	Epoch: [1][338/408]	Loss 0.6310 (0.4862)	
training:	Epoch: [1][339/408]	Loss 0.4451 (0.4861)	
training:	Epoch: [1][340/408]	Loss 0.2652 (0.4855)	
training:	Epoch: [1][341/408]	Loss 0.3983 (0.4852)	
training:	Epoch: [1][342/408]	Loss 0.5342 (0.4853)	
training:	Epoch: [1][343/408]	Loss 0.4670 (0.4853)	
training:	Epoch: [1][344/408]	Loss 0.2333 (0.4846)	
training:	Epoch: [1][345/408]	Loss 0.5923 (0.4849)	
training:	Epoch: [1][346/408]	Loss 0.2579 (0.4842)	
training:	Epoch: [1][347/408]	Loss 0.2360 (0.4835)	
training:	Epoch: [1][348/408]	Loss 0.5040 (0.4836)	
training:	Epoch: [1][349/408]	Loss 0.4778 (0.4835)	
training:	Epoch: [1][350/408]	Loss 0.5198 (0.4836)	
training:	Epoch: [1][351/408]	Loss 0.3357 (0.4832)	
training:	Epoch: [1][352/408]	Loss 0.3928 (0.4830)	
training:	Epoch: [1][353/408]	Loss 0.3459 (0.4826)	
training:	Epoch: [1][354/408]	Loss 0.4735 (0.4826)	
training:	Epoch: [1][355/408]	Loss 0.4085 (0.4823)	
training:	Epoch: [1][356/408]	Loss 0.3270 (0.4819)	
training:	Epoch: [1][357/408]	Loss 0.4113 (0.4817)	
training:	Epoch: [1][358/408]	Loss 0.4627 (0.4817)	
training:	Epoch: [1][359/408]	Loss 0.2541 (0.4810)	
training:	Epoch: [1][360/408]	Loss 0.5504 (0.4812)	
training:	Epoch: [1][361/408]	Loss 0.3899 (0.4810)	
training:	Epoch: [1][362/408]	Loss 0.6575 (0.4815)	
training:	Epoch: [1][363/408]	Loss 0.3123 (0.4810)	
training:	Epoch: [1][364/408]	Loss 0.3733 (0.4807)	
training:	Epoch: [1][365/408]	Loss 0.4519 (0.4806)	
training:	Epoch: [1][366/408]	Loss 0.3718 (0.4803)	
training:	Epoch: [1][367/408]	Loss 0.3100 (0.4799)	
training:	Epoch: [1][368/408]	Loss 0.4073 (0.4797)	
training:	Epoch: [1][369/408]	Loss 0.2480 (0.4790)	
training:	Epoch: [1][370/408]	Loss 0.4993 (0.4791)	
training:	Epoch: [1][371/408]	Loss 0.4801 (0.4791)	
training:	Epoch: [1][372/408]	Loss 0.5503 (0.4793)	
training:	Epoch: [1][373/408]	Loss 0.3712 (0.4790)	
training:	Epoch: [1][374/408]	Loss 0.3310 (0.4786)	
training:	Epoch: [1][375/408]	Loss 0.3742 (0.4783)	
training:	Epoch: [1][376/408]	Loss 0.4163 (0.4781)	
training:	Epoch: [1][377/408]	Loss 0.3538 (0.4778)	
training:	Epoch: [1][378/408]	Loss 0.3087 (0.4774)	
training:	Epoch: [1][379/408]	Loss 0.5192 (0.4775)	
training:	Epoch: [1][380/408]	Loss 0.2517 (0.4769)	
training:	Epoch: [1][381/408]	Loss 0.4225 (0.4767)	
training:	Epoch: [1][382/408]	Loss 0.4762 (0.4767)	
training:	Epoch: [1][383/408]	Loss 0.2667 (0.4762)	
training:	Epoch: [1][384/408]	Loss 0.3080 (0.4758)	
training:	Epoch: [1][385/408]	Loss 0.4956 (0.4758)	
training:	Epoch: [1][386/408]	Loss 0.5137 (0.4759)	
training:	Epoch: [1][387/408]	Loss 0.5338 (0.4761)	
training:	Epoch: [1][388/408]	Loss 0.2073 (0.4754)	
training:	Epoch: [1][389/408]	Loss 0.3639 (0.4751)	
training:	Epoch: [1][390/408]	Loss 0.3798 (0.4748)	
training:	Epoch: [1][391/408]	Loss 0.4309 (0.4747)	
training:	Epoch: [1][392/408]	Loss 0.7939 (0.4755)	
training:	Epoch: [1][393/408]	Loss 0.3050 (0.4751)	
training:	Epoch: [1][394/408]	Loss 0.4994 (0.4752)	
training:	Epoch: [1][395/408]	Loss 0.3224 (0.4748)	
training:	Epoch: [1][396/408]	Loss 0.2609 (0.4742)	
training:	Epoch: [1][397/408]	Loss 0.2433 (0.4737)	
training:	Epoch: [1][398/408]	Loss 0.3584 (0.4734)	
training:	Epoch: [1][399/408]	Loss 0.2549 (0.4728)	
training:	Epoch: [1][400/408]	Loss 0.4522 (0.4728)	
training:	Epoch: [1][401/408]	Loss 0.2774 (0.4723)	
training:	Epoch: [1][402/408]	Loss 0.6608 (0.4727)	
training:	Epoch: [1][403/408]	Loss 0.3946 (0.4726)	
training:	Epoch: [1][404/408]	Loss 0.4563 (0.4725)	
training:	Epoch: [1][405/408]	Loss 0.5068 (0.4726)	
training:	Epoch: [1][406/408]	Loss 0.2759 (0.4721)	
training:	Epoch: [1][407/408]	Loss 0.3214 (0.4717)	
training:	Epoch: [1][408/408]	Loss 0.4484 (0.4717)	
Training:	 Loss: 0.4710

Training:	 ACC: 0.9167 0.9171 0.9250 0.9085
Validation:	 ACC: 0.8310 0.8320 0.8519 0.8101
Validation:	 Best_BACC: 0.8310 0.8320 0.8519 0.8101
Validation:	 Loss: 0.3865
Pretraining:	Epoch 2/400
----------
training:	Epoch: [2][1/408]	Loss 0.3139 (0.3139)	
training:	Epoch: [2][2/408]	Loss 0.3296 (0.3217)	
training:	Epoch: [2][3/408]	Loss 0.2100 (0.2845)	
training:	Epoch: [2][4/408]	Loss 0.3848 (0.3096)	
training:	Epoch: [2][5/408]	Loss 0.3965 (0.3270)	
training:	Epoch: [2][6/408]	Loss 0.2670 (0.3170)	
training:	Epoch: [2][7/408]	Loss 0.2610 (0.3090)	
training:	Epoch: [2][8/408]	Loss 0.1408 (0.2880)	
training:	Epoch: [2][9/408]	Loss 0.1587 (0.2736)	
training:	Epoch: [2][10/408]	Loss 0.3025 (0.2765)	
training:	Epoch: [2][11/408]	Loss 0.2469 (0.2738)	
training:	Epoch: [2][12/408]	Loss 0.2671 (0.2732)	
training:	Epoch: [2][13/408]	Loss 0.3699 (0.2807)	
training:	Epoch: [2][14/408]	Loss 0.2247 (0.2767)	
training:	Epoch: [2][15/408]	Loss 0.2049 (0.2719)	
training:	Epoch: [2][16/408]	Loss 0.3968 (0.2797)	
training:	Epoch: [2][17/408]	Loss 0.2581 (0.2784)	
training:	Epoch: [2][18/408]	Loss 0.2248 (0.2754)	
training:	Epoch: [2][19/408]	Loss 0.1712 (0.2700)	
training:	Epoch: [2][20/408]	Loss 0.1240 (0.2627)	
training:	Epoch: [2][21/408]	Loss 0.3716 (0.2678)	
training:	Epoch: [2][22/408]	Loss 0.2619 (0.2676)	
training:	Epoch: [2][23/408]	Loss 0.2596 (0.2672)	
training:	Epoch: [2][24/408]	Loss 0.1579 (0.2627)	
training:	Epoch: [2][25/408]	Loss 0.3160 (0.2648)	
training:	Epoch: [2][26/408]	Loss 0.6554 (0.2798)	
training:	Epoch: [2][27/408]	Loss 0.5100 (0.2884)	
training:	Epoch: [2][28/408]	Loss 0.2408 (0.2867)	
training:	Epoch: [2][29/408]	Loss 0.2715 (0.2861)	
training:	Epoch: [2][30/408]	Loss 0.1563 (0.2818)	
training:	Epoch: [2][31/408]	Loss 0.1928 (0.2789)	
training:	Epoch: [2][32/408]	Loss 0.2684 (0.2786)	
training:	Epoch: [2][33/408]	Loss 0.2578 (0.2780)	
training:	Epoch: [2][34/408]	Loss 0.3980 (0.2815)	
training:	Epoch: [2][35/408]	Loss 0.2070 (0.2794)	
training:	Epoch: [2][36/408]	Loss 0.2930 (0.2798)	
training:	Epoch: [2][37/408]	Loss 0.2386 (0.2786)	
training:	Epoch: [2][38/408]	Loss 0.4076 (0.2820)	
training:	Epoch: [2][39/408]	Loss 0.3544 (0.2839)	
training:	Epoch: [2][40/408]	Loss 0.5990 (0.2918)	
training:	Epoch: [2][41/408]	Loss 0.2913 (0.2918)	
training:	Epoch: [2][42/408]	Loss 0.3655 (0.2935)	
training:	Epoch: [2][43/408]	Loss 0.2332 (0.2921)	
training:	Epoch: [2][44/408]	Loss 0.3221 (0.2928)	
training:	Epoch: [2][45/408]	Loss 0.3079 (0.2931)	
training:	Epoch: [2][46/408]	Loss 0.2537 (0.2923)	
training:	Epoch: [2][47/408]	Loss 0.2341 (0.2910)	
training:	Epoch: [2][48/408]	Loss 0.2644 (0.2905)	
training:	Epoch: [2][49/408]	Loss 0.2488 (0.2896)	
training:	Epoch: [2][50/408]	Loss 0.1826 (0.2875)	
training:	Epoch: [2][51/408]	Loss 0.3709 (0.2891)	
training:	Epoch: [2][52/408]	Loss 0.2186 (0.2878)	
training:	Epoch: [2][53/408]	Loss 0.2452 (0.2870)	
training:	Epoch: [2][54/408]	Loss 0.1987 (0.2853)	
training:	Epoch: [2][55/408]	Loss 0.3344 (0.2862)	
training:	Epoch: [2][56/408]	Loss 0.2757 (0.2860)	
training:	Epoch: [2][57/408]	Loss 0.1908 (0.2844)	
training:	Epoch: [2][58/408]	Loss 0.1814 (0.2826)	
training:	Epoch: [2][59/408]	Loss 0.2228 (0.2816)	
training:	Epoch: [2][60/408]	Loss 0.2852 (0.2816)	
training:	Epoch: [2][61/408]	Loss 0.3844 (0.2833)	
training:	Epoch: [2][62/408]	Loss 0.2987 (0.2836)	
training:	Epoch: [2][63/408]	Loss 0.6097 (0.2887)	
training:	Epoch: [2][64/408]	Loss 0.2134 (0.2876)	
training:	Epoch: [2][65/408]	Loss 0.3816 (0.2890)	
training:	Epoch: [2][66/408]	Loss 0.1908 (0.2875)	
training:	Epoch: [2][67/408]	Loss 0.2203 (0.2865)	
training:	Epoch: [2][68/408]	Loss 0.2417 (0.2859)	
training:	Epoch: [2][69/408]	Loss 0.1914 (0.2845)	
training:	Epoch: [2][70/408]	Loss 0.5045 (0.2876)	
training:	Epoch: [2][71/408]	Loss 0.2223 (0.2867)	
training:	Epoch: [2][72/408]	Loss 0.1899 (0.2854)	
training:	Epoch: [2][73/408]	Loss 0.2002 (0.2842)	
training:	Epoch: [2][74/408]	Loss 0.1992 (0.2831)	
training:	Epoch: [2][75/408]	Loss 0.2465 (0.2826)	
training:	Epoch: [2][76/408]	Loss 0.2285 (0.2819)	
training:	Epoch: [2][77/408]	Loss 0.2878 (0.2819)	
training:	Epoch: [2][78/408]	Loss 0.1026 (0.2796)	
training:	Epoch: [2][79/408]	Loss 0.2341 (0.2791)	
training:	Epoch: [2][80/408]	Loss 0.2039 (0.2781)	
training:	Epoch: [2][81/408]	Loss 0.1340 (0.2763)	
training:	Epoch: [2][82/408]	Loss 0.3469 (0.2772)	
training:	Epoch: [2][83/408]	Loss 0.4785 (0.2796)	
training:	Epoch: [2][84/408]	Loss 0.5255 (0.2826)	
training:	Epoch: [2][85/408]	Loss 0.1938 (0.2815)	
training:	Epoch: [2][86/408]	Loss 0.2094 (0.2807)	
training:	Epoch: [2][87/408]	Loss 0.3466 (0.2814)	
training:	Epoch: [2][88/408]	Loss 0.1538 (0.2800)	
training:	Epoch: [2][89/408]	Loss 0.2102 (0.2792)	
training:	Epoch: [2][90/408]	Loss 0.2994 (0.2794)	
training:	Epoch: [2][91/408]	Loss 0.1498 (0.2780)	
training:	Epoch: [2][92/408]	Loss 0.1783 (0.2769)	
training:	Epoch: [2][93/408]	Loss 0.2812 (0.2770)	
training:	Epoch: [2][94/408]	Loss 0.2196 (0.2763)	
training:	Epoch: [2][95/408]	Loss 0.2360 (0.2759)	
training:	Epoch: [2][96/408]	Loss 0.2630 (0.2758)	
training:	Epoch: [2][97/408]	Loss 0.4650 (0.2777)	
training:	Epoch: [2][98/408]	Loss 0.1887 (0.2768)	
training:	Epoch: [2][99/408]	Loss 0.1556 (0.2756)	
training:	Epoch: [2][100/408]	Loss 0.5358 (0.2782)	
training:	Epoch: [2][101/408]	Loss 0.5332 (0.2807)	
training:	Epoch: [2][102/408]	Loss 0.3165 (0.2811)	
training:	Epoch: [2][103/408]	Loss 0.2323 (0.2806)	
training:	Epoch: [2][104/408]	Loss 0.2488 (0.2803)	
training:	Epoch: [2][105/408]	Loss 0.3181 (0.2807)	
training:	Epoch: [2][106/408]	Loss 0.1247 (0.2792)	
training:	Epoch: [2][107/408]	Loss 0.4128 (0.2804)	
training:	Epoch: [2][108/408]	Loss 0.4421 (0.2819)	
training:	Epoch: [2][109/408]	Loss 0.3571 (0.2826)	
training:	Epoch: [2][110/408]	Loss 0.2630 (0.2824)	
training:	Epoch: [2][111/408]	Loss 0.3084 (0.2827)	
training:	Epoch: [2][112/408]	Loss 0.1738 (0.2817)	
training:	Epoch: [2][113/408]	Loss 0.4917 (0.2836)	
training:	Epoch: [2][114/408]	Loss 0.1400 (0.2823)	
training:	Epoch: [2][115/408]	Loss 0.2172 (0.2817)	
training:	Epoch: [2][116/408]	Loss 0.2309 (0.2813)	
training:	Epoch: [2][117/408]	Loss 0.2819 (0.2813)	
training:	Epoch: [2][118/408]	Loss 0.1635 (0.2803)	
training:	Epoch: [2][119/408]	Loss 0.3825 (0.2812)	
training:	Epoch: [2][120/408]	Loss 0.1511 (0.2801)	
training:	Epoch: [2][121/408]	Loss 0.1617 (0.2791)	
training:	Epoch: [2][122/408]	Loss 0.1929 (0.2784)	
training:	Epoch: [2][123/408]	Loss 0.2242 (0.2780)	
training:	Epoch: [2][124/408]	Loss 0.1746 (0.2771)	
training:	Epoch: [2][125/408]	Loss 0.2612 (0.2770)	
training:	Epoch: [2][126/408]	Loss 0.3311 (0.2774)	
training:	Epoch: [2][127/408]	Loss 0.4590 (0.2789)	
training:	Epoch: [2][128/408]	Loss 0.1860 (0.2781)	
training:	Epoch: [2][129/408]	Loss 0.2125 (0.2776)	
training:	Epoch: [2][130/408]	Loss 0.1134 (0.2764)	
training:	Epoch: [2][131/408]	Loss 0.2875 (0.2764)	
training:	Epoch: [2][132/408]	Loss 0.2329 (0.2761)	
training:	Epoch: [2][133/408]	Loss 0.2049 (0.2756)	
training:	Epoch: [2][134/408]	Loss 0.3134 (0.2759)	
training:	Epoch: [2][135/408]	Loss 0.2145 (0.2754)	
training:	Epoch: [2][136/408]	Loss 0.4199 (0.2765)	
training:	Epoch: [2][137/408]	Loss 0.3367 (0.2769)	
training:	Epoch: [2][138/408]	Loss 0.2055 (0.2764)	
training:	Epoch: [2][139/408]	Loss 0.3390 (0.2768)	
training:	Epoch: [2][140/408]	Loss 0.2475 (0.2766)	
training:	Epoch: [2][141/408]	Loss 0.1712 (0.2759)	
training:	Epoch: [2][142/408]	Loss 0.1461 (0.2750)	
training:	Epoch: [2][143/408]	Loss 0.2433 (0.2748)	
training:	Epoch: [2][144/408]	Loss 0.2541 (0.2746)	
training:	Epoch: [2][145/408]	Loss 0.1548 (0.2738)	
training:	Epoch: [2][146/408]	Loss 0.1545 (0.2730)	
training:	Epoch: [2][147/408]	Loss 0.3998 (0.2738)	
training:	Epoch: [2][148/408]	Loss 0.1474 (0.2730)	
training:	Epoch: [2][149/408]	Loss 0.4294 (0.2740)	
training:	Epoch: [2][150/408]	Loss 0.4503 (0.2752)	
training:	Epoch: [2][151/408]	Loss 0.2307 (0.2749)	
training:	Epoch: [2][152/408]	Loss 0.2595 (0.2748)	
training:	Epoch: [2][153/408]	Loss 0.2488 (0.2746)	
training:	Epoch: [2][154/408]	Loss 0.3981 (0.2754)	
training:	Epoch: [2][155/408]	Loss 0.1511 (0.2746)	
training:	Epoch: [2][156/408]	Loss 0.2920 (0.2747)	
training:	Epoch: [2][157/408]	Loss 0.2387 (0.2745)	
training:	Epoch: [2][158/408]	Loss 0.2346 (0.2743)	
training:	Epoch: [2][159/408]	Loss 0.3137 (0.2745)	
training:	Epoch: [2][160/408]	Loss 0.3195 (0.2748)	
training:	Epoch: [2][161/408]	Loss 0.2251 (0.2745)	
training:	Epoch: [2][162/408]	Loss 0.2481 (0.2743)	
training:	Epoch: [2][163/408]	Loss 0.1569 (0.2736)	
training:	Epoch: [2][164/408]	Loss 0.2070 (0.2732)	
training:	Epoch: [2][165/408]	Loss 0.1370 (0.2724)	
training:	Epoch: [2][166/408]	Loss 0.0876 (0.2713)	
training:	Epoch: [2][167/408]	Loss 0.2782 (0.2713)	
training:	Epoch: [2][168/408]	Loss 0.1488 (0.2706)	
training:	Epoch: [2][169/408]	Loss 0.4050 (0.2714)	
training:	Epoch: [2][170/408]	Loss 0.2115 (0.2710)	
training:	Epoch: [2][171/408]	Loss 0.1365 (0.2702)	
training:	Epoch: [2][172/408]	Loss 0.3771 (0.2708)	
training:	Epoch: [2][173/408]	Loss 0.2593 (0.2708)	
training:	Epoch: [2][174/408]	Loss 0.3732 (0.2714)	
training:	Epoch: [2][175/408]	Loss 0.2511 (0.2713)	
training:	Epoch: [2][176/408]	Loss 0.2588 (0.2712)	
training:	Epoch: [2][177/408]	Loss 0.3725 (0.2718)	
training:	Epoch: [2][178/408]	Loss 0.2978 (0.2719)	
training:	Epoch: [2][179/408]	Loss 0.3390 (0.2723)	
training:	Epoch: [2][180/408]	Loss 0.6382 (0.2743)	
training:	Epoch: [2][181/408]	Loss 0.3328 (0.2746)	
training:	Epoch: [2][182/408]	Loss 0.2286 (0.2744)	
training:	Epoch: [2][183/408]	Loss 0.2543 (0.2743)	
training:	Epoch: [2][184/408]	Loss 0.5006 (0.2755)	
training:	Epoch: [2][185/408]	Loss 0.1960 (0.2751)	
training:	Epoch: [2][186/408]	Loss 0.1793 (0.2746)	
training:	Epoch: [2][187/408]	Loss 0.2407 (0.2744)	
training:	Epoch: [2][188/408]	Loss 0.1485 (0.2737)	
training:	Epoch: [2][189/408]	Loss 0.4205 (0.2745)	
training:	Epoch: [2][190/408]	Loss 0.1447 (0.2738)	
training:	Epoch: [2][191/408]	Loss 0.1800 (0.2733)	
training:	Epoch: [2][192/408]	Loss 0.2852 (0.2734)	
training:	Epoch: [2][193/408]	Loss 0.1445 (0.2727)	
training:	Epoch: [2][194/408]	Loss 0.2158 (0.2724)	
training:	Epoch: [2][195/408]	Loss 0.3728 (0.2729)	
training:	Epoch: [2][196/408]	Loss 0.4216 (0.2737)	
training:	Epoch: [2][197/408]	Loss 0.1587 (0.2731)	
training:	Epoch: [2][198/408]	Loss 0.3230 (0.2733)	
training:	Epoch: [2][199/408]	Loss 0.4117 (0.2740)	
training:	Epoch: [2][200/408]	Loss 0.2016 (0.2737)	
training:	Epoch: [2][201/408]	Loss 0.2002 (0.2733)	
training:	Epoch: [2][202/408]	Loss 0.2399 (0.2731)	
training:	Epoch: [2][203/408]	Loss 0.2772 (0.2732)	
training:	Epoch: [2][204/408]	Loss 0.3622 (0.2736)	
training:	Epoch: [2][205/408]	Loss 0.3741 (0.2741)	
training:	Epoch: [2][206/408]	Loss 0.4088 (0.2747)	
training:	Epoch: [2][207/408]	Loss 0.3023 (0.2749)	
training:	Epoch: [2][208/408]	Loss 0.2118 (0.2746)	
training:	Epoch: [2][209/408]	Loss 0.3209 (0.2748)	
training:	Epoch: [2][210/408]	Loss 0.4727 (0.2757)	
training:	Epoch: [2][211/408]	Loss 0.2014 (0.2754)	
training:	Epoch: [2][212/408]	Loss 0.3148 (0.2756)	
training:	Epoch: [2][213/408]	Loss 0.4383 (0.2763)	
training:	Epoch: [2][214/408]	Loss 0.3053 (0.2765)	
training:	Epoch: [2][215/408]	Loss 0.1638 (0.2760)	
training:	Epoch: [2][216/408]	Loss 0.1232 (0.2752)	
training:	Epoch: [2][217/408]	Loss 0.1292 (0.2746)	
training:	Epoch: [2][218/408]	Loss 0.3420 (0.2749)	
training:	Epoch: [2][219/408]	Loss 0.3968 (0.2754)	
training:	Epoch: [2][220/408]	Loss 0.1971 (0.2751)	
training:	Epoch: [2][221/408]	Loss 0.2067 (0.2748)	
training:	Epoch: [2][222/408]	Loss 0.1504 (0.2742)	
training:	Epoch: [2][223/408]	Loss 0.1721 (0.2738)	
training:	Epoch: [2][224/408]	Loss 0.3481 (0.2741)	
training:	Epoch: [2][225/408]	Loss 0.2860 (0.2741)	
training:	Epoch: [2][226/408]	Loss 0.2890 (0.2742)	
training:	Epoch: [2][227/408]	Loss 0.1292 (0.2736)	
training:	Epoch: [2][228/408]	Loss 0.2298 (0.2734)	
training:	Epoch: [2][229/408]	Loss 0.3337 (0.2736)	
training:	Epoch: [2][230/408]	Loss 0.2562 (0.2736)	
training:	Epoch: [2][231/408]	Loss 0.3662 (0.2740)	
training:	Epoch: [2][232/408]	Loss 0.2856 (0.2740)	
training:	Epoch: [2][233/408]	Loss 0.1850 (0.2736)	
training:	Epoch: [2][234/408]	Loss 0.4270 (0.2743)	
training:	Epoch: [2][235/408]	Loss 0.1454 (0.2737)	
training:	Epoch: [2][236/408]	Loss 0.3043 (0.2739)	
training:	Epoch: [2][237/408]	Loss 0.2653 (0.2738)	
training:	Epoch: [2][238/408]	Loss 0.2739 (0.2738)	
training:	Epoch: [2][239/408]	Loss 0.2901 (0.2739)	
training:	Epoch: [2][240/408]	Loss 0.3696 (0.2743)	
training:	Epoch: [2][241/408]	Loss 0.5285 (0.2754)	
training:	Epoch: [2][242/408]	Loss 0.2094 (0.2751)	
training:	Epoch: [2][243/408]	Loss 0.5154 (0.2761)	
training:	Epoch: [2][244/408]	Loss 0.4125 (0.2766)	
training:	Epoch: [2][245/408]	Loss 0.1985 (0.2763)	
training:	Epoch: [2][246/408]	Loss 0.2998 (0.2764)	
training:	Epoch: [2][247/408]	Loss 0.2246 (0.2762)	
training:	Epoch: [2][248/408]	Loss 0.2603 (0.2761)	
training:	Epoch: [2][249/408]	Loss 0.1959 (0.2758)	
training:	Epoch: [2][250/408]	Loss 0.3633 (0.2762)	
training:	Epoch: [2][251/408]	Loss 0.1230 (0.2755)	
training:	Epoch: [2][252/408]	Loss 0.1773 (0.2752)	
training:	Epoch: [2][253/408]	Loss 0.3380 (0.2754)	
training:	Epoch: [2][254/408]	Loss 0.3684 (0.2758)	
training:	Epoch: [2][255/408]	Loss 0.1668 (0.2753)	
training:	Epoch: [2][256/408]	Loss 0.3243 (0.2755)	
training:	Epoch: [2][257/408]	Loss 0.6796 (0.2771)	
training:	Epoch: [2][258/408]	Loss 0.1383 (0.2766)	
training:	Epoch: [2][259/408]	Loss 0.1959 (0.2763)	
training:	Epoch: [2][260/408]	Loss 0.1366 (0.2757)	
training:	Epoch: [2][261/408]	Loss 0.2938 (0.2758)	
training:	Epoch: [2][262/408]	Loss 0.1300 (0.2752)	
training:	Epoch: [2][263/408]	Loss 0.1441 (0.2747)	
training:	Epoch: [2][264/408]	Loss 0.1701 (0.2743)	
training:	Epoch: [2][265/408]	Loss 0.1255 (0.2738)	
training:	Epoch: [2][266/408]	Loss 0.2541 (0.2737)	
training:	Epoch: [2][267/408]	Loss 0.1639 (0.2733)	
training:	Epoch: [2][268/408]	Loss 0.2602 (0.2732)	
training:	Epoch: [2][269/408]	Loss 0.3072 (0.2734)	
training:	Epoch: [2][270/408]	Loss 0.2409 (0.2733)	
training:	Epoch: [2][271/408]	Loss 0.2084 (0.2730)	
training:	Epoch: [2][272/408]	Loss 0.2455 (0.2729)	
training:	Epoch: [2][273/408]	Loss 0.1261 (0.2724)	
training:	Epoch: [2][274/408]	Loss 0.1267 (0.2718)	
training:	Epoch: [2][275/408]	Loss 0.1984 (0.2716)	
training:	Epoch: [2][276/408]	Loss 0.2488 (0.2715)	
training:	Epoch: [2][277/408]	Loss 0.4105 (0.2720)	
training:	Epoch: [2][278/408]	Loss 0.5084 (0.2728)	
training:	Epoch: [2][279/408]	Loss 0.0764 (0.2721)	
training:	Epoch: [2][280/408]	Loss 0.5970 (0.2733)	
training:	Epoch: [2][281/408]	Loss 0.1215 (0.2728)	
training:	Epoch: [2][282/408]	Loss 0.2783 (0.2728)	
training:	Epoch: [2][283/408]	Loss 0.1678 (0.2724)	
training:	Epoch: [2][284/408]	Loss 0.2716 (0.2724)	
training:	Epoch: [2][285/408]	Loss 0.3738 (0.2728)	
training:	Epoch: [2][286/408]	Loss 0.3531 (0.2730)	
training:	Epoch: [2][287/408]	Loss 0.2534 (0.2730)	
training:	Epoch: [2][288/408]	Loss 0.3378 (0.2732)	
training:	Epoch: [2][289/408]	Loss 0.3425 (0.2734)	
training:	Epoch: [2][290/408]	Loss 0.2543 (0.2734)	
training:	Epoch: [2][291/408]	Loss 0.3699 (0.2737)	
training:	Epoch: [2][292/408]	Loss 0.2640 (0.2737)	
training:	Epoch: [2][293/408]	Loss 0.2987 (0.2738)	
training:	Epoch: [2][294/408]	Loss 0.2645 (0.2737)	
training:	Epoch: [2][295/408]	Loss 0.1746 (0.2734)	
training:	Epoch: [2][296/408]	Loss 0.1783 (0.2731)	
training:	Epoch: [2][297/408]	Loss 0.2716 (0.2731)	
training:	Epoch: [2][298/408]	Loss 0.1459 (0.2726)	
training:	Epoch: [2][299/408]	Loss 0.1655 (0.2723)	
training:	Epoch: [2][300/408]	Loss 0.2896 (0.2723)	
training:	Epoch: [2][301/408]	Loss 0.2586 (0.2723)	
training:	Epoch: [2][302/408]	Loss 0.2347 (0.2722)	
training:	Epoch: [2][303/408]	Loss 0.5290 (0.2730)	
training:	Epoch: [2][304/408]	Loss 0.4102 (0.2735)	
training:	Epoch: [2][305/408]	Loss 0.2739 (0.2735)	
training:	Epoch: [2][306/408]	Loss 0.4810 (0.2741)	
training:	Epoch: [2][307/408]	Loss 0.1834 (0.2738)	
training:	Epoch: [2][308/408]	Loss 0.3575 (0.2741)	
training:	Epoch: [2][309/408]	Loss 0.4228 (0.2746)	
training:	Epoch: [2][310/408]	Loss 0.1115 (0.2741)	
training:	Epoch: [2][311/408]	Loss 0.2703 (0.2741)	
training:	Epoch: [2][312/408]	Loss 0.2340 (0.2739)	
training:	Epoch: [2][313/408]	Loss 0.5410 (0.2748)	
training:	Epoch: [2][314/408]	Loss 0.4228 (0.2753)	
training:	Epoch: [2][315/408]	Loss 0.2244 (0.2751)	
training:	Epoch: [2][316/408]	Loss 0.5843 (0.2761)	
training:	Epoch: [2][317/408]	Loss 0.2479 (0.2760)	
training:	Epoch: [2][318/408]	Loss 0.2308 (0.2758)	
training:	Epoch: [2][319/408]	Loss 0.2029 (0.2756)	
training:	Epoch: [2][320/408]	Loss 0.3962 (0.2760)	
training:	Epoch: [2][321/408]	Loss 0.3440 (0.2762)	
training:	Epoch: [2][322/408]	Loss 0.3382 (0.2764)	
training:	Epoch: [2][323/408]	Loss 0.2584 (0.2763)	
training:	Epoch: [2][324/408]	Loss 0.2576 (0.2763)	
training:	Epoch: [2][325/408]	Loss 0.2941 (0.2763)	
training:	Epoch: [2][326/408]	Loss 0.1006 (0.2758)	
training:	Epoch: [2][327/408]	Loss 0.2444 (0.2757)	
training:	Epoch: [2][328/408]	Loss 0.3459 (0.2759)	
training:	Epoch: [2][329/408]	Loss 0.1337 (0.2755)	
training:	Epoch: [2][330/408]	Loss 0.3731 (0.2758)	
training:	Epoch: [2][331/408]	Loss 0.2250 (0.2756)	
training:	Epoch: [2][332/408]	Loss 0.3360 (0.2758)	
training:	Epoch: [2][333/408]	Loss 0.2486 (0.2757)	
training:	Epoch: [2][334/408]	Loss 0.1569 (0.2754)	
training:	Epoch: [2][335/408]	Loss 0.3281 (0.2755)	
training:	Epoch: [2][336/408]	Loss 0.3720 (0.2758)	
training:	Epoch: [2][337/408]	Loss 0.2644 (0.2758)	
training:	Epoch: [2][338/408]	Loss 0.1814 (0.2755)	
training:	Epoch: [2][339/408]	Loss 0.1873 (0.2752)	
training:	Epoch: [2][340/408]	Loss 0.1999 (0.2750)	
training:	Epoch: [2][341/408]	Loss 0.4596 (0.2756)	
training:	Epoch: [2][342/408]	Loss 0.5919 (0.2765)	
training:	Epoch: [2][343/408]	Loss 0.1394 (0.2761)	
training:	Epoch: [2][344/408]	Loss 0.4498 (0.2766)	
training:	Epoch: [2][345/408]	Loss 0.4561 (0.2771)	
training:	Epoch: [2][346/408]	Loss 0.3381 (0.2773)	
training:	Epoch: [2][347/408]	Loss 0.2425 (0.2772)	
training:	Epoch: [2][348/408]	Loss 0.1457 (0.2768)	
training:	Epoch: [2][349/408]	Loss 0.1886 (0.2766)	
training:	Epoch: [2][350/408]	Loss 0.4949 (0.2772)	
training:	Epoch: [2][351/408]	Loss 0.1650 (0.2769)	
training:	Epoch: [2][352/408]	Loss 0.3325 (0.2770)	
training:	Epoch: [2][353/408]	Loss 0.1665 (0.2767)	
