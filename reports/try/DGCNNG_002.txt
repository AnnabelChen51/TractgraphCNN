Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=200, tensorboard=True, net_architecture='DGCNNG', batch_size=16, rate=0.0001, weight=0.0, sched_step=100, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20)
Training the 'DGCNNG' architecture

The following parameters are used:
Batch size:	16
Number of workers:	0
Learning rate:	0.0001
Weight decay:	0.0
Scheduler steps:	100
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	7473
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/467]	Loss 0.7724 (0.7724)	
training:	Epoch: [1][2/467]	Loss 0.8707 (0.8216)	
training:	Epoch: [1][3/467]	Loss 0.7860 (0.8097)	
training:	Epoch: [1][4/467]	Loss 0.5540 (0.7458)	
training:	Epoch: [1][5/467]	Loss 1.0711 (0.8108)	
training:	Epoch: [1][6/467]	Loss 0.6916 (0.7910)	
training:	Epoch: [1][7/467]	Loss 0.7825 (0.7898)	
training:	Epoch: [1][8/467]	Loss 0.9993 (0.8160)	
training:	Epoch: [1][9/467]	Loss 0.5480 (0.7862)	
training:	Epoch: [1][10/467]	Loss 0.7444 (0.7820)	
training:	Epoch: [1][11/467]	Loss 0.6756 (0.7723)	
training:	Epoch: [1][12/467]	Loss 0.8413 (0.7781)	
training:	Epoch: [1][13/467]	Loss 0.2671 (0.7388)	
training:	Epoch: [1][14/467]	Loss 0.4177 (0.7158)	
training:	Epoch: [1][15/467]	Loss 0.8976 (0.7280)	
training:	Epoch: [1][16/467]	Loss 0.5944 (0.7196)	
training:	Epoch: [1][17/467]	Loss 0.8255 (0.7258)	
training:	Epoch: [1][18/467]	Loss 0.5904 (0.7183)	
training:	Epoch: [1][19/467]	Loss 0.7947 (0.7223)	
training:	Epoch: [1][20/467]	Loss 0.6626 (0.7194)	
training:	Epoch: [1][21/467]	Loss 0.4671 (0.7073)	
training:	Epoch: [1][22/467]	Loss 1.0895 (0.7247)	
training:	Epoch: [1][23/467]	Loss 0.8418 (0.7298)	
training:	Epoch: [1][24/467]	Loss 0.6690 (0.7273)	
training:	Epoch: [1][25/467]	Loss 0.5894 (0.7218)	
training:	Epoch: [1][26/467]	Loss 0.8450 (0.7265)	
training:	Epoch: [1][27/467]	Loss 0.6426 (0.7234)	
training:	Epoch: [1][28/467]	Loss 0.4583 (0.7139)	
training:	Epoch: [1][29/467]	Loss 0.8558 (0.7188)	
training:	Epoch: [1][30/467]	Loss 0.7053 (0.7184)	
training:	Epoch: [1][31/467]	Loss 0.5678 (0.7135)	
training:	Epoch: [1][32/467]	Loss 0.5982 (0.7099)	
training:	Epoch: [1][33/467]	Loss 0.7609 (0.7115)	
training:	Epoch: [1][34/467]	Loss 0.8376 (0.7152)	
training:	Epoch: [1][35/467]	Loss 0.5331 (0.7100)	
training:	Epoch: [1][36/467]	Loss 0.7791 (0.7119)	
training:	Epoch: [1][37/467]	Loss 0.6876 (0.7112)	
training:	Epoch: [1][38/467]	Loss 0.6363 (0.7093)	
training:	Epoch: [1][39/467]	Loss 0.5980 (0.7064)	
training:	Epoch: [1][40/467]	Loss 0.7535 (0.7076)	
training:	Epoch: [1][41/467]	Loss 0.5980 (0.7049)	
training:	Epoch: [1][42/467]	Loss 0.7362 (0.7057)	
training:	Epoch: [1][43/467]	Loss 0.5225 (0.7014)	
training:	Epoch: [1][44/467]	Loss 0.6796 (0.7009)	
training:	Epoch: [1][45/467]	Loss 0.6160 (0.6990)	
training:	Epoch: [1][46/467]	Loss 0.6161 (0.6972)	
training:	Epoch: [1][47/467]	Loss 0.6875 (0.6970)	
training:	Epoch: [1][48/467]	Loss 0.5828 (0.6946)	
training:	Epoch: [1][49/467]	Loss 0.5293 (0.6912)	
training:	Epoch: [1][50/467]	Loss 0.5948 (0.6893)	
training:	Epoch: [1][51/467]	Loss 0.4338 (0.6843)	
training:	Epoch: [1][52/467]	Loss 0.4727 (0.6802)	
training:	Epoch: [1][53/467]	Loss 0.4702 (0.6763)	
training:	Epoch: [1][54/467]	Loss 0.4504 (0.6721)	
training:	Epoch: [1][55/467]	Loss 0.5048 (0.6691)	
training:	Epoch: [1][56/467]	Loss 0.5569 (0.6670)	
training:	Epoch: [1][57/467]	Loss 0.5415 (0.6648)	
training:	Epoch: [1][58/467]	Loss 0.7665 (0.6666)	
training:	Epoch: [1][59/467]	Loss 0.5265 (0.6642)	
training:	Epoch: [1][60/467]	Loss 0.7184 (0.6651)	
training:	Epoch: [1][61/467]	Loss 0.6149 (0.6643)	
training:	Epoch: [1][62/467]	Loss 0.6686 (0.6644)	
training:	Epoch: [1][63/467]	Loss 0.5208 (0.6621)	
training:	Epoch: [1][64/467]	Loss 0.6391 (0.6617)	
training:	Epoch: [1][65/467]	Loss 0.5968 (0.6607)	
training:	Epoch: [1][66/467]	Loss 0.6854 (0.6611)	
training:	Epoch: [1][67/467]	Loss 0.5084 (0.6588)	
training:	Epoch: [1][68/467]	Loss 0.5102 (0.6566)	
training:	Epoch: [1][69/467]	Loss 0.6232 (0.6562)	
training:	Epoch: [1][70/467]	Loss 0.5203 (0.6542)	
training:	Epoch: [1][71/467]	Loss 0.7110 (0.6550)	
training:	Epoch: [1][72/467]	Loss 0.6181 (0.6545)	
training:	Epoch: [1][73/467]	Loss 0.4401 (0.6516)	
training:	Epoch: [1][74/467]	Loss 0.3464 (0.6474)	
training:	Epoch: [1][75/467]	Loss 0.4766 (0.6452)	
training:	Epoch: [1][76/467]	Loss 0.4879 (0.6431)	
training:	Epoch: [1][77/467]	Loss 0.5143 (0.6414)	
training:	Epoch: [1][78/467]	Loss 0.6858 (0.6420)	
training:	Epoch: [1][79/467]	Loss 0.3645 (0.6385)	
training:	Epoch: [1][80/467]	Loss 0.4068 (0.6356)	
training:	Epoch: [1][81/467]	Loss 0.6923 (0.6363)	
training:	Epoch: [1][82/467]	Loss 0.4358 (0.6338)	
training:	Epoch: [1][83/467]	Loss 0.4337 (0.6314)	
training:	Epoch: [1][84/467]	Loss 0.6338 (0.6315)	
training:	Epoch: [1][85/467]	Loss 0.2958 (0.6275)	
training:	Epoch: [1][86/467]	Loss 0.5307 (0.6264)	
training:	Epoch: [1][87/467]	Loss 0.6392 (0.6265)	
training:	Epoch: [1][88/467]	Loss 0.6354 (0.6266)	
training:	Epoch: [1][89/467]	Loss 0.6260 (0.6266)	
training:	Epoch: [1][90/467]	Loss 0.5288 (0.6255)	
training:	Epoch: [1][91/467]	Loss 0.6953 (0.6263)	
training:	Epoch: [1][92/467]	Loss 0.5118 (0.6251)	
training:	Epoch: [1][93/467]	Loss 0.6751 (0.6256)	
training:	Epoch: [1][94/467]	Loss 0.4833 (0.6241)	
training:	Epoch: [1][95/467]	Loss 0.7154 (0.6250)	
training:	Epoch: [1][96/467]	Loss 0.3787 (0.6225)	
training:	Epoch: [1][97/467]	Loss 0.6301 (0.6226)	
training:	Epoch: [1][98/467]	Loss 0.5293 (0.6216)	
training:	Epoch: [1][99/467]	Loss 0.4245 (0.6196)	
training:	Epoch: [1][100/467]	Loss 0.5462 (0.6189)	
training:	Epoch: [1][101/467]	Loss 0.4410 (0.6171)	
training:	Epoch: [1][102/467]	Loss 0.5411 (0.6164)	
training:	Epoch: [1][103/467]	Loss 0.5280 (0.6155)	
training:	Epoch: [1][104/467]	Loss 0.3891 (0.6133)	
training:	Epoch: [1][105/467]	Loss 0.4686 (0.6120)	
training:	Epoch: [1][106/467]	Loss 0.5371 (0.6113)	
training:	Epoch: [1][107/467]	Loss 0.3531 (0.6088)	
training:	Epoch: [1][108/467]	Loss 0.4275 (0.6072)	
training:	Epoch: [1][109/467]	Loss 0.7217 (0.6082)	
training:	Epoch: [1][110/467]	Loss 0.4051 (0.6064)	
training:	Epoch: [1][111/467]	Loss 0.4945 (0.6054)	
training:	Epoch: [1][112/467]	Loss 0.5067 (0.6045)	
training:	Epoch: [1][113/467]	Loss 0.3788 (0.6025)	
training:	Epoch: [1][114/467]	Loss 0.4220 (0.6009)	
training:	Epoch: [1][115/467]	Loss 0.5974 (0.6009)	
training:	Epoch: [1][116/467]	Loss 0.5536 (0.6005)	
training:	Epoch: [1][117/467]	Loss 0.3626 (0.5984)	
training:	Epoch: [1][118/467]	Loss 0.4281 (0.5970)	
training:	Epoch: [1][119/467]	Loss 0.7210 (0.5980)	
training:	Epoch: [1][120/467]	Loss 0.7483 (0.5993)	
training:	Epoch: [1][121/467]	Loss 0.5289 (0.5987)	
training:	Epoch: [1][122/467]	Loss 0.4895 (0.5978)	
training:	Epoch: [1][123/467]	Loss 0.5354 (0.5973)	
training:	Epoch: [1][124/467]	Loss 0.5347 (0.5968)	
training:	Epoch: [1][125/467]	Loss 0.3710 (0.5950)	
training:	Epoch: [1][126/467]	Loss 0.3881 (0.5933)	
training:	Epoch: [1][127/467]	Loss 0.7893 (0.5949)	
training:	Epoch: [1][128/467]	Loss 0.4601 (0.5938)	
training:	Epoch: [1][129/467]	Loss 1.0530 (0.5974)	
training:	Epoch: [1][130/467]	Loss 0.3389 (0.5954)	
training:	Epoch: [1][131/467]	Loss 0.4443 (0.5942)	
training:	Epoch: [1][132/467]	Loss 0.2915 (0.5920)	
training:	Epoch: [1][133/467]	Loss 0.4871 (0.5912)	
training:	Epoch: [1][134/467]	Loss 0.5463 (0.5908)	
training:	Epoch: [1][135/467]	Loss 0.4003 (0.5894)	
training:	Epoch: [1][136/467]	Loss 0.4348 (0.5883)	
training:	Epoch: [1][137/467]	Loss 0.6273 (0.5886)	
training:	Epoch: [1][138/467]	Loss 0.2059 (0.5858)	
training:	Epoch: [1][139/467]	Loss 0.6745 (0.5864)	
training:	Epoch: [1][140/467]	Loss 0.4767 (0.5856)	
training:	Epoch: [1][141/467]	Loss 0.3302 (0.5838)	
training:	Epoch: [1][142/467]	Loss 0.4600 (0.5830)	
training:	Epoch: [1][143/467]	Loss 0.8131 (0.5846)	
training:	Epoch: [1][144/467]	Loss 0.6086 (0.5847)	
training:	Epoch: [1][145/467]	Loss 0.6508 (0.5852)	
training:	Epoch: [1][146/467]	Loss 0.4780 (0.5845)	
training:	Epoch: [1][147/467]	Loss 0.4763 (0.5837)	
training:	Epoch: [1][148/467]	Loss 0.5008 (0.5832)	
training:	Epoch: [1][149/467]	Loss 0.3959 (0.5819)	
training:	Epoch: [1][150/467]	Loss 0.5219 (0.5815)	
training:	Epoch: [1][151/467]	Loss 0.5245 (0.5811)	
training:	Epoch: [1][152/467]	Loss 0.6203 (0.5814)	
training:	Epoch: [1][153/467]	Loss 0.4867 (0.5808)	
training:	Epoch: [1][154/467]	Loss 0.6435 (0.5812)	
training:	Epoch: [1][155/467]	Loss 0.2563 (0.5791)	
training:	Epoch: [1][156/467]	Loss 0.4869 (0.5785)	
training:	Epoch: [1][157/467]	Loss 0.3436 (0.5770)	
training:	Epoch: [1][158/467]	Loss 0.5839 (0.5770)	
training:	Epoch: [1][159/467]	Loss 0.4061 (0.5760)	
training:	Epoch: [1][160/467]	Loss 0.5949 (0.5761)	
training:	Epoch: [1][161/467]	Loss 0.4523 (0.5753)	
training:	Epoch: [1][162/467]	Loss 0.4890 (0.5748)	
training:	Epoch: [1][163/467]	Loss 0.6059 (0.5750)	
training:	Epoch: [1][164/467]	Loss 0.4298 (0.5741)	
training:	Epoch: [1][165/467]	Loss 0.5440 (0.5739)	
training:	Epoch: [1][166/467]	Loss 0.4964 (0.5734)	
training:	Epoch: [1][167/467]	Loss 0.2979 (0.5718)	
training:	Epoch: [1][168/467]	Loss 0.4137 (0.5708)	
training:	Epoch: [1][169/467]	Loss 0.3612 (0.5696)	
training:	Epoch: [1][170/467]	Loss 0.7395 (0.5706)	
training:	Epoch: [1][171/467]	Loss 0.4769 (0.5701)	
training:	Epoch: [1][172/467]	Loss 0.4843 (0.5696)	
training:	Epoch: [1][173/467]	Loss 0.2872 (0.5679)	
training:	Epoch: [1][174/467]	Loss 0.5890 (0.5680)	
training:	Epoch: [1][175/467]	Loss 0.5012 (0.5677)	
training:	Epoch: [1][176/467]	Loss 0.6728 (0.5683)	
training:	Epoch: [1][177/467]	Loss 0.5535 (0.5682)	
training:	Epoch: [1][178/467]	Loss 0.5684 (0.5682)	
training:	Epoch: [1][179/467]	Loss 0.3854 (0.5672)	
training:	Epoch: [1][180/467]	Loss 0.4299 (0.5664)	
training:	Epoch: [1][181/467]	Loss 0.4876 (0.5660)	
training:	Epoch: [1][182/467]	Loss 0.4467 (0.5653)	
training:	Epoch: [1][183/467]	Loss 0.7815 (0.5665)	
training:	Epoch: [1][184/467]	Loss 0.4317 (0.5658)	
training:	Epoch: [1][185/467]	Loss 0.4412 (0.5651)	
training:	Epoch: [1][186/467]	Loss 0.5201 (0.5648)	
training:	Epoch: [1][187/467]	Loss 0.3764 (0.5638)	
training:	Epoch: [1][188/467]	Loss 0.5014 (0.5635)	
training:	Epoch: [1][189/467]	Loss 0.3986 (0.5626)	
training:	Epoch: [1][190/467]	Loss 0.4545 (0.5621)	
training:	Epoch: [1][191/467]	Loss 0.5433 (0.5620)	
training:	Epoch: [1][192/467]	Loss 0.5721 (0.5620)	
training:	Epoch: [1][193/467]	Loss 0.7759 (0.5631)	
training:	Epoch: [1][194/467]	Loss 0.5882 (0.5633)	
training:	Epoch: [1][195/467]	Loss 0.4072 (0.5625)	
training:	Epoch: [1][196/467]	Loss 0.4928 (0.5621)	
training:	Epoch: [1][197/467]	Loss 0.4057 (0.5613)	
training:	Epoch: [1][198/467]	Loss 0.4464 (0.5607)	
training:	Epoch: [1][199/467]	Loss 0.3065 (0.5594)	
training:	Epoch: [1][200/467]	Loss 0.3810 (0.5586)	
training:	Epoch: [1][201/467]	Loss 0.4267 (0.5579)	
training:	Epoch: [1][202/467]	Loss 0.3228 (0.5567)	
training:	Epoch: [1][203/467]	Loss 0.5715 (0.5568)	
training:	Epoch: [1][204/467]	Loss 0.5027 (0.5565)	
training:	Epoch: [1][205/467]	Loss 0.8834 (0.5581)	
training:	Epoch: [1][206/467]	Loss 0.2555 (0.5567)	
training:	Epoch: [1][207/467]	Loss 0.6570 (0.5572)	
training:	Epoch: [1][208/467]	Loss 0.5361 (0.5570)	
training:	Epoch: [1][209/467]	Loss 0.3710 (0.5562)	
training:	Epoch: [1][210/467]	Loss 0.2749 (0.5548)	
training:	Epoch: [1][211/467]	Loss 0.6278 (0.5552)	
training:	Epoch: [1][212/467]	Loss 0.5172 (0.5550)	
training:	Epoch: [1][213/467]	Loss 0.5585 (0.5550)	
training:	Epoch: [1][214/467]	Loss 0.4580 (0.5546)	
training:	Epoch: [1][215/467]	Loss 0.3985 (0.5538)	
training:	Epoch: [1][216/467]	Loss 0.4941 (0.5535)	
training:	Epoch: [1][217/467]	Loss 0.5371 (0.5535)	
training:	Epoch: [1][218/467]	Loss 0.3787 (0.5527)	
training:	Epoch: [1][219/467]	Loss 0.5901 (0.5528)	
training:	Epoch: [1][220/467]	Loss 0.6530 (0.5533)	
training:	Epoch: [1][221/467]	Loss 0.3025 (0.5522)	
training:	Epoch: [1][222/467]	Loss 0.5165 (0.5520)	
training:	Epoch: [1][223/467]	Loss 0.5120 (0.5518)	
training:	Epoch: [1][224/467]	Loss 0.3891 (0.5511)	
training:	Epoch: [1][225/467]	Loss 0.4096 (0.5505)	
training:	Epoch: [1][226/467]	Loss 0.7331 (0.5513)	
training:	Epoch: [1][227/467]	Loss 0.4075 (0.5506)	
training:	Epoch: [1][228/467]	Loss 0.4432 (0.5502)	
training:	Epoch: [1][229/467]	Loss 0.1821 (0.5486)	
training:	Epoch: [1][230/467]	Loss 0.4933 (0.5483)	
training:	Epoch: [1][231/467]	Loss 0.3349 (0.5474)	
training:	Epoch: [1][232/467]	Loss 0.3984 (0.5468)	
training:	Epoch: [1][233/467]	Loss 0.4791 (0.5465)	
training:	Epoch: [1][234/467]	Loss 0.2495 (0.5452)	
training:	Epoch: [1][235/467]	Loss 0.5541 (0.5452)	
training:	Epoch: [1][236/467]	Loss 0.3656 (0.5445)	
training:	Epoch: [1][237/467]	Loss 0.4174 (0.5439)	
training:	Epoch: [1][238/467]	Loss 0.8181 (0.5451)	
training:	Epoch: [1][239/467]	Loss 0.3872 (0.5444)	
training:	Epoch: [1][240/467]	Loss 0.4217 (0.5439)	
training:	Epoch: [1][241/467]	Loss 0.4439 (0.5435)	
training:	Epoch: [1][242/467]	Loss 0.7154 (0.5442)	
training:	Epoch: [1][243/467]	Loss 0.3601 (0.5435)	
training:	Epoch: [1][244/467]	Loss 0.4834 (0.5432)	
training:	Epoch: [1][245/467]	Loss 0.3953 (0.5426)	
training:	Epoch: [1][246/467]	Loss 0.2812 (0.5415)	
training:	Epoch: [1][247/467]	Loss 0.3724 (0.5409)	
training:	Epoch: [1][248/467]	Loss 0.4812 (0.5406)	
training:	Epoch: [1][249/467]	Loss 0.6922 (0.5412)	
training:	Epoch: [1][250/467]	Loss 0.3546 (0.5405)	
training:	Epoch: [1][251/467]	Loss 0.5625 (0.5406)	
training:	Epoch: [1][252/467]	Loss 0.3509 (0.5398)	
training:	Epoch: [1][253/467]	Loss 0.4519 (0.5395)	
training:	Epoch: [1][254/467]	Loss 0.5063 (0.5393)	
training:	Epoch: [1][255/467]	Loss 0.7714 (0.5402)	
training:	Epoch: [1][256/467]	Loss 0.2174 (0.5390)	
training:	Epoch: [1][257/467]	Loss 0.6262 (0.5393)	
training:	Epoch: [1][258/467]	Loss 0.2707 (0.5383)	
training:	Epoch: [1][259/467]	Loss 0.4536 (0.5380)	
training:	Epoch: [1][260/467]	Loss 0.5495 (0.5380)	
training:	Epoch: [1][261/467]	Loss 0.2302 (0.5368)	
training:	Epoch: [1][262/467]	Loss 0.4948 (0.5367)	
training:	Epoch: [1][263/467]	Loss 0.4779 (0.5364)	
training:	Epoch: [1][264/467]	Loss 0.2482 (0.5353)	
training:	Epoch: [1][265/467]	Loss 0.3947 (0.5348)	
training:	Epoch: [1][266/467]	Loss 0.5566 (0.5349)	
training:	Epoch: [1][267/467]	Loss 0.8343 (0.5360)	
training:	Epoch: [1][268/467]	Loss 0.3712 (0.5354)	
training:	Epoch: [1][269/467]	Loss 0.5301 (0.5354)	
training:	Epoch: [1][270/467]	Loss 0.4964 (0.5352)	
training:	Epoch: [1][271/467]	Loss 0.4803 (0.5350)	
training:	Epoch: [1][272/467]	Loss 0.5577 (0.5351)	
training:	Epoch: [1][273/467]	Loss 0.6589 (0.5356)	
training:	Epoch: [1][274/467]	Loss 0.2151 (0.5344)	
training:	Epoch: [1][275/467]	Loss 0.4059 (0.5339)	
training:	Epoch: [1][276/467]	Loss 0.3706 (0.5333)	
training:	Epoch: [1][277/467]	Loss 0.3588 (0.5327)	
training:	Epoch: [1][278/467]	Loss 0.5169 (0.5327)	
training:	Epoch: [1][279/467]	Loss 0.2935 (0.5318)	
training:	Epoch: [1][280/467]	Loss 0.2370 (0.5307)	
training:	Epoch: [1][281/467]	Loss 0.6610 (0.5312)	
training:	Epoch: [1][282/467]	Loss 0.6900 (0.5318)	
training:	Epoch: [1][283/467]	Loss 0.3535 (0.5311)	
training:	Epoch: [1][284/467]	Loss 0.4260 (0.5308)	
training:	Epoch: [1][285/467]	Loss 0.4813 (0.5306)	
training:	Epoch: [1][286/467]	Loss 0.3393 (0.5299)	
training:	Epoch: [1][287/467]	Loss 0.3218 (0.5292)	
training:	Epoch: [1][288/467]	Loss 0.5155 (0.5292)	
training:	Epoch: [1][289/467]	Loss 0.6312 (0.5295)	
training:	Epoch: [1][290/467]	Loss 0.4619 (0.5293)	
training:	Epoch: [1][291/467]	Loss 0.3251 (0.5286)	
training:	Epoch: [1][292/467]	Loss 0.3529 (0.5280)	
training:	Epoch: [1][293/467]	Loss 0.5236 (0.5280)	
training:	Epoch: [1][294/467]	Loss 0.4619 (0.5277)	
training:	Epoch: [1][295/467]	Loss 0.3962 (0.5273)	
training:	Epoch: [1][296/467]	Loss 0.2104 (0.5262)	
training:	Epoch: [1][297/467]	Loss 0.5776 (0.5264)	
training:	Epoch: [1][298/467]	Loss 0.4123 (0.5260)	
training:	Epoch: [1][299/467]	Loss 0.3853 (0.5255)	
training:	Epoch: [1][300/467]	Loss 0.4759 (0.5254)	
training:	Epoch: [1][301/467]	Loss 0.3799 (0.5249)	
training:	Epoch: [1][302/467]	Loss 0.7773 (0.5257)	
training:	Epoch: [1][303/467]	Loss 0.6949 (0.5263)	
training:	Epoch: [1][304/467]	Loss 0.4009 (0.5259)	
training:	Epoch: [1][305/467]	Loss 0.3157 (0.5252)	
training:	Epoch: [1][306/467]	Loss 0.4003 (0.5248)	
training:	Epoch: [1][307/467]	Loss 0.4253 (0.5245)	
training:	Epoch: [1][308/467]	Loss 0.4596 (0.5242)	
training:	Epoch: [1][309/467]	Loss 0.4920 (0.5241)	
training:	Epoch: [1][310/467]	Loss 0.3026 (0.5234)	
training:	Epoch: [1][311/467]	Loss 0.6814 (0.5239)	
training:	Epoch: [1][312/467]	Loss 0.6013 (0.5242)	
training:	Epoch: [1][313/467]	Loss 0.3085 (0.5235)	
training:	Epoch: [1][314/467]	Loss 0.2085 (0.5225)	
training:	Epoch: [1][315/467]	Loss 0.2944 (0.5218)	
training:	Epoch: [1][316/467]	Loss 0.3721 (0.5213)	
training:	Epoch: [1][317/467]	Loss 0.3194 (0.5206)	
training:	Epoch: [1][318/467]	Loss 0.5188 (0.5206)	
training:	Epoch: [1][319/467]	Loss 0.5721 (0.5208)	
training:	Epoch: [1][320/467]	Loss 0.4109 (0.5205)	
training:	Epoch: [1][321/467]	Loss 0.4774 (0.5203)	
training:	Epoch: [1][322/467]	Loss 0.5286 (0.5204)	
training:	Epoch: [1][323/467]	Loss 0.2927 (0.5196)	
training:	Epoch: [1][324/467]	Loss 0.3058 (0.5190)	
training:	Epoch: [1][325/467]	Loss 0.2970 (0.5183)	
training:	Epoch: [1][326/467]	Loss 0.3223 (0.5177)	
training:	Epoch: [1][327/467]	Loss 0.3501 (0.5172)	
training:	Epoch: [1][328/467]	Loss 0.7257 (0.5178)	
training:	Epoch: [1][329/467]	Loss 0.3015 (0.5172)	
training:	Epoch: [1][330/467]	Loss 0.4656 (0.5170)	
training:	Epoch: [1][331/467]	Loss 0.3136 (0.5164)	
training:	Epoch: [1][332/467]	Loss 0.4356 (0.5162)	
training:	Epoch: [1][333/467]	Loss 0.4532 (0.5160)	
training:	Epoch: [1][334/467]	Loss 0.3359 (0.5154)	
training:	Epoch: [1][335/467]	Loss 0.2920 (0.5148)	
training:	Epoch: [1][336/467]	Loss 0.6846 (0.5153)	
training:	Epoch: [1][337/467]	Loss 0.3129 (0.5147)	
training:	Epoch: [1][338/467]	Loss 0.4878 (0.5146)	
training:	Epoch: [1][339/467]	Loss 0.4760 (0.5145)	
training:	Epoch: [1][340/467]	Loss 0.2625 (0.5137)	
training:	Epoch: [1][341/467]	Loss 0.4481 (0.5135)	
training:	Epoch: [1][342/467]	Loss 0.3245 (0.5130)	
training:	Epoch: [1][343/467]	Loss 0.8890 (0.5141)	
training:	Epoch: [1][344/467]	Loss 0.3910 (0.5137)	
training:	Epoch: [1][345/467]	Loss 0.4850 (0.5136)	
training:	Epoch: [1][346/467]	Loss 0.5287 (0.5137)	
training:	Epoch: [1][347/467]	Loss 0.2903 (0.5130)	
training:	Epoch: [1][348/467]	Loss 0.3331 (0.5125)	
training:	Epoch: [1][349/467]	Loss 0.2334 (0.5117)	
training:	Epoch: [1][350/467]	Loss 0.5245 (0.5118)	
training:	Epoch: [1][351/467]	Loss 0.3298 (0.5112)	
training:	Epoch: [1][352/467]	Loss 0.3914 (0.5109)	
training:	Epoch: [1][353/467]	Loss 0.4086 (0.5106)	
training:	Epoch: [1][354/467]	Loss 0.2874 (0.5100)	
training:	Epoch: [1][355/467]	Loss 0.4104 (0.5097)	
training:	Epoch: [1][356/467]	Loss 0.6082 (0.5100)	
training:	Epoch: [1][357/467]	Loss 0.5235 (0.5100)	
training:	Epoch: [1][358/467]	Loss 0.3183 (0.5095)	
training:	Epoch: [1][359/467]	Loss 0.7251 (0.5101)	
training:	Epoch: [1][360/467]	Loss 0.4100 (0.5098)	
training:	Epoch: [1][361/467]	Loss 0.5148 (0.5098)	
training:	Epoch: [1][362/467]	Loss 0.4896 (0.5098)	
training:	Epoch: [1][363/467]	Loss 0.4821 (0.5097)	
training:	Epoch: [1][364/467]	Loss 0.2321 (0.5089)	
training:	Epoch: [1][365/467]	Loss 0.4835 (0.5089)	
training:	Epoch: [1][366/467]	Loss 0.2365 (0.5081)	
training:	Epoch: [1][367/467]	Loss 0.2934 (0.5075)	
training:	Epoch: [1][368/467]	Loss 0.5033 (0.5075)	
training:	Epoch: [1][369/467]	Loss 0.4406 (0.5073)	
training:	Epoch: [1][370/467]	Loss 0.2342 (0.5066)	
training:	Epoch: [1][371/467]	Loss 0.2810 (0.5060)	
training:	Epoch: [1][372/467]	Loss 0.3380 (0.5055)	
training:	Epoch: [1][373/467]	Loss 0.5814 (0.5057)	
training:	Epoch: [1][374/467]	Loss 0.3234 (0.5052)	
training:	Epoch: [1][375/467]	Loss 0.2518 (0.5046)	
training:	Epoch: [1][376/467]	Loss 0.5749 (0.5048)	
training:	Epoch: [1][377/467]	Loss 0.4031 (0.5045)	
training:	Epoch: [1][378/467]	Loss 0.5470 (0.5046)	
training:	Epoch: [1][379/467]	Loss 0.3013 (0.5041)	
training:	Epoch: [1][380/467]	Loss 0.4665 (0.5040)	
training:	Epoch: [1][381/467]	Loss 0.3438 (0.5035)	
training:	Epoch: [1][382/467]	Loss 0.5552 (0.5037)	
training:	Epoch: [1][383/467]	Loss 0.3648 (0.5033)	
training:	Epoch: [1][384/467]	Loss 0.4543 (0.5032)	
training:	Epoch: [1][385/467]	Loss 0.5114 (0.5032)	
training:	Epoch: [1][386/467]	Loss 0.2841 (0.5026)	
training:	Epoch: [1][387/467]	Loss 0.3216 (0.5022)	
training:	Epoch: [1][388/467]	Loss 0.3445 (0.5018)	
training:	Epoch: [1][389/467]	Loss 0.3966 (0.5015)	
training:	Epoch: [1][390/467]	Loss 0.5073 (0.5015)	
training:	Epoch: [1][391/467]	Loss 0.7243 (0.5021)	
training:	Epoch: [1][392/467]	Loss 0.7420 (0.5027)	
training:	Epoch: [1][393/467]	Loss 0.3243 (0.5022)	
training:	Epoch: [1][394/467]	Loss 0.6155 (0.5025)	
training:	Epoch: [1][395/467]	Loss 0.4219 (0.5023)	
training:	Epoch: [1][396/467]	Loss 0.3853 (0.5020)	
training:	Epoch: [1][397/467]	Loss 0.6031 (0.5023)	
training:	Epoch: [1][398/467]	Loss 0.3316 (0.5019)	
training:	Epoch: [1][399/467]	Loss 0.4861 (0.5018)	
training:	Epoch: [1][400/467]	Loss 0.3782 (0.5015)	
training:	Epoch: [1][401/467]	Loss 0.7763 (0.5022)	
training:	Epoch: [1][402/467]	Loss 0.3769 (0.5019)	
training:	Epoch: [1][403/467]	Loss 0.4763 (0.5018)	
training:	Epoch: [1][404/467]	Loss 0.5001 (0.5018)	
training:	Epoch: [1][405/467]	Loss 0.3018 (0.5013)	
training:	Epoch: [1][406/467]	Loss 0.2363 (0.5007)	
training:	Epoch: [1][407/467]	Loss 0.3685 (0.5003)	
training:	Epoch: [1][408/467]	Loss 0.6619 (0.5007)	
training:	Epoch: [1][409/467]	Loss 0.2725 (0.5002)	
training:	Epoch: [1][410/467]	Loss 0.1832 (0.4994)	
training:	Epoch: [1][411/467]	Loss 0.2373 (0.4988)	
training:	Epoch: [1][412/467]	Loss 0.2766 (0.4982)	
training:	Epoch: [1][413/467]	Loss 0.7592 (0.4989)	
training:	Epoch: [1][414/467]	Loss 0.6004 (0.4991)	
training:	Epoch: [1][415/467]	Loss 0.5402 (0.4992)	
training:	Epoch: [1][416/467]	Loss 0.3881 (0.4989)	
training:	Epoch: [1][417/467]	Loss 0.4379 (0.4988)	
training:	Epoch: [1][418/467]	Loss 0.6881 (0.4992)	
training:	Epoch: [1][419/467]	Loss 0.3336 (0.4989)	
training:	Epoch: [1][420/467]	Loss 0.4047 (0.4986)	
training:	Epoch: [1][421/467]	Loss 0.3485 (0.4983)	
training:	Epoch: [1][422/467]	Loss 0.3947 (0.4980)	
training:	Epoch: [1][423/467]	Loss 0.2030 (0.4973)	
training:	Epoch: [1][424/467]	Loss 0.3575 (0.4970)	
training:	Epoch: [1][425/467]	Loss 0.4554 (0.4969)	
training:	Epoch: [1][426/467]	Loss 0.4249 (0.4967)	
training:	Epoch: [1][427/467]	Loss 0.5414 (0.4968)	
training:	Epoch: [1][428/467]	Loss 0.6008 (0.4971)	
training:	Epoch: [1][429/467]	Loss 0.2767 (0.4966)	
training:	Epoch: [1][430/467]	Loss 0.5048 (0.4966)	
training:	Epoch: [1][431/467]	Loss 0.4732 (0.4965)	
training:	Epoch: [1][432/467]	Loss 0.4926 (0.4965)	
training:	Epoch: [1][433/467]	Loss 0.4271 (0.4964)	
training:	Epoch: [1][434/467]	Loss 0.5127 (0.4964)	
training:	Epoch: [1][435/467]	Loss 0.3784 (0.4961)	
training:	Epoch: [1][436/467]	Loss 0.4543 (0.4960)	
training:	Epoch: [1][437/467]	Loss 0.2781 (0.4955)	
training:	Epoch: [1][438/467]	Loss 0.5498 (0.4957)	
training:	Epoch: [1][439/467]	Loss 0.5847 (0.4959)	
training:	Epoch: [1][440/467]	Loss 0.2464 (0.4953)	
training:	Epoch: [1][441/467]	Loss 0.2486 (0.4947)	
training:	Epoch: [1][442/467]	Loss 0.3783 (0.4945)	
training:	Epoch: [1][443/467]	Loss 0.3892 (0.4942)	
training:	Epoch: [1][444/467]	Loss 0.5074 (0.4943)	
training:	Epoch: [1][445/467]	Loss 0.4388 (0.4941)	
training:	Epoch: [1][446/467]	Loss 0.4146 (0.4940)	
training:	Epoch: [1][447/467]	Loss 0.5388 (0.4941)	
training:	Epoch: [1][448/467]	Loss 0.4376 (0.4939)	
training:	Epoch: [1][449/467]	Loss 0.1666 (0.4932)	
training:	Epoch: [1][450/467]	Loss 0.3805 (0.4930)	
training:	Epoch: [1][451/467]	Loss 0.4501 (0.4929)	
training:	Epoch: [1][452/467]	Loss 0.3239 (0.4925)	
training:	Epoch: [1][453/467]	Loss 0.4631 (0.4924)	
training:	Epoch: [1][454/467]	Loss 0.2783 (0.4919)	
training:	Epoch: [1][455/467]	Loss 0.3495 (0.4916)	
training:	Epoch: [1][456/467]	Loss 0.2710 (0.4911)	
training:	Epoch: [1][457/467]	Loss 0.2069 (0.4905)	
training:	Epoch: [1][458/467]	Loss 0.2695 (0.4900)	
training:	Epoch: [1][459/467]	Loss 0.3591 (0.4898)	
training:	Epoch: [1][460/467]	Loss 0.4658 (0.4897)	
training:	Epoch: [1][461/467]	Loss 0.3826 (0.4895)	
training:	Epoch: [1][462/467]	Loss 0.2238 (0.4889)	
training:	Epoch: [1][463/467]	Loss 0.4566 (0.4888)	
training:	Epoch: [1][464/467]	Loss 0.3795 (0.4886)	
training:	Epoch: [1][465/467]	Loss 0.1675 (0.4879)	
training:	Epoch: [1][466/467]	Loss 0.5727 (0.4881)	
training:	Epoch: [1][467/467]	Loss 0.2189 (0.4875)	
Training:	 Loss: 0.4874

