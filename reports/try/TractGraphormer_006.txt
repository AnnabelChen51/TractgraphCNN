Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, epochs=200, tensorboard=True, net_architecture='TractGraphormer', batch_size=32, rate=1e-05, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20, fl=64, nh=1)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	1e-05
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	2

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/204]	Loss 0.7371 (0.7371)	
training:	Epoch: [1][2/204]	Loss 0.7143 (0.7257)	
training:	Epoch: [1][3/204]	Loss 0.7776 (0.7430)	
training:	Epoch: [1][4/204]	Loss 0.6874 (0.7291)	
training:	Epoch: [1][5/204]	Loss 0.6887 (0.7210)	
training:	Epoch: [1][6/204]	Loss 0.6817 (0.7145)	
training:	Epoch: [1][7/204]	Loss 0.6972 (0.7120)	
training:	Epoch: [1][8/204]	Loss 0.6711 (0.7069)	
training:	Epoch: [1][9/204]	Loss 0.6766 (0.7035)	
training:	Epoch: [1][10/204]	Loss 0.7067 (0.7038)	
training:	Epoch: [1][11/204]	Loss 0.6802 (0.7017)	
training:	Epoch: [1][12/204]	Loss 0.6807 (0.6999)	
training:	Epoch: [1][13/204]	Loss 0.6870 (0.6989)	
training:	Epoch: [1][14/204]	Loss 0.6947 (0.6986)	
training:	Epoch: [1][15/204]	Loss 0.7505 (0.7021)	
training:	Epoch: [1][16/204]	Loss 0.7182 (0.7031)	
training:	Epoch: [1][17/204]	Loss 0.7201 (0.7041)	
training:	Epoch: [1][18/204]	Loss 0.6856 (0.7031)	
training:	Epoch: [1][19/204]	Loss 0.7440 (0.7052)	
training:	Epoch: [1][20/204]	Loss 0.6919 (0.7046)	
training:	Epoch: [1][21/204]	Loss 0.7094 (0.7048)	
training:	Epoch: [1][22/204]	Loss 0.6852 (0.7039)	
training:	Epoch: [1][23/204]	Loss 0.7031 (0.7039)	
training:	Epoch: [1][24/204]	Loss 0.6941 (0.7035)	
training:	Epoch: [1][25/204]	Loss 0.6856 (0.7027)	
training:	Epoch: [1][26/204]	Loss 0.7175 (0.7033)	
training:	Epoch: [1][27/204]	Loss 0.6988 (0.7031)	
training:	Epoch: [1][28/204]	Loss 0.7216 (0.7038)	
training:	Epoch: [1][29/204]	Loss 0.6898 (0.7033)	
training:	Epoch: [1][30/204]	Loss 0.7098 (0.7035)	
training:	Epoch: [1][31/204]	Loss 0.6952 (0.7033)	
training:	Epoch: [1][32/204]	Loss 0.7217 (0.7038)	
training:	Epoch: [1][33/204]	Loss 0.7340 (0.7048)	
training:	Epoch: [1][34/204]	Loss 0.7507 (0.7061)	
training:	Epoch: [1][35/204]	Loss 0.7066 (0.7061)	
training:	Epoch: [1][36/204]	Loss 0.6886 (0.7056)	
training:	Epoch: [1][37/204]	Loss 0.7140 (0.7059)	
training:	Epoch: [1][38/204]	Loss 0.7126 (0.7060)	
training:	Epoch: [1][39/204]	Loss 0.6802 (0.7054)	
training:	Epoch: [1][40/204]	Loss 0.6879 (0.7049)	
training:	Epoch: [1][41/204]	Loss 0.7157 (0.7052)	
training:	Epoch: [1][42/204]	Loss 0.6876 (0.7048)	
training:	Epoch: [1][43/204]	Loss 0.7202 (0.7051)	
training:	Epoch: [1][44/204]	Loss 0.6754 (0.7045)	
training:	Epoch: [1][45/204]	Loss 0.6985 (0.7043)	
training:	Epoch: [1][46/204]	Loss 0.7236 (0.7048)	
training:	Epoch: [1][47/204]	Loss 0.6638 (0.7039)	
training:	Epoch: [1][48/204]	Loss 0.7046 (0.7039)	
training:	Epoch: [1][49/204]	Loss 0.7057 (0.7039)	
training:	Epoch: [1][50/204]	Loss 0.6710 (0.7033)	
training:	Epoch: [1][51/204]	Loss 0.7423 (0.7040)	
training:	Epoch: [1][52/204]	Loss 0.6976 (0.7039)	
training:	Epoch: [1][53/204]	Loss 0.6754 (0.7034)	
training:	Epoch: [1][54/204]	Loss 0.7011 (0.7033)	
training:	Epoch: [1][55/204]	Loss 0.7224 (0.7037)	
training:	Epoch: [1][56/204]	Loss 0.6575 (0.7029)	
training:	Epoch: [1][57/204]	Loss 0.7298 (0.7033)	
training:	Epoch: [1][58/204]	Loss 0.6882 (0.7031)	
training:	Epoch: [1][59/204]	Loss 0.7031 (0.7031)	
training:	Epoch: [1][60/204]	Loss 0.7041 (0.7031)	
training:	Epoch: [1][61/204]	Loss 0.6700 (0.7025)	
training:	Epoch: [1][62/204]	Loss 0.7155 (0.7028)	
training:	Epoch: [1][63/204]	Loss 0.6880 (0.7025)	
training:	Epoch: [1][64/204]	Loss 0.6806 (0.7022)	
training:	Epoch: [1][65/204]	Loss 0.6543 (0.7014)	
training:	Epoch: [1][66/204]	Loss 0.6773 (0.7011)	
training:	Epoch: [1][67/204]	Loss 0.6534 (0.7004)	
training:	Epoch: [1][68/204]	Loss 0.7329 (0.7008)	
training:	Epoch: [1][69/204]	Loss 0.6795 (0.7005)	
training:	Epoch: [1][70/204]	Loss 0.6749 (0.7002)	
training:	Epoch: [1][71/204]	Loss 0.6796 (0.6999)	
training:	Epoch: [1][72/204]	Loss 0.6989 (0.6999)	
training:	Epoch: [1][73/204]	Loss 0.6784 (0.6996)	
training:	Epoch: [1][74/204]	Loss 0.6761 (0.6992)	
training:	Epoch: [1][75/204]	Loss 0.7059 (0.6993)	
training:	Epoch: [1][76/204]	Loss 0.7176 (0.6996)	
training:	Epoch: [1][77/204]	Loss 0.6828 (0.6994)	
training:	Epoch: [1][78/204]	Loss 0.6977 (0.6993)	
training:	Epoch: [1][79/204]	Loss 0.6877 (0.6992)	
training:	Epoch: [1][80/204]	Loss 0.7155 (0.6994)	
training:	Epoch: [1][81/204]	Loss 0.6574 (0.6989)	
training:	Epoch: [1][82/204]	Loss 0.7002 (0.6989)	
training:	Epoch: [1][83/204]	Loss 0.6253 (0.6980)	
training:	Epoch: [1][84/204]	Loss 0.7209 (0.6983)	
training:	Epoch: [1][85/204]	Loss 0.6794 (0.6981)	
training:	Epoch: [1][86/204]	Loss 0.7154 (0.6983)	
training:	Epoch: [1][87/204]	Loss 0.7106 (0.6984)	
training:	Epoch: [1][88/204]	Loss 0.6969 (0.6984)	
training:	Epoch: [1][89/204]	Loss 0.6908 (0.6983)	
training:	Epoch: [1][90/204]	Loss 0.7429 (0.6988)	
training:	Epoch: [1][91/204]	Loss 0.6452 (0.6982)	
training:	Epoch: [1][92/204]	Loss 0.7102 (0.6983)	
training:	Epoch: [1][93/204]	Loss 0.6402 (0.6977)	
training:	Epoch: [1][94/204]	Loss 0.6817 (0.6975)	
training:	Epoch: [1][95/204]	Loss 0.6791 (0.6973)	
training:	Epoch: [1][96/204]	Loss 0.6771 (0.6971)	
training:	Epoch: [1][97/204]	Loss 0.6999 (0.6972)	
training:	Epoch: [1][98/204]	Loss 0.6966 (0.6972)	
training:	Epoch: [1][99/204]	Loss 0.6834 (0.6970)	
training:	Epoch: [1][100/204]	Loss 0.6688 (0.6967)	
training:	Epoch: [1][101/204]	Loss 0.6910 (0.6967)	
training:	Epoch: [1][102/204]	Loss 0.6746 (0.6965)	
training:	Epoch: [1][103/204]	Loss 0.7148 (0.6966)	
training:	Epoch: [1][104/204]	Loss 0.6717 (0.6964)	
training:	Epoch: [1][105/204]	Loss 0.6627 (0.6961)	
training:	Epoch: [1][106/204]	Loss 0.6921 (0.6960)	
training:	Epoch: [1][107/204]	Loss 0.7184 (0.6963)	
training:	Epoch: [1][108/204]	Loss 0.6860 (0.6962)	
training:	Epoch: [1][109/204]	Loss 0.6750 (0.6960)	
training:	Epoch: [1][110/204]	Loss 0.6675 (0.6957)	
training:	Epoch: [1][111/204]	Loss 0.7733 (0.6964)	
training:	Epoch: [1][112/204]	Loss 0.6479 (0.6960)	
training:	Epoch: [1][113/204]	Loss 0.6891 (0.6959)	
training:	Epoch: [1][114/204]	Loss 0.6422 (0.6954)	
training:	Epoch: [1][115/204]	Loss 0.7186 (0.6956)	
training:	Epoch: [1][116/204]	Loss 0.6835 (0.6955)	
training:	Epoch: [1][117/204]	Loss 0.6505 (0.6952)	
training:	Epoch: [1][118/204]	Loss 0.7015 (0.6952)	
training:	Epoch: [1][119/204]	Loss 0.6385 (0.6947)	
training:	Epoch: [1][120/204]	Loss 0.6557 (0.6944)	
training:	Epoch: [1][121/204]	Loss 0.6801 (0.6943)	
training:	Epoch: [1][122/204]	Loss 0.6503 (0.6939)	
training:	Epoch: [1][123/204]	Loss 0.7004 (0.6940)	
training:	Epoch: [1][124/204]	Loss 0.7014 (0.6940)	
training:	Epoch: [1][125/204]	Loss 0.7127 (0.6942)	
training:	Epoch: [1][126/204]	Loss 0.6751 (0.6940)	
training:	Epoch: [1][127/204]	Loss 0.7215 (0.6943)	
training:	Epoch: [1][128/204]	Loss 0.7189 (0.6944)	
training:	Epoch: [1][129/204]	Loss 0.6616 (0.6942)	
training:	Epoch: [1][130/204]	Loss 0.6871 (0.6941)	
training:	Epoch: [1][131/204]	Loss 0.7236 (0.6944)	
training:	Epoch: [1][132/204]	Loss 0.6894 (0.6943)	
training:	Epoch: [1][133/204]	Loss 0.6650 (0.6941)	
training:	Epoch: [1][134/204]	Loss 0.7096 (0.6942)	
training:	Epoch: [1][135/204]	Loss 0.6613 (0.6940)	
training:	Epoch: [1][136/204]	Loss 0.6828 (0.6939)	
training:	Epoch: [1][137/204]	Loss 0.6607 (0.6937)	
training:	Epoch: [1][138/204]	Loss 0.6644 (0.6934)	
training:	Epoch: [1][139/204]	Loss 0.7258 (0.6937)	
training:	Epoch: [1][140/204]	Loss 0.7001 (0.6937)	
training:	Epoch: [1][141/204]	Loss 0.6310 (0.6933)	
training:	Epoch: [1][142/204]	Loss 0.6844 (0.6932)	
training:	Epoch: [1][143/204]	Loss 0.6502 (0.6929)	
training:	Epoch: [1][144/204]	Loss 0.6676 (0.6927)	
training:	Epoch: [1][145/204]	Loss 0.6896 (0.6927)	
training:	Epoch: [1][146/204]	Loss 0.6769 (0.6926)	
training:	Epoch: [1][147/204]	Loss 0.7102 (0.6927)	
training:	Epoch: [1][148/204]	Loss 0.6901 (0.6927)	
training:	Epoch: [1][149/204]	Loss 0.6298 (0.6923)	
training:	Epoch: [1][150/204]	Loss 0.6753 (0.6922)	
training:	Epoch: [1][151/204]	Loss 0.6934 (0.6922)	
training:	Epoch: [1][152/204]	Loss 0.6868 (0.6921)	
training:	Epoch: [1][153/204]	Loss 0.7069 (0.6922)	
training:	Epoch: [1][154/204]	Loss 0.7064 (0.6923)	
training:	Epoch: [1][155/204]	Loss 0.6752 (0.6922)	
training:	Epoch: [1][156/204]	Loss 0.6868 (0.6922)	
training:	Epoch: [1][157/204]	Loss 0.6775 (0.6921)	
training:	Epoch: [1][158/204]	Loss 0.6983 (0.6921)	
training:	Epoch: [1][159/204]	Loss 0.7146 (0.6923)	
training:	Epoch: [1][160/204]	Loss 0.6873 (0.6922)	
training:	Epoch: [1][161/204]	Loss 0.6527 (0.6920)	
training:	Epoch: [1][162/204]	Loss 0.7076 (0.6921)	
training:	Epoch: [1][163/204]	Loss 0.6534 (0.6919)	
training:	Epoch: [1][164/204]	Loss 0.6488 (0.6916)	
training:	Epoch: [1][165/204]	Loss 0.6601 (0.6914)	
training:	Epoch: [1][166/204]	Loss 0.7028 (0.6915)	
training:	Epoch: [1][167/204]	Loss 0.6824 (0.6914)	
training:	Epoch: [1][168/204]	Loss 0.6431 (0.6911)	
training:	Epoch: [1][169/204]	Loss 0.6836 (0.6911)	
training:	Epoch: [1][170/204]	Loss 0.6762 (0.6910)	
training:	Epoch: [1][171/204]	Loss 0.6937 (0.6910)	
training:	Epoch: [1][172/204]	Loss 0.6919 (0.6910)	
training:	Epoch: [1][173/204]	Loss 0.6382 (0.6907)	
training:	Epoch: [1][174/204]	Loss 0.6251 (0.6903)	
training:	Epoch: [1][175/204]	Loss 0.6943 (0.6904)	
training:	Epoch: [1][176/204]	Loss 0.6754 (0.6903)	
training:	Epoch: [1][177/204]	Loss 0.7058 (0.6904)	
training:	Epoch: [1][178/204]	Loss 0.6928 (0.6904)	
training:	Epoch: [1][179/204]	Loss 0.6440 (0.6901)	
training:	Epoch: [1][180/204]	Loss 0.6904 (0.6901)	
training:	Epoch: [1][181/204]	Loss 0.6634 (0.6900)	
training:	Epoch: [1][182/204]	Loss 0.6739 (0.6899)	
training:	Epoch: [1][183/204]	Loss 0.6561 (0.6897)	
training:	Epoch: [1][184/204]	Loss 0.6725 (0.6896)	
training:	Epoch: [1][185/204]	Loss 0.7371 (0.6899)	
training:	Epoch: [1][186/204]	Loss 0.6530 (0.6897)	
training:	Epoch: [1][187/204]	Loss 0.6743 (0.6896)	
training:	Epoch: [1][188/204]	Loss 0.6590 (0.6894)	
training:	Epoch: [1][189/204]	Loss 0.6773 (0.6894)	
training:	Epoch: [1][190/204]	Loss 0.7552 (0.6897)	
training:	Epoch: [1][191/204]	Loss 0.6865 (0.6897)	
training:	Epoch: [1][192/204]	Loss 0.6652 (0.6896)	
training:	Epoch: [1][193/204]	Loss 0.6780 (0.6895)	
training:	Epoch: [1][194/204]	Loss 0.6817 (0.6895)	
training:	Epoch: [1][195/204]	Loss 0.6852 (0.6894)	
training:	Epoch: [1][196/204]	Loss 0.7213 (0.6896)	
training:	Epoch: [1][197/204]	Loss 0.6867 (0.6896)	
training:	Epoch: [1][198/204]	Loss 0.6798 (0.6895)	
training:	Epoch: [1][199/204]	Loss 0.6541 (0.6894)	
training:	Epoch: [1][200/204]	Loss 0.6598 (0.6892)	
training:	Epoch: [1][201/204]	Loss 0.6900 (0.6892)	
training:	Epoch: [1][202/204]	Loss 0.6719 (0.6891)	
training:	Epoch: [1][203/204]	Loss 0.6467 (0.6889)	
training:	Epoch: [1][204/204]	Loss 0.6624 (0.6888)	
Training:	 Loss: 0.6877

Training:	 ACC: 0.5822 0.5863 0.6817 0.4828
Validation:	 ACC: 0.5781 0.5832 0.6899 0.4664
Validation:	 Best_BACC: 0.5781 0.5832 0.6899 0.4664
Validation:	 Loss: 0.6743
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/204]	Loss 0.6249 (0.6249)	
training:	Epoch: [2][2/204]	Loss 0.6476 (0.6363)	
training:	Epoch: [2][3/204]	Loss 0.6355 (0.6360)	
training:	Epoch: [2][4/204]	Loss 0.7008 (0.6522)	
training:	Epoch: [2][5/204]	Loss 0.6480 (0.6514)	
training:	Epoch: [2][6/204]	Loss 0.6650 (0.6536)	
training:	Epoch: [2][7/204]	Loss 0.6887 (0.6586)	
training:	Epoch: [2][8/204]	Loss 0.6743 (0.6606)	
training:	Epoch: [2][9/204]	Loss 0.6414 (0.6585)	
training:	Epoch: [2][10/204]	Loss 0.6809 (0.6607)	
training:	Epoch: [2][11/204]	Loss 0.6624 (0.6609)	
training:	Epoch: [2][12/204]	Loss 0.7210 (0.6659)	
training:	Epoch: [2][13/204]	Loss 0.6803 (0.6670)	
training:	Epoch: [2][14/204]	Loss 0.6281 (0.6642)	
training:	Epoch: [2][15/204]	Loss 0.7344 (0.6689)	
training:	Epoch: [2][16/204]	Loss 0.6523 (0.6678)	
training:	Epoch: [2][17/204]	Loss 0.6391 (0.6661)	
training:	Epoch: [2][18/204]	Loss 0.6792 (0.6669)	
training:	Epoch: [2][19/204]	Loss 0.6879 (0.6680)	
training:	Epoch: [2][20/204]	Loss 0.6561 (0.6674)	
training:	Epoch: [2][21/204]	Loss 0.6857 (0.6683)	
training:	Epoch: [2][22/204]	Loss 0.6910 (0.6693)	
training:	Epoch: [2][23/204]	Loss 0.6981 (0.6705)	
training:	Epoch: [2][24/204]	Loss 0.7289 (0.6730)	
training:	Epoch: [2][25/204]	Loss 0.6397 (0.6716)	
training:	Epoch: [2][26/204]	Loss 0.6739 (0.6717)	
training:	Epoch: [2][27/204]	Loss 0.6145 (0.6696)	
training:	Epoch: [2][28/204]	Loss 0.6439 (0.6687)	
training:	Epoch: [2][29/204]	Loss 0.6713 (0.6688)	
training:	Epoch: [2][30/204]	Loss 0.6142 (0.6670)	
training:	Epoch: [2][31/204]	Loss 0.6687 (0.6670)	
training:	Epoch: [2][32/204]	Loss 0.6625 (0.6669)	
training:	Epoch: [2][33/204]	Loss 0.6732 (0.6671)	
training:	Epoch: [2][34/204]	Loss 0.7027 (0.6681)	
training:	Epoch: [2][35/204]	Loss 0.6894 (0.6687)	
training:	Epoch: [2][36/204]	Loss 0.6493 (0.6682)	
training:	Epoch: [2][37/204]	Loss 0.6621 (0.6680)	
training:	Epoch: [2][38/204]	Loss 0.6790 (0.6683)	
training:	Epoch: [2][39/204]	Loss 0.7044 (0.6692)	
training:	Epoch: [2][40/204]	Loss 0.6914 (0.6698)	
training:	Epoch: [2][41/204]	Loss 0.6571 (0.6695)	
training:	Epoch: [2][42/204]	Loss 0.6656 (0.6694)	
training:	Epoch: [2][43/204]	Loss 0.6878 (0.6698)	
training:	Epoch: [2][44/204]	Loss 0.6741 (0.6699)	
training:	Epoch: [2][45/204]	Loss 0.6691 (0.6699)	
training:	Epoch: [2][46/204]	Loss 0.6984 (0.6705)	
training:	Epoch: [2][47/204]	Loss 0.6649 (0.6704)	
training:	Epoch: [2][48/204]	Loss 0.6664 (0.6703)	
training:	Epoch: [2][49/204]	Loss 0.7067 (0.6711)	
training:	Epoch: [2][50/204]	Loss 0.7398 (0.6724)	
training:	Epoch: [2][51/204]	Loss 0.7070 (0.6731)	
training:	Epoch: [2][52/204]	Loss 0.6631 (0.6729)	
training:	Epoch: [2][53/204]	Loss 0.6927 (0.6733)	
training:	Epoch: [2][54/204]	Loss 0.7044 (0.6739)	
training:	Epoch: [2][55/204]	Loss 0.6460 (0.6734)	
training:	Epoch: [2][56/204]	Loss 0.6702 (0.6733)	
training:	Epoch: [2][57/204]	Loss 0.6469 (0.6728)	
training:	Epoch: [2][58/204]	Loss 0.7067 (0.6734)	
training:	Epoch: [2][59/204]	Loss 0.6710 (0.6734)	
training:	Epoch: [2][60/204]	Loss 0.6425 (0.6729)	
training:	Epoch: [2][61/204]	Loss 0.6978 (0.6733)	
training:	Epoch: [2][62/204]	Loss 0.6827 (0.6734)	
training:	Epoch: [2][63/204]	Loss 0.7271 (0.6743)	
training:	Epoch: [2][64/204]	Loss 0.6561 (0.6740)	
training:	Epoch: [2][65/204]	Loss 0.6414 (0.6735)	
training:	Epoch: [2][66/204]	Loss 0.6653 (0.6734)	
training:	Epoch: [2][67/204]	Loss 0.6823 (0.6735)	
training:	Epoch: [2][68/204]	Loss 0.7109 (0.6741)	
training:	Epoch: [2][69/204]	Loss 0.6010 (0.6730)	
training:	Epoch: [2][70/204]	Loss 0.6836 (0.6731)	
training:	Epoch: [2][71/204]	Loss 0.6574 (0.6729)	
training:	Epoch: [2][72/204]	Loss 0.7502 (0.6740)	
training:	Epoch: [2][73/204]	Loss 0.6644 (0.6739)	
training:	Epoch: [2][74/204]	Loss 0.6990 (0.6742)	
training:	Epoch: [2][75/204]	Loss 0.6543 (0.6739)	
training:	Epoch: [2][76/204]	Loss 0.6588 (0.6737)	
training:	Epoch: [2][77/204]	Loss 0.6994 (0.6741)	
training:	Epoch: [2][78/204]	Loss 0.6964 (0.6744)	
training:	Epoch: [2][79/204]	Loss 0.6578 (0.6741)	
training:	Epoch: [2][80/204]	Loss 0.6142 (0.6734)	
training:	Epoch: [2][81/204]	Loss 0.6908 (0.6736)	
training:	Epoch: [2][82/204]	Loss 0.7185 (0.6742)	
training:	Epoch: [2][83/204]	Loss 0.6933 (0.6744)	
training:	Epoch: [2][84/204]	Loss 0.6788 (0.6744)	
training:	Epoch: [2][85/204]	Loss 0.6394 (0.6740)	
training:	Epoch: [2][86/204]	Loss 0.7076 (0.6744)	
training:	Epoch: [2][87/204]	Loss 0.6471 (0.6741)	
training:	Epoch: [2][88/204]	Loss 0.6552 (0.6739)	
training:	Epoch: [2][89/204]	Loss 0.6696 (0.6738)	
training:	Epoch: [2][90/204]	Loss 0.7113 (0.6743)	
training:	Epoch: [2][91/204]	Loss 0.6540 (0.6740)	
training:	Epoch: [2][92/204]	Loss 0.6678 (0.6740)	
training:	Epoch: [2][93/204]	Loss 0.6114 (0.6733)	
training:	Epoch: [2][94/204]	Loss 0.6950 (0.6735)	
training:	Epoch: [2][95/204]	Loss 0.6660 (0.6734)	
training:	Epoch: [2][96/204]	Loss 0.6533 (0.6732)	
training:	Epoch: [2][97/204]	Loss 0.6597 (0.6731)	
training:	Epoch: [2][98/204]	Loss 0.6690 (0.6731)	
training:	Epoch: [2][99/204]	Loss 0.7317 (0.6737)	
training:	Epoch: [2][100/204]	Loss 0.7025 (0.6739)	
training:	Epoch: [2][101/204]	Loss 0.6508 (0.6737)	
training:	Epoch: [2][102/204]	Loss 0.6784 (0.6738)	
training:	Epoch: [2][103/204]	Loss 0.6883 (0.6739)	
training:	Epoch: [2][104/204]	Loss 0.6346 (0.6735)	
training:	Epoch: [2][105/204]	Loss 0.6784 (0.6736)	
training:	Epoch: [2][106/204]	Loss 0.6416 (0.6733)	
training:	Epoch: [2][107/204]	Loss 0.6818 (0.6733)	
training:	Epoch: [2][108/204]	Loss 0.7015 (0.6736)	
training:	Epoch: [2][109/204]	Loss 0.6359 (0.6733)	
training:	Epoch: [2][110/204]	Loss 0.6290 (0.6729)	
training:	Epoch: [2][111/204]	Loss 0.6453 (0.6726)	
training:	Epoch: [2][112/204]	Loss 0.6330 (0.6723)	
training:	Epoch: [2][113/204]	Loss 0.6493 (0.6721)	
training:	Epoch: [2][114/204]	Loss 0.6074 (0.6715)	
training:	Epoch: [2][115/204]	Loss 0.7168 (0.6719)	
training:	Epoch: [2][116/204]	Loss 0.7323 (0.6724)	
training:	Epoch: [2][117/204]	Loss 0.6668 (0.6724)	
training:	Epoch: [2][118/204]	Loss 0.6866 (0.6725)	
training:	Epoch: [2][119/204]	Loss 0.6349 (0.6722)	
training:	Epoch: [2][120/204]	Loss 0.6776 (0.6722)	
training:	Epoch: [2][121/204]	Loss 0.7273 (0.6727)	
training:	Epoch: [2][122/204]	Loss 0.6701 (0.6726)	
training:	Epoch: [2][123/204]	Loss 0.6590 (0.6725)	
training:	Epoch: [2][124/204]	Loss 0.6801 (0.6726)	
training:	Epoch: [2][125/204]	Loss 0.6773 (0.6726)	
training:	Epoch: [2][126/204]	Loss 0.6337 (0.6723)	
training:	Epoch: [2][127/204]	Loss 0.6655 (0.6723)	
training:	Epoch: [2][128/204]	Loss 0.6385 (0.6720)	
training:	Epoch: [2][129/204]	Loss 0.6603 (0.6719)	
training:	Epoch: [2][130/204]	Loss 0.6690 (0.6719)	
training:	Epoch: [2][131/204]	Loss 0.6906 (0.6720)	
training:	Epoch: [2][132/204]	Loss 0.6632 (0.6720)	
training:	Epoch: [2][133/204]	Loss 0.6383 (0.6717)	
training:	Epoch: [2][134/204]	Loss 0.5904 (0.6711)	
training:	Epoch: [2][135/204]	Loss 0.6739 (0.6711)	
training:	Epoch: [2][136/204]	Loss 0.6036 (0.6706)	
training:	Epoch: [2][137/204]	Loss 0.6704 (0.6706)	
training:	Epoch: [2][138/204]	Loss 0.6961 (0.6708)	
training:	Epoch: [2][139/204]	Loss 0.6517 (0.6707)	
training:	Epoch: [2][140/204]	Loss 0.6420 (0.6705)	
training:	Epoch: [2][141/204]	Loss 0.6467 (0.6703)	
training:	Epoch: [2][142/204]	Loss 0.6478 (0.6701)	
training:	Epoch: [2][143/204]	Loss 0.7137 (0.6704)	
training:	Epoch: [2][144/204]	Loss 0.7097 (0.6707)	
training:	Epoch: [2][145/204]	Loss 0.6506 (0.6706)	
training:	Epoch: [2][146/204]	Loss 0.6490 (0.6704)	
training:	Epoch: [2][147/204]	Loss 0.6674 (0.6704)	
training:	Epoch: [2][148/204]	Loss 0.6622 (0.6704)	
training:	Epoch: [2][149/204]	Loss 0.6445 (0.6702)	
training:	Epoch: [2][150/204]	Loss 0.6394 (0.6700)	
training:	Epoch: [2][151/204]	Loss 0.6567 (0.6699)	
training:	Epoch: [2][152/204]	Loss 0.6997 (0.6701)	
training:	Epoch: [2][153/204]	Loss 0.7094 (0.6703)	
training:	Epoch: [2][154/204]	Loss 0.6550 (0.6702)	
training:	Epoch: [2][155/204]	Loss 0.7118 (0.6705)	
training:	Epoch: [2][156/204]	Loss 0.7060 (0.6707)	
training:	Epoch: [2][157/204]	Loss 0.6821 (0.6708)	
training:	Epoch: [2][158/204]	Loss 0.6304 (0.6706)	
training:	Epoch: [2][159/204]	Loss 0.7022 (0.6708)	
training:	Epoch: [2][160/204]	Loss 0.5856 (0.6702)	
training:	Epoch: [2][161/204]	Loss 0.6867 (0.6703)	
training:	Epoch: [2][162/204]	Loss 0.6520 (0.6702)	
training:	Epoch: [2][163/204]	Loss 0.6643 (0.6702)	
training:	Epoch: [2][164/204]	Loss 0.6865 (0.6703)	
training:	Epoch: [2][165/204]	Loss 0.6555 (0.6702)	
training:	Epoch: [2][166/204]	Loss 0.6426 (0.6700)	
training:	Epoch: [2][167/204]	Loss 0.5988 (0.6696)	
training:	Epoch: [2][168/204]	Loss 0.6643 (0.6696)	
training:	Epoch: [2][169/204]	Loss 0.6549 (0.6695)	
training:	Epoch: [2][170/204]	Loss 0.6453 (0.6693)	
training:	Epoch: [2][171/204]	Loss 0.7150 (0.6696)	
training:	Epoch: [2][172/204]	Loss 0.5923 (0.6691)	
training:	Epoch: [2][173/204]	Loss 0.6316 (0.6689)	
training:	Epoch: [2][174/204]	Loss 0.6560 (0.6689)	
training:	Epoch: [2][175/204]	Loss 0.6446 (0.6687)	
training:	Epoch: [2][176/204]	Loss 0.6883 (0.6688)	
training:	Epoch: [2][177/204]	Loss 0.6627 (0.6688)	
training:	Epoch: [2][178/204]	Loss 0.6694 (0.6688)	
training:	Epoch: [2][179/204]	Loss 0.6475 (0.6687)	
training:	Epoch: [2][180/204]	Loss 0.6820 (0.6688)	
training:	Epoch: [2][181/204]	Loss 0.6650 (0.6687)	
training:	Epoch: [2][182/204]	Loss 0.5984 (0.6683)	
training:	Epoch: [2][183/204]	Loss 0.6753 (0.6684)	
training:	Epoch: [2][184/204]	Loss 0.6995 (0.6686)	
training:	Epoch: [2][185/204]	Loss 0.6879 (0.6687)	
training:	Epoch: [2][186/204]	Loss 0.6421 (0.6685)	
training:	Epoch: [2][187/204]	Loss 0.6666 (0.6685)	
training:	Epoch: [2][188/204]	Loss 0.6420 (0.6684)	
training:	Epoch: [2][189/204]	Loss 0.6688 (0.6684)	
training:	Epoch: [2][190/204]	Loss 0.7346 (0.6687)	
training:	Epoch: [2][191/204]	Loss 0.6012 (0.6684)	
training:	Epoch: [2][192/204]	Loss 0.6673 (0.6684)	
training:	Epoch: [2][193/204]	Loss 0.6620 (0.6683)	
training:	Epoch: [2][194/204]	Loss 0.6819 (0.6684)	
training:	Epoch: [2][195/204]	Loss 0.6177 (0.6681)	
training:	Epoch: [2][196/204]	Loss 0.6470 (0.6680)	
training:	Epoch: [2][197/204]	Loss 0.6256 (0.6678)	
training:	Epoch: [2][198/204]	Loss 0.6449 (0.6677)	
training:	Epoch: [2][199/204]	Loss 0.7042 (0.6679)	
training:	Epoch: [2][200/204]	Loss 0.6872 (0.6680)	
training:	Epoch: [2][201/204]	Loss 0.6504 (0.6679)	
training:	Epoch: [2][202/204]	Loss 0.6317 (0.6677)	
training:	Epoch: [2][203/204]	Loss 0.6508 (0.6676)	
training:	Epoch: [2][204/204]	Loss 0.6951 (0.6678)	
Training:	 Loss: 0.6667

