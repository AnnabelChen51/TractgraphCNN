Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=400, tensorboard=True, net_architecture='TractGraphormer', batch_size=16, rate=0.0001, weight=0.0, sched_step=200, sched_gamma=0.1, printing_frequency=1, seed=0, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	16
Number of workers:	0
Learning rate:	0.0001
Weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Number of epochs of training:	400
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/400
----------
training:	Epoch: [1][1/408]	Loss 0.7767 (0.7767)	
training:	Epoch: [1][2/408]	Loss 0.7536 (0.7651)	
training:	Epoch: [1][3/408]	Loss 0.6843 (0.7382)	
training:	Epoch: [1][4/408]	Loss 0.7277 (0.7356)	
training:	Epoch: [1][5/408]	Loss 0.7325 (0.7350)	
training:	Epoch: [1][6/408]	Loss 0.6992 (0.7290)	
training:	Epoch: [1][7/408]	Loss 0.7173 (0.7273)	
training:	Epoch: [1][8/408]	Loss 0.6766 (0.7210)	
training:	Epoch: [1][9/408]	Loss 0.6909 (0.7176)	
training:	Epoch: [1][10/408]	Loss 0.6618 (0.7121)	
training:	Epoch: [1][11/408]	Loss 0.7252 (0.7133)	
training:	Epoch: [1][12/408]	Loss 0.6824 (0.7107)	
training:	Epoch: [1][13/408]	Loss 0.7090 (0.7106)	
training:	Epoch: [1][14/408]	Loss 0.6634 (0.7072)	
training:	Epoch: [1][15/408]	Loss 0.6796 (0.7054)	
training:	Epoch: [1][16/408]	Loss 0.7118 (0.7058)	
training:	Epoch: [1][17/408]	Loss 0.6710 (0.7037)	
training:	Epoch: [1][18/408]	Loss 0.6950 (0.7032)	
training:	Epoch: [1][19/408]	Loss 0.6561 (0.7007)	
training:	Epoch: [1][20/408]	Loss 0.6734 (0.6994)	
training:	Epoch: [1][21/408]	Loss 0.6706 (0.6980)	
training:	Epoch: [1][22/408]	Loss 0.7356 (0.6997)	
training:	Epoch: [1][23/408]	Loss 0.6602 (0.6980)	
training:	Epoch: [1][24/408]	Loss 0.6047 (0.6941)	
training:	Epoch: [1][25/408]	Loss 0.6622 (0.6928)	
training:	Epoch: [1][26/408]	Loss 0.7310 (0.6943)	
training:	Epoch: [1][27/408]	Loss 0.6893 (0.6941)	
training:	Epoch: [1][28/408]	Loss 0.5883 (0.6903)	
training:	Epoch: [1][29/408]	Loss 0.7432 (0.6922)	
training:	Epoch: [1][30/408]	Loss 0.6153 (0.6896)	
training:	Epoch: [1][31/408]	Loss 0.6535 (0.6884)	
training:	Epoch: [1][32/408]	Loss 0.7381 (0.6900)	
training:	Epoch: [1][33/408]	Loss 0.7128 (0.6907)	
training:	Epoch: [1][34/408]	Loss 0.6889 (0.6906)	
training:	Epoch: [1][35/408]	Loss 0.6071 (0.6882)	
training:	Epoch: [1][36/408]	Loss 0.6548 (0.6873)	
training:	Epoch: [1][37/408]	Loss 0.7231 (0.6883)	
training:	Epoch: [1][38/408]	Loss 0.6004 (0.6860)	
training:	Epoch: [1][39/408]	Loss 0.6573 (0.6852)	
training:	Epoch: [1][40/408]	Loss 0.6876 (0.6853)	
training:	Epoch: [1][41/408]	Loss 0.6568 (0.6846)	
training:	Epoch: [1][42/408]	Loss 0.6640 (0.6841)	
training:	Epoch: [1][43/408]	Loss 0.6412 (0.6831)	
training:	Epoch: [1][44/408]	Loss 0.6244 (0.6818)	
training:	Epoch: [1][45/408]	Loss 0.6185 (0.6804)	
training:	Epoch: [1][46/408]	Loss 0.5459 (0.6774)	
training:	Epoch: [1][47/408]	Loss 0.6397 (0.6766)	
training:	Epoch: [1][48/408]	Loss 0.7024 (0.6772)	
training:	Epoch: [1][49/408]	Loss 0.6266 (0.6761)	
training:	Epoch: [1][50/408]	Loss 0.5928 (0.6745)	
training:	Epoch: [1][51/408]	Loss 0.6141 (0.6733)	
training:	Epoch: [1][52/408]	Loss 0.5669 (0.6712)	
training:	Epoch: [1][53/408]	Loss 0.7237 (0.6722)	
training:	Epoch: [1][54/408]	Loss 0.6200 (0.6713)	
training:	Epoch: [1][55/408]	Loss 0.5201 (0.6685)	
training:	Epoch: [1][56/408]	Loss 0.5874 (0.6671)	
training:	Epoch: [1][57/408]	Loss 0.6715 (0.6672)	
training:	Epoch: [1][58/408]	Loss 0.6872 (0.6675)	
training:	Epoch: [1][59/408]	Loss 0.6592 (0.6674)	
training:	Epoch: [1][60/408]	Loss 0.7202 (0.6682)	
training:	Epoch: [1][61/408]	Loss 0.6568 (0.6680)	
training:	Epoch: [1][62/408]	Loss 0.5517 (0.6662)	
training:	Epoch: [1][63/408]	Loss 0.6751 (0.6663)	
training:	Epoch: [1][64/408]	Loss 0.5083 (0.6638)	
training:	Epoch: [1][65/408]	Loss 0.6176 (0.6631)	
training:	Epoch: [1][66/408]	Loss 0.6161 (0.6624)	
training:	Epoch: [1][67/408]	Loss 0.6525 (0.6623)	
training:	Epoch: [1][68/408]	Loss 0.5263 (0.6603)	
training:	Epoch: [1][69/408]	Loss 0.6153 (0.6596)	
training:	Epoch: [1][70/408]	Loss 0.5835 (0.6585)	
training:	Epoch: [1][71/408]	Loss 0.5975 (0.6577)	
training:	Epoch: [1][72/408]	Loss 0.6004 (0.6569)	
training:	Epoch: [1][73/408]	Loss 0.5038 (0.6548)	
training:	Epoch: [1][74/408]	Loss 0.6751 (0.6551)	
training:	Epoch: [1][75/408]	Loss 0.5911 (0.6542)	
training:	Epoch: [1][76/408]	Loss 0.6052 (0.6536)	
training:	Epoch: [1][77/408]	Loss 0.7676 (0.6550)	
training:	Epoch: [1][78/408]	Loss 0.5157 (0.6533)	
training:	Epoch: [1][79/408]	Loss 0.6188 (0.6528)	
training:	Epoch: [1][80/408]	Loss 0.4286 (0.6500)	
training:	Epoch: [1][81/408]	Loss 0.5469 (0.6487)	
training:	Epoch: [1][82/408]	Loss 0.6333 (0.6486)	
training:	Epoch: [1][83/408]	Loss 0.9317 (0.6520)	
training:	Epoch: [1][84/408]	Loss 0.4579 (0.6497)	
training:	Epoch: [1][85/408]	Loss 0.6634 (0.6498)	
training:	Epoch: [1][86/408]	Loss 0.4319 (0.6473)	
training:	Epoch: [1][87/408]	Loss 0.7607 (0.6486)	
training:	Epoch: [1][88/408]	Loss 0.5356 (0.6473)	
training:	Epoch: [1][89/408]	Loss 0.5919 (0.6467)	
training:	Epoch: [1][90/408]	Loss 0.8407 (0.6488)	
training:	Epoch: [1][91/408]	Loss 0.6752 (0.6491)	
training:	Epoch: [1][92/408]	Loss 0.7116 (0.6498)	
training:	Epoch: [1][93/408]	Loss 0.6563 (0.6499)	
training:	Epoch: [1][94/408]	Loss 0.6745 (0.6501)	
training:	Epoch: [1][95/408]	Loss 0.8198 (0.6519)	
