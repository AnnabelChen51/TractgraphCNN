Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=200, tensorboard=True, net_architecture='DGCNNG', batch_size=16, rate=0.0001, weight=0.0, sched_step=100, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20)
Training the 'DGCNNG' architecture

The following parameters are used:
Batch size:	16
Number of workers:	0
Learning rate:	0.0001
Weight decay:	0.0
Scheduler steps:	100
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	7473
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/467]	Loss 0.7724 (0.7724)	
training:	Epoch: [1][2/467]	Loss 0.8707 (0.8216)	
training:	Epoch: [1][3/467]	Loss 0.7860 (0.8097)	
training:	Epoch: [1][4/467]	Loss 0.5540 (0.7458)	
training:	Epoch: [1][5/467]	Loss 1.0711 (0.8108)	
training:	Epoch: [1][6/467]	Loss 0.6916 (0.7910)	
training:	Epoch: [1][7/467]	Loss 0.7825 (0.7898)	
training:	Epoch: [1][8/467]	Loss 0.9993 (0.8160)	
training:	Epoch: [1][9/467]	Loss 0.5480 (0.7862)	
training:	Epoch: [1][10/467]	Loss 0.7444 (0.7820)	
training:	Epoch: [1][11/467]	Loss 0.6756 (0.7723)	
training:	Epoch: [1][12/467]	Loss 0.8413 (0.7781)	
training:	Epoch: [1][13/467]	Loss 0.2671 (0.7388)	
training:	Epoch: [1][14/467]	Loss 0.4177 (0.7158)	
training:	Epoch: [1][15/467]	Loss 0.8976 (0.7280)	
training:	Epoch: [1][16/467]	Loss 0.5944 (0.7196)	
training:	Epoch: [1][17/467]	Loss 0.8255 (0.7258)	
training:	Epoch: [1][18/467]	Loss 0.5904 (0.7183)	
training:	Epoch: [1][19/467]	Loss 0.7947 (0.7223)	
training:	Epoch: [1][20/467]	Loss 0.6625 (0.7193)	
training:	Epoch: [1][21/467]	Loss 0.4670 (0.7073)	
training:	Epoch: [1][22/467]	Loss 1.0893 (0.7247)	
training:	Epoch: [1][23/467]	Loss 0.8417 (0.7298)	
training:	Epoch: [1][24/467]	Loss 0.6690 (0.7272)	
training:	Epoch: [1][25/467]	Loss 0.5895 (0.7217)	
training:	Epoch: [1][26/467]	Loss 0.8448 (0.7265)	
training:	Epoch: [1][27/467]	Loss 0.6423 (0.7234)	
training:	Epoch: [1][28/467]	Loss 0.4583 (0.7139)	
training:	Epoch: [1][29/467]	Loss 0.8561 (0.7188)	
training:	Epoch: [1][30/467]	Loss 0.7052 (0.7183)	
training:	Epoch: [1][31/467]	Loss 0.5679 (0.7135)	
training:	Epoch: [1][32/467]	Loss 0.5984 (0.7099)	
training:	Epoch: [1][33/467]	Loss 0.7609 (0.7114)	
training:	Epoch: [1][34/467]	Loss 0.8375 (0.7151)	
training:	Epoch: [1][35/467]	Loss 0.5329 (0.7099)	
training:	Epoch: [1][36/467]	Loss 0.7794 (0.7119)	
training:	Epoch: [1][37/467]	Loss 0.6874 (0.7112)	
training:	Epoch: [1][38/467]	Loss 0.6366 (0.7092)	
training:	Epoch: [1][39/467]	Loss 0.5979 (0.7064)	
training:	Epoch: [1][40/467]	Loss 0.7540 (0.7076)	
training:	Epoch: [1][41/467]	Loss 0.5980 (0.7049)	
training:	Epoch: [1][42/467]	Loss 0.7358 (0.7056)	
training:	Epoch: [1][43/467]	Loss 0.5231 (0.7014)	
training:	Epoch: [1][44/467]	Loss 0.6802 (0.7009)	
training:	Epoch: [1][45/467]	Loss 0.6156 (0.6990)	
training:	Epoch: [1][46/467]	Loss 0.6159 (0.6972)	
training:	Epoch: [1][47/467]	Loss 0.6875 (0.6970)	
training:	Epoch: [1][48/467]	Loss 0.5821 (0.6946)	
training:	Epoch: [1][49/467]	Loss 0.5292 (0.6912)	
training:	Epoch: [1][50/467]	Loss 0.5946 (0.6893)	
training:	Epoch: [1][51/467]	Loss 0.4339 (0.6843)	
training:	Epoch: [1][52/467]	Loss 0.4718 (0.6802)	
training:	Epoch: [1][53/467]	Loss 0.4697 (0.6762)	
training:	Epoch: [1][54/467]	Loss 0.4513 (0.6721)	
training:	Epoch: [1][55/467]	Loss 0.5045 (0.6690)	
training:	Epoch: [1][56/467]	Loss 0.5563 (0.6670)	
training:	Epoch: [1][57/467]	Loss 0.5400 (0.6648)	
training:	Epoch: [1][58/467]	Loss 0.7663 (0.6665)	
training:	Epoch: [1][59/467]	Loss 0.5271 (0.6642)	
training:	Epoch: [1][60/467]	Loss 0.7172 (0.6651)	
training:	Epoch: [1][61/467]	Loss 0.6146 (0.6642)	
training:	Epoch: [1][62/467]	Loss 0.6697 (0.6643)	
training:	Epoch: [1][63/467]	Loss 0.5206 (0.6620)	
training:	Epoch: [1][64/467]	Loss 0.6396 (0.6617)	
training:	Epoch: [1][65/467]	Loss 0.5967 (0.6607)	
training:	Epoch: [1][66/467]	Loss 0.6851 (0.6611)	
training:	Epoch: [1][67/467]	Loss 0.5082 (0.6588)	
training:	Epoch: [1][68/467]	Loss 0.5097 (0.6566)	
training:	Epoch: [1][69/467]	Loss 0.6243 (0.6561)	
training:	Epoch: [1][70/467]	Loss 0.5196 (0.6542)	
training:	Epoch: [1][71/467]	Loss 0.7107 (0.6550)	
training:	Epoch: [1][72/467]	Loss 0.6193 (0.6545)	
training:	Epoch: [1][73/467]	Loss 0.4395 (0.6515)	
training:	Epoch: [1][74/467]	Loss 0.3477 (0.6474)	
training:	Epoch: [1][75/467]	Loss 0.4745 (0.6451)	
training:	Epoch: [1][76/467]	Loss 0.4889 (0.6431)	
training:	Epoch: [1][77/467]	Loss 0.5153 (0.6414)	
training:	Epoch: [1][78/467]	Loss 0.6871 (0.6420)	
training:	Epoch: [1][79/467]	Loss 0.3633 (0.6384)	
training:	Epoch: [1][80/467]	Loss 0.4063 (0.6355)	
training:	Epoch: [1][81/467]	Loss 0.6921 (0.6362)	
training:	Epoch: [1][82/467]	Loss 0.4355 (0.6338)	
training:	Epoch: [1][83/467]	Loss 0.4340 (0.6314)	
training:	Epoch: [1][84/467]	Loss 0.6338 (0.6314)	
training:	Epoch: [1][85/467]	Loss 0.2953 (0.6275)	
training:	Epoch: [1][86/467]	Loss 0.5327 (0.6264)	
training:	Epoch: [1][87/467]	Loss 0.6392 (0.6265)	
training:	Epoch: [1][88/467]	Loss 0.6362 (0.6266)	
training:	Epoch: [1][89/467]	Loss 0.6255 (0.6266)	
training:	Epoch: [1][90/467]	Loss 0.5272 (0.6255)	
training:	Epoch: [1][91/467]	Loss 0.6941 (0.6263)	
training:	Epoch: [1][92/467]	Loss 0.5115 (0.6250)	
training:	Epoch: [1][93/467]	Loss 0.6758 (0.6256)	
training:	Epoch: [1][94/467]	Loss 0.4831 (0.6240)	
training:	Epoch: [1][95/467]	Loss 0.7110 (0.6250)	
training:	Epoch: [1][96/467]	Loss 0.3788 (0.6224)	
training:	Epoch: [1][97/467]	Loss 0.6313 (0.6225)	
training:	Epoch: [1][98/467]	Loss 0.5301 (0.6215)	
training:	Epoch: [1][99/467]	Loss 0.4242 (0.6195)	
training:	Epoch: [1][100/467]	Loss 0.5476 (0.6188)	
training:	Epoch: [1][101/467]	Loss 0.4418 (0.6171)	
training:	Epoch: [1][102/467]	Loss 0.5443 (0.6164)	
training:	Epoch: [1][103/467]	Loss 0.5284 (0.6155)	
training:	Epoch: [1][104/467]	Loss 0.3871 (0.6133)	
training:	Epoch: [1][105/467]	Loss 0.4683 (0.6119)	
training:	Epoch: [1][106/467]	Loss 0.5345 (0.6112)	
training:	Epoch: [1][107/467]	Loss 0.3524 (0.6088)	
training:	Epoch: [1][108/467]	Loss 0.4271 (0.6071)	
training:	Epoch: [1][109/467]	Loss 0.7267 (0.6082)	
training:	Epoch: [1][110/467]	Loss 0.4032 (0.6063)	
training:	Epoch: [1][111/467]	Loss 0.4899 (0.6053)	
training:	Epoch: [1][112/467]	Loss 0.5056 (0.6044)	
training:	Epoch: [1][113/467]	Loss 0.3772 (0.6024)	
training:	Epoch: [1][114/467]	Loss 0.4197 (0.6008)	
training:	Epoch: [1][115/467]	Loss 0.6032 (0.6008)	
training:	Epoch: [1][116/467]	Loss 0.5742 (0.6006)	
training:	Epoch: [1][117/467]	Loss 0.3639 (0.5986)	
training:	Epoch: [1][118/467]	Loss 0.4304 (0.5971)	
training:	Epoch: [1][119/467]	Loss 0.7174 (0.5981)	
training:	Epoch: [1][120/467]	Loss 0.7521 (0.5994)	
training:	Epoch: [1][121/467]	Loss 0.5381 (0.5989)	
training:	Epoch: [1][122/467]	Loss 0.4891 (0.5980)	
training:	Epoch: [1][123/467]	Loss 0.5339 (0.5975)	
training:	Epoch: [1][124/467]	Loss 0.5412 (0.5970)	
training:	Epoch: [1][125/467]	Loss 0.3719 (0.5952)	
training:	Epoch: [1][126/467]	Loss 0.3838 (0.5936)	
training:	Epoch: [1][127/467]	Loss 0.7981 (0.5952)	
training:	Epoch: [1][128/467]	Loss 0.4621 (0.5941)	
training:	Epoch: [1][129/467]	Loss 1.0516 (0.5977)	
training:	Epoch: [1][130/467]	Loss 0.3372 (0.5957)	
training:	Epoch: [1][131/467]	Loss 0.4491 (0.5946)	
training:	Epoch: [1][132/467]	Loss 0.2916 (0.5923)	
training:	Epoch: [1][133/467]	Loss 0.4771 (0.5914)	
training:	Epoch: [1][134/467]	Loss 0.5196 (0.5909)	
training:	Epoch: [1][135/467]	Loss 0.3955 (0.5894)	
training:	Epoch: [1][136/467]	Loss 0.4401 (0.5883)	
training:	Epoch: [1][137/467]	Loss 0.6200 (0.5885)	
training:	Epoch: [1][138/467]	Loss 0.2028 (0.5857)	
training:	Epoch: [1][139/467]	Loss 0.6767 (0.5864)	
training:	Epoch: [1][140/467]	Loss 0.4743 (0.5856)	
training:	Epoch: [1][141/467]	Loss 0.3279 (0.5838)	
training:	Epoch: [1][142/467]	Loss 0.4576 (0.5829)	
training:	Epoch: [1][143/467]	Loss 0.8091 (0.5845)	
training:	Epoch: [1][144/467]	Loss 0.6141 (0.5847)	
training:	Epoch: [1][145/467]	Loss 0.6482 (0.5851)	
training:	Epoch: [1][146/467]	Loss 0.4800 (0.5844)	
training:	Epoch: [1][147/467]	Loss 0.4699 (0.5836)	
training:	Epoch: [1][148/467]	Loss 0.4975 (0.5830)	
training:	Epoch: [1][149/467]	Loss 0.3942 (0.5818)	
training:	Epoch: [1][150/467]	Loss 0.5183 (0.5813)	
training:	Epoch: [1][151/467]	Loss 0.5290 (0.5810)	
training:	Epoch: [1][152/467]	Loss 0.6274 (0.5813)	
training:	Epoch: [1][153/467]	Loss 0.4779 (0.5806)	
training:	Epoch: [1][154/467]	Loss 0.6424 (0.5810)	
training:	Epoch: [1][155/467]	Loss 0.2565 (0.5789)	
training:	Epoch: [1][156/467]	Loss 0.4941 (0.5784)	
training:	Epoch: [1][157/467]	Loss 0.3443 (0.5769)	
training:	Epoch: [1][158/467]	Loss 0.5896 (0.5770)	
training:	Epoch: [1][159/467]	Loss 0.4137 (0.5759)	
training:	Epoch: [1][160/467]	Loss 0.5937 (0.5761)	
training:	Epoch: [1][161/467]	Loss 0.4514 (0.5753)	
training:	Epoch: [1][162/467]	Loss 0.4769 (0.5747)	
training:	Epoch: [1][163/467]	Loss 0.6016 (0.5748)	
training:	Epoch: [1][164/467]	Loss 0.4347 (0.5740)	
training:	Epoch: [1][165/467]	Loss 0.5449 (0.5738)	
training:	Epoch: [1][166/467]	Loss 0.4962 (0.5733)	
training:	Epoch: [1][167/467]	Loss 0.2950 (0.5717)	
training:	Epoch: [1][168/467]	Loss 0.4257 (0.5708)	
training:	Epoch: [1][169/467]	Loss 0.3627 (0.5696)	
training:	Epoch: [1][170/467]	Loss 0.7461 (0.5706)	
training:	Epoch: [1][171/467]	Loss 0.4769 (0.5701)	
training:	Epoch: [1][172/467]	Loss 0.4771 (0.5695)	
training:	Epoch: [1][173/467]	Loss 0.2823 (0.5679)	
training:	Epoch: [1][174/467]	Loss 0.5854 (0.5680)	
training:	Epoch: [1][175/467]	Loss 0.5051 (0.5676)	
training:	Epoch: [1][176/467]	Loss 0.6712 (0.5682)	
training:	Epoch: [1][177/467]	Loss 0.5534 (0.5681)	
training:	Epoch: [1][178/467]	Loss 0.5594 (0.5681)	
training:	Epoch: [1][179/467]	Loss 0.3866 (0.5671)	
training:	Epoch: [1][180/467]	Loss 0.4334 (0.5663)	
training:	Epoch: [1][181/467]	Loss 0.4865 (0.5659)	
training:	Epoch: [1][182/467]	Loss 0.4471 (0.5652)	
training:	Epoch: [1][183/467]	Loss 0.7846 (0.5664)	
training:	Epoch: [1][184/467]	Loss 0.4313 (0.5657)	
training:	Epoch: [1][185/467]	Loss 0.4450 (0.5650)	
training:	Epoch: [1][186/467]	Loss 0.5186 (0.5648)	
training:	Epoch: [1][187/467]	Loss 0.3647 (0.5637)	
training:	Epoch: [1][188/467]	Loss 0.4994 (0.5634)	
training:	Epoch: [1][189/467]	Loss 0.3994 (0.5625)	
training:	Epoch: [1][190/467]	Loss 0.4495 (0.5619)	
training:	Epoch: [1][191/467]	Loss 0.5305 (0.5617)	
training:	Epoch: [1][192/467]	Loss 0.5975 (0.5619)	
training:	Epoch: [1][193/467]	Loss 0.7791 (0.5631)	
training:	Epoch: [1][194/467]	Loss 0.5937 (0.5632)	
training:	Epoch: [1][195/467]	Loss 0.4136 (0.5624)	
training:	Epoch: [1][196/467]	Loss 0.4896 (0.5621)	
training:	Epoch: [1][197/467]	Loss 0.4052 (0.5613)	
training:	Epoch: [1][198/467]	Loss 0.4475 (0.5607)	
training:	Epoch: [1][199/467]	Loss 0.3030 (0.5594)	
training:	Epoch: [1][200/467]	Loss 0.3876 (0.5585)	
training:	Epoch: [1][201/467]	Loss 0.4244 (0.5579)	
training:	Epoch: [1][202/467]	Loss 0.3199 (0.5567)	
training:	Epoch: [1][203/467]	Loss 0.5727 (0.5568)	
training:	Epoch: [1][204/467]	Loss 0.5231 (0.5566)	
training:	Epoch: [1][205/467]	Loss 0.8923 (0.5583)	
training:	Epoch: [1][206/467]	Loss 0.2540 (0.5568)	
training:	Epoch: [1][207/467]	Loss 0.6410 (0.5572)	
training:	Epoch: [1][208/467]	Loss 0.5485 (0.5571)	
training:	Epoch: [1][209/467]	Loss 0.3726 (0.5563)	
training:	Epoch: [1][210/467]	Loss 0.2653 (0.5549)	
training:	Epoch: [1][211/467]	Loss 0.6345 (0.5552)	
training:	Epoch: [1][212/467]	Loss 0.5142 (0.5551)	
training:	Epoch: [1][213/467]	Loss 0.5630 (0.5551)	
training:	Epoch: [1][214/467]	Loss 0.4560 (0.5546)	
training:	Epoch: [1][215/467]	Loss 0.3966 (0.5539)	
training:	Epoch: [1][216/467]	Loss 0.5033 (0.5537)	
training:	Epoch: [1][217/467]	Loss 0.5378 (0.5536)	
training:	Epoch: [1][218/467]	Loss 0.3742 (0.5528)	
training:	Epoch: [1][219/467]	Loss 0.5967 (0.5530)	
training:	Epoch: [1][220/467]	Loss 0.6601 (0.5535)	
training:	Epoch: [1][221/467]	Loss 0.3017 (0.5523)	
training:	Epoch: [1][222/467]	Loss 0.5241 (0.5522)	
training:	Epoch: [1][223/467]	Loss 0.5144 (0.5520)	
training:	Epoch: [1][224/467]	Loss 0.3813 (0.5513)	
training:	Epoch: [1][225/467]	Loss 0.4112 (0.5506)	
training:	Epoch: [1][226/467]	Loss 0.7349 (0.5514)	
training:	Epoch: [1][227/467]	Loss 0.4265 (0.5509)	
training:	Epoch: [1][228/467]	Loss 0.4438 (0.5504)	
training:	Epoch: [1][229/467]	Loss 0.1743 (0.5488)	
training:	Epoch: [1][230/467]	Loss 0.4831 (0.5485)	
training:	Epoch: [1][231/467]	Loss 0.3312 (0.5476)	
training:	Epoch: [1][232/467]	Loss 0.3968 (0.5469)	
training:	Epoch: [1][233/467]	Loss 0.4753 (0.5466)	
training:	Epoch: [1][234/467]	Loss 0.2509 (0.5453)	
training:	Epoch: [1][235/467]	Loss 0.5635 (0.5454)	
training:	Epoch: [1][236/467]	Loss 0.3672 (0.5447)	
training:	Epoch: [1][237/467]	Loss 0.4066 (0.5441)	
training:	Epoch: [1][238/467]	Loss 0.8250 (0.5453)	
training:	Epoch: [1][239/467]	Loss 0.4061 (0.5447)	
training:	Epoch: [1][240/467]	Loss 0.4291 (0.5442)	
training:	Epoch: [1][241/467]	Loss 0.4422 (0.5438)	
training:	Epoch: [1][242/467]	Loss 0.7305 (0.5445)	
training:	Epoch: [1][243/467]	Loss 0.3624 (0.5438)	
training:	Epoch: [1][244/467]	Loss 0.4969 (0.5436)	
training:	Epoch: [1][245/467]	Loss 0.3893 (0.5430)	
training:	Epoch: [1][246/467]	Loss 0.2808 (0.5419)	
training:	Epoch: [1][247/467]	Loss 0.3638 (0.5412)	
training:	Epoch: [1][248/467]	Loss 0.4747 (0.5409)	
training:	Epoch: [1][249/467]	Loss 0.6831 (0.5415)	
training:	Epoch: [1][250/467]	Loss 0.3524 (0.5407)	
training:	Epoch: [1][251/467]	Loss 0.5629 (0.5408)	
training:	Epoch: [1][252/467]	Loss 0.3554 (0.5401)	
training:	Epoch: [1][253/467]	Loss 0.4404 (0.5397)	
training:	Epoch: [1][254/467]	Loss 0.5082 (0.5396)	
training:	Epoch: [1][255/467]	Loss 0.7673 (0.5405)	
training:	Epoch: [1][256/467]	Loss 0.2229 (0.5392)	
training:	Epoch: [1][257/467]	Loss 0.6582 (0.5397)	
training:	Epoch: [1][258/467]	Loss 0.2919 (0.5387)	
training:	Epoch: [1][259/467]	Loss 0.4662 (0.5384)	
training:	Epoch: [1][260/467]	Loss 0.5437 (0.5385)	
training:	Epoch: [1][261/467]	Loss 0.2237 (0.5373)	
training:	Epoch: [1][262/467]	Loss 0.4948 (0.5371)	
training:	Epoch: [1][263/467]	Loss 0.4728 (0.5368)	
training:	Epoch: [1][264/467]	Loss 0.2700 (0.5358)	
training:	Epoch: [1][265/467]	Loss 0.3979 (0.5353)	
training:	Epoch: [1][266/467]	Loss 0.5414 (0.5353)	
training:	Epoch: [1][267/467]	Loss 0.8555 (0.5365)	
training:	Epoch: [1][268/467]	Loss 0.3697 (0.5359)	
training:	Epoch: [1][269/467]	Loss 0.5359 (0.5359)	
training:	Epoch: [1][270/467]	Loss 0.5234 (0.5359)	
training:	Epoch: [1][271/467]	Loss 0.4790 (0.5357)	
training:	Epoch: [1][272/467]	Loss 0.5615 (0.5358)	
training:	Epoch: [1][273/467]	Loss 0.6285 (0.5361)	
training:	Epoch: [1][274/467]	Loss 0.2267 (0.5350)	
training:	Epoch: [1][275/467]	Loss 0.4017 (0.5345)	
training:	Epoch: [1][276/467]	Loss 0.3696 (0.5339)	
training:	Epoch: [1][277/467]	Loss 0.3605 (0.5333)	
training:	Epoch: [1][278/467]	Loss 0.5033 (0.5331)	
training:	Epoch: [1][279/467]	Loss 0.2960 (0.5323)	
training:	Epoch: [1][280/467]	Loss 0.2300 (0.5312)	
training:	Epoch: [1][281/467]	Loss 0.6507 (0.5316)	
training:	Epoch: [1][282/467]	Loss 0.6787 (0.5322)	
training:	Epoch: [1][283/467]	Loss 0.3613 (0.5316)	
training:	Epoch: [1][284/467]	Loss 0.4240 (0.5312)	
training:	Epoch: [1][285/467]	Loss 0.4762 (0.5310)	
training:	Epoch: [1][286/467]	Loss 0.3306 (0.5303)	
training:	Epoch: [1][287/467]	Loss 0.3175 (0.5295)	
training:	Epoch: [1][288/467]	Loss 0.5352 (0.5296)	
training:	Epoch: [1][289/467]	Loss 0.6679 (0.5300)	
training:	Epoch: [1][290/467]	Loss 0.4486 (0.5298)	
training:	Epoch: [1][291/467]	Loss 0.3239 (0.5291)	
training:	Epoch: [1][292/467]	Loss 0.3520 (0.5285)	
training:	Epoch: [1][293/467]	Loss 0.5256 (0.5284)	
training:	Epoch: [1][294/467]	Loss 0.4711 (0.5282)	
training:	Epoch: [1][295/467]	Loss 0.3816 (0.5278)	
training:	Epoch: [1][296/467]	Loss 0.2151 (0.5267)	
training:	Epoch: [1][297/467]	Loss 0.5489 (0.5268)	
training:	Epoch: [1][298/467]	Loss 0.4235 (0.5264)	
training:	Epoch: [1][299/467]	Loss 0.3874 (0.5260)	
training:	Epoch: [1][300/467]	Loss 0.4404 (0.5257)	
training:	Epoch: [1][301/467]	Loss 0.3834 (0.5252)	
training:	Epoch: [1][302/467]	Loss 0.7854 (0.5261)	
training:	Epoch: [1][303/467]	Loss 0.6994 (0.5266)	
training:	Epoch: [1][304/467]	Loss 0.4089 (0.5262)	
training:	Epoch: [1][305/467]	Loss 0.3182 (0.5256)	
training:	Epoch: [1][306/467]	Loss 0.3963 (0.5251)	
training:	Epoch: [1][307/467]	Loss 0.4141 (0.5248)	
training:	Epoch: [1][308/467]	Loss 0.4425 (0.5245)	
training:	Epoch: [1][309/467]	Loss 0.4829 (0.5244)	
training:	Epoch: [1][310/467]	Loss 0.3011 (0.5237)	
training:	Epoch: [1][311/467]	Loss 0.6705 (0.5241)	
training:	Epoch: [1][312/467]	Loss 0.6051 (0.5244)	
training:	Epoch: [1][313/467]	Loss 0.3228 (0.5237)	
training:	Epoch: [1][314/467]	Loss 0.2084 (0.5227)	
training:	Epoch: [1][315/467]	Loss 0.2817 (0.5220)	
training:	Epoch: [1][316/467]	Loss 0.3798 (0.5215)	
training:	Epoch: [1][317/467]	Loss 0.3147 (0.5209)	
training:	Epoch: [1][318/467]	Loss 0.4905 (0.5208)	
training:	Epoch: [1][319/467]	Loss 0.5790 (0.5210)	
training:	Epoch: [1][320/467]	Loss 0.4173 (0.5206)	
training:	Epoch: [1][321/467]	Loss 0.4448 (0.5204)	
training:	Epoch: [1][322/467]	Loss 0.5387 (0.5205)	
training:	Epoch: [1][323/467]	Loss 0.2802 (0.5197)	
training:	Epoch: [1][324/467]	Loss 0.3045 (0.5190)	
training:	Epoch: [1][325/467]	Loss 0.3104 (0.5184)	
training:	Epoch: [1][326/467]	Loss 0.3071 (0.5178)	
training:	Epoch: [1][327/467]	Loss 0.3726 (0.5173)	
training:	Epoch: [1][328/467]	Loss 0.7311 (0.5180)	
training:	Epoch: [1][329/467]	Loss 0.3107 (0.5173)	
training:	Epoch: [1][330/467]	Loss 0.4650 (0.5172)	
training:	Epoch: [1][331/467]	Loss 0.3094 (0.5166)	
training:	Epoch: [1][332/467]	Loss 0.4439 (0.5163)	
training:	Epoch: [1][333/467]	Loss 0.4432 (0.5161)	
training:	Epoch: [1][334/467]	Loss 0.3471 (0.5156)	
training:	Epoch: [1][335/467]	Loss 0.2890 (0.5149)	
training:	Epoch: [1][336/467]	Loss 0.6647 (0.5154)	
training:	Epoch: [1][337/467]	Loss 0.3104 (0.5148)	
training:	Epoch: [1][338/467]	Loss 0.4916 (0.5147)	
training:	Epoch: [1][339/467]	Loss 0.4727 (0.5146)	
training:	Epoch: [1][340/467]	Loss 0.2652 (0.5138)	
training:	Epoch: [1][341/467]	Loss 0.4550 (0.5137)	
training:	Epoch: [1][342/467]	Loss 0.3225 (0.5131)	
training:	Epoch: [1][343/467]	Loss 0.8976 (0.5142)	
training:	Epoch: [1][344/467]	Loss 0.3830 (0.5138)	
training:	Epoch: [1][345/467]	Loss 0.4820 (0.5138)	
training:	Epoch: [1][346/467]	Loss 0.5151 (0.5138)	
training:	Epoch: [1][347/467]	Loss 0.3052 (0.5132)	
training:	Epoch: [1][348/467]	Loss 0.3471 (0.5127)	
training:	Epoch: [1][349/467]	Loss 0.2238 (0.5119)	
training:	Epoch: [1][350/467]	Loss 0.5617 (0.5120)	
training:	Epoch: [1][351/467]	Loss 0.3372 (0.5115)	
training:	Epoch: [1][352/467]	Loss 0.3957 (0.5112)	
training:	Epoch: [1][353/467]	Loss 0.4109 (0.5109)	
training:	Epoch: [1][354/467]	Loss 0.2901 (0.5103)	
training:	Epoch: [1][355/467]	Loss 0.4441 (0.5101)	
training:	Epoch: [1][356/467]	Loss 0.6264 (0.5104)	
training:	Epoch: [1][357/467]	Loss 0.5194 (0.5104)	
training:	Epoch: [1][358/467]	Loss 0.3079 (0.5099)	
training:	Epoch: [1][359/467]	Loss 0.7570 (0.5106)	
training:	Epoch: [1][360/467]	Loss 0.3877 (0.5102)	
training:	Epoch: [1][361/467]	Loss 0.5435 (0.5103)	
training:	Epoch: [1][362/467]	Loss 0.4701 (0.5102)	
training:	Epoch: [1][363/467]	Loss 0.4788 (0.5101)	
training:	Epoch: [1][364/467]	Loss 0.2204 (0.5093)	
training:	Epoch: [1][365/467]	Loss 0.4875 (0.5092)	
training:	Epoch: [1][366/467]	Loss 0.2476 (0.5085)	
training:	Epoch: [1][367/467]	Loss 0.3038 (0.5080)	
training:	Epoch: [1][368/467]	Loss 0.5105 (0.5080)	
training:	Epoch: [1][369/467]	Loss 0.4439 (0.5078)	
training:	Epoch: [1][370/467]	Loss 0.2435 (0.5071)	
training:	Epoch: [1][371/467]	Loss 0.2884 (0.5065)	
training:	Epoch: [1][372/467]	Loss 0.3288 (0.5060)	
training:	Epoch: [1][373/467]	Loss 0.5715 (0.5062)	
training:	Epoch: [1][374/467]	Loss 0.3213 (0.5057)	
training:	Epoch: [1][375/467]	Loss 0.2350 (0.5050)	
training:	Epoch: [1][376/467]	Loss 0.5952 (0.5052)	
training:	Epoch: [1][377/467]	Loss 0.3763 (0.5049)	
training:	Epoch: [1][378/467]	Loss 0.5331 (0.5050)	
training:	Epoch: [1][379/467]	Loss 0.3109 (0.5044)	
training:	Epoch: [1][380/467]	Loss 0.4736 (0.5044)	
training:	Epoch: [1][381/467]	Loss 0.3393 (0.5039)	
training:	Epoch: [1][382/467]	Loss 0.5164 (0.5040)	
training:	Epoch: [1][383/467]	Loss 0.3729 (0.5036)	
training:	Epoch: [1][384/467]	Loss 0.4424 (0.5035)	
training:	Epoch: [1][385/467]	Loss 0.5085 (0.5035)	
training:	Epoch: [1][386/467]	Loss 0.2877 (0.5029)	
training:	Epoch: [1][387/467]	Loss 0.3286 (0.5025)	
training:	Epoch: [1][388/467]	Loss 0.3320 (0.5020)	
training:	Epoch: [1][389/467]	Loss 0.4223 (0.5018)	
training:	Epoch: [1][390/467]	Loss 0.4935 (0.5018)	
training:	Epoch: [1][391/467]	Loss 0.7197 (0.5024)	
training:	Epoch: [1][392/467]	Loss 0.7275 (0.5029)	
training:	Epoch: [1][393/467]	Loss 0.3369 (0.5025)	
training:	Epoch: [1][394/467]	Loss 0.5993 (0.5028)	
training:	Epoch: [1][395/467]	Loss 0.4253 (0.5026)	
training:	Epoch: [1][396/467]	Loss 0.3713 (0.5022)	
training:	Epoch: [1][397/467]	Loss 0.6110 (0.5025)	
training:	Epoch: [1][398/467]	Loss 0.3526 (0.5021)	
training:	Epoch: [1][399/467]	Loss 0.4921 (0.5021)	
training:	Epoch: [1][400/467]	Loss 0.3823 (0.5018)	
training:	Epoch: [1][401/467]	Loss 0.7693 (0.5025)	
training:	Epoch: [1][402/467]	Loss 0.4064 (0.5022)	
training:	Epoch: [1][403/467]	Loss 0.4816 (0.5022)	
training:	Epoch: [1][404/467]	Loss 0.5101 (0.5022)	
training:	Epoch: [1][405/467]	Loss 0.3165 (0.5017)	
training:	Epoch: [1][406/467]	Loss 0.2237 (0.5011)	
training:	Epoch: [1][407/467]	Loss 0.3952 (0.5008)	
training:	Epoch: [1][408/467]	Loss 0.6797 (0.5012)	
training:	Epoch: [1][409/467]	Loss 0.2755 (0.5007)	
training:	Epoch: [1][410/467]	Loss 0.1902 (0.4999)	
training:	Epoch: [1][411/467]	Loss 0.2493 (0.4993)	
training:	Epoch: [1][412/467]	Loss 0.2847 (0.4988)	
training:	Epoch: [1][413/467]	Loss 0.7559 (0.4994)	
training:	Epoch: [1][414/467]	Loss 0.6022 (0.4997)	
training:	Epoch: [1][415/467]	Loss 0.5499 (0.4998)	
training:	Epoch: [1][416/467]	Loss 0.4100 (0.4996)	
training:	Epoch: [1][417/467]	Loss 0.4260 (0.4994)	
training:	Epoch: [1][418/467]	Loss 0.6798 (0.4998)	
training:	Epoch: [1][419/467]	Loss 0.3343 (0.4994)	
training:	Epoch: [1][420/467]	Loss 0.4092 (0.4992)	
training:	Epoch: [1][421/467]	Loss 0.3464 (0.4989)	
training:	Epoch: [1][422/467]	Loss 0.4071 (0.4986)	
training:	Epoch: [1][423/467]	Loss 0.1931 (0.4979)	
training:	Epoch: [1][424/467]	Loss 0.3603 (0.4976)	
training:	Epoch: [1][425/467]	Loss 0.4665 (0.4975)	
training:	Epoch: [1][426/467]	Loss 0.4087 (0.4973)	
training:	Epoch: [1][427/467]	Loss 0.5576 (0.4974)	
training:	Epoch: [1][428/467]	Loss 0.6135 (0.4977)	
training:	Epoch: [1][429/467]	Loss 0.2799 (0.4972)	
training:	Epoch: [1][430/467]	Loss 0.4991 (0.4972)	
training:	Epoch: [1][431/467]	Loss 0.4676 (0.4971)	
training:	Epoch: [1][432/467]	Loss 0.4851 (0.4971)	
training:	Epoch: [1][433/467]	Loss 0.4171 (0.4969)	
training:	Epoch: [1][434/467]	Loss 0.5228 (0.4970)	
training:	Epoch: [1][435/467]	Loss 0.3600 (0.4967)	
training:	Epoch: [1][436/467]	Loss 0.4712 (0.4966)	
training:	Epoch: [1][437/467]	Loss 0.2685 (0.4961)	
training:	Epoch: [1][438/467]	Loss 0.5200 (0.4962)	
training:	Epoch: [1][439/467]	Loss 0.5776 (0.4963)	
training:	Epoch: [1][440/467]	Loss 0.2657 (0.4958)	
training:	Epoch: [1][441/467]	Loss 0.2403 (0.4952)	
training:	Epoch: [1][442/467]	Loss 0.3828 (0.4950)	
training:	Epoch: [1][443/467]	Loss 0.4308 (0.4948)	
training:	Epoch: [1][444/467]	Loss 0.5152 (0.4949)	
training:	Epoch: [1][445/467]	Loss 0.4288 (0.4947)	
training:	Epoch: [1][446/467]	Loss 0.4173 (0.4946)	
training:	Epoch: [1][447/467]	Loss 0.5195 (0.4946)	
training:	Epoch: [1][448/467]	Loss 0.4509 (0.4945)	
training:	Epoch: [1][449/467]	Loss 0.1735 (0.4938)	
training:	Epoch: [1][450/467]	Loss 0.3812 (0.4936)	
training:	Epoch: [1][451/467]	Loss 0.4565 (0.4935)	
training:	Epoch: [1][452/467]	Loss 0.3145 (0.4931)	
training:	Epoch: [1][453/467]	Loss 0.4379 (0.4930)	
training:	Epoch: [1][454/467]	Loss 0.2596 (0.4924)	
training:	Epoch: [1][455/467]	Loss 0.3256 (0.4921)	
training:	Epoch: [1][456/467]	Loss 0.2515 (0.4915)	
training:	Epoch: [1][457/467]	Loss 0.2040 (0.4909)	
training:	Epoch: [1][458/467]	Loss 0.2916 (0.4905)	
training:	Epoch: [1][459/467]	Loss 0.3628 (0.4902)	
training:	Epoch: [1][460/467]	Loss 0.4791 (0.4902)	
training:	Epoch: [1][461/467]	Loss 0.3812 (0.4899)	
training:	Epoch: [1][462/467]	Loss 0.2379 (0.4894)	
training:	Epoch: [1][463/467]	Loss 0.4790 (0.4894)	
training:	Epoch: [1][464/467]	Loss 0.3805 (0.4891)	
training:	Epoch: [1][465/467]	Loss 0.1678 (0.4884)	
training:	Epoch: [1][466/467]	Loss 0.5910 (0.4887)	
training:	Epoch: [1][467/467]	Loss 0.2150 (0.4881)	
Training:	 Loss: 0.4880

Training:	 ACC: 0.8883 0.8907 0.9426 0.8339
Validation:	 ACC: 0.8121 0.8159 0.8976 0.7265
Validation:	 Best_BACC: 0.8121 0.8159 0.8976 0.7265
Validation:	 Loss: 0.3932
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/467]	Loss 0.3509 (0.3509)	
training:	Epoch: [2][2/467]	Loss 0.2632 (0.3070)	
training:	Epoch: [2][3/467]	Loss 0.4776 (0.3639)	
training:	Epoch: [2][4/467]	Loss 0.2519 (0.3359)	
training:	Epoch: [2][5/467]	Loss 0.1501 (0.2987)	
training:	Epoch: [2][6/467]	Loss 0.1612 (0.2758)	
training:	Epoch: [2][7/467]	Loss 0.3025 (0.2796)	
training:	Epoch: [2][8/467]	Loss 0.4453 (0.3003)	
training:	Epoch: [2][9/467]	Loss 0.1332 (0.2818)	
training:	Epoch: [2][10/467]	Loss 0.2715 (0.2807)	
training:	Epoch: [2][11/467]	Loss 0.2994 (0.2824)	
training:	Epoch: [2][12/467]	Loss 0.1698 (0.2731)	
training:	Epoch: [2][13/467]	Loss 0.3236 (0.2769)	
training:	Epoch: [2][14/467]	Loss 0.2841 (0.2775)	
training:	Epoch: [2][15/467]	Loss 0.3605 (0.2830)	
training:	Epoch: [2][16/467]	Loss 0.3315 (0.2860)	
training:	Epoch: [2][17/467]	Loss 0.2778 (0.2855)	
training:	Epoch: [2][18/467]	Loss 0.3962 (0.2917)	
training:	Epoch: [2][19/467]	Loss 0.2640 (0.2902)	
training:	Epoch: [2][20/467]	Loss 0.3516 (0.2933)	
training:	Epoch: [2][21/467]	Loss 0.1909 (0.2884)	
training:	Epoch: [2][22/467]	Loss 0.2389 (0.2862)	
training:	Epoch: [2][23/467]	Loss 0.2616 (0.2851)	
training:	Epoch: [2][24/467]	Loss 0.3350 (0.2872)	
training:	Epoch: [2][25/467]	Loss 0.2206 (0.2845)	
training:	Epoch: [2][26/467]	Loss 0.3415 (0.2867)	
training:	Epoch: [2][27/467]	Loss 0.1700 (0.2824)	
training:	Epoch: [2][28/467]	Loss 0.5873 (0.2933)	
training:	Epoch: [2][29/467]	Loss 0.4865 (0.2999)	
training:	Epoch: [2][30/467]	Loss 0.1797 (0.2959)	
training:	Epoch: [2][31/467]	Loss 0.3217 (0.2968)	
training:	Epoch: [2][32/467]	Loss 0.7470 (0.3108)	
training:	Epoch: [2][33/467]	Loss 0.0798 (0.3038)	
training:	Epoch: [2][34/467]	Loss 0.2091 (0.3010)	
training:	Epoch: [2][35/467]	Loss 0.6134 (0.3100)	
training:	Epoch: [2][36/467]	Loss 0.3761 (0.3118)	
training:	Epoch: [2][37/467]	Loss 0.2502 (0.3101)	
training:	Epoch: [2][38/467]	Loss 0.3915 (0.3123)	
training:	Epoch: [2][39/467]	Loss 0.3522 (0.3133)	
training:	Epoch: [2][40/467]	Loss 0.4320 (0.3163)	
training:	Epoch: [2][41/467]	Loss 0.2145 (0.3138)	
training:	Epoch: [2][42/467]	Loss 0.1825 (0.3107)	
training:	Epoch: [2][43/467]	Loss 0.3154 (0.3108)	
training:	Epoch: [2][44/467]	Loss 0.2366 (0.3091)	
training:	Epoch: [2][45/467]	Loss 0.5067 (0.3135)	
training:	Epoch: [2][46/467]	Loss 0.3980 (0.3153)	
training:	Epoch: [2][47/467]	Loss 0.1915 (0.3127)	
