Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=200, tensorboard=True, net_architecture='DGCNN', batch_size=32, rate=1e-06, weight=0.0, sched_step=100, sched_gamma=0.1, printing_frequency=1, seed=3, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20)
Training the 'DGCNN' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	1e-06
Weight decay:	0.0
Scheduler steps:	100
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/204]	Loss 0.7711 (0.7711)	
training:	Epoch: [1][2/204]	Loss 0.6818 (0.7265)	
training:	Epoch: [1][3/204]	Loss 0.7342 (0.7290)	
training:	Epoch: [1][4/204]	Loss 0.7270 (0.7285)	
training:	Epoch: [1][5/204]	Loss 0.6131 (0.7055)	
training:	Epoch: [1][6/204]	Loss 0.7093 (0.7061)	
training:	Epoch: [1][7/204]	Loss 0.7102 (0.7067)	
training:	Epoch: [1][8/204]	Loss 0.7720 (0.7149)	
training:	Epoch: [1][9/204]	Loss 0.7522 (0.7190)	
training:	Epoch: [1][10/204]	Loss 0.6448 (0.7116)	
training:	Epoch: [1][11/204]	Loss 0.6440 (0.7054)	
training:	Epoch: [1][12/204]	Loss 0.7412 (0.7084)	
training:	Epoch: [1][13/204]	Loss 0.7280 (0.7099)	
training:	Epoch: [1][14/204]	Loss 0.6590 (0.7063)	
training:	Epoch: [1][15/204]	Loss 0.7318 (0.7080)	
training:	Epoch: [1][16/204]	Loss 0.6858 (0.7066)	
training:	Epoch: [1][17/204]	Loss 0.6959 (0.7060)	
training:	Epoch: [1][18/204]	Loss 0.7577 (0.7088)	
training:	Epoch: [1][19/204]	Loss 0.6766 (0.7071)	
training:	Epoch: [1][20/204]	Loss 0.7886 (0.7112)	
training:	Epoch: [1][21/204]	Loss 0.6753 (0.7095)	
training:	Epoch: [1][22/204]	Loss 0.7532 (0.7115)	
training:	Epoch: [1][23/204]	Loss 0.5902 (0.7062)	
training:	Epoch: [1][24/204]	Loss 0.6364 (0.7033)	
training:	Epoch: [1][25/204]	Loss 0.6458 (0.7010)	
training:	Epoch: [1][26/204]	Loss 0.5701 (0.6960)	
training:	Epoch: [1][27/204]	Loss 0.6492 (0.6942)	
training:	Epoch: [1][28/204]	Loss 0.6936 (0.6942)	
training:	Epoch: [1][29/204]	Loss 0.7534 (0.6963)	
training:	Epoch: [1][30/204]	Loss 0.6864 (0.6959)	
training:	Epoch: [1][31/204]	Loss 0.6203 (0.6935)	
training:	Epoch: [1][32/204]	Loss 0.6898 (0.6934)	
training:	Epoch: [1][33/204]	Loss 0.6763 (0.6929)	
training:	Epoch: [1][34/204]	Loss 0.7076 (0.6933)	
training:	Epoch: [1][35/204]	Loss 0.6149 (0.6911)	
training:	Epoch: [1][36/204]	Loss 0.6118 (0.6889)	
training:	Epoch: [1][37/204]	Loss 0.6131 (0.6868)	
training:	Epoch: [1][38/204]	Loss 0.6216 (0.6851)	
training:	Epoch: [1][39/204]	Loss 0.6180 (0.6834)	
training:	Epoch: [1][40/204]	Loss 0.5981 (0.6812)	
training:	Epoch: [1][41/204]	Loss 0.5578 (0.6782)	
training:	Epoch: [1][42/204]	Loss 0.6468 (0.6775)	
training:	Epoch: [1][43/204]	Loss 0.6340 (0.6765)	
training:	Epoch: [1][44/204]	Loss 0.7079 (0.6772)	
training:	Epoch: [1][45/204]	Loss 0.7204 (0.6781)	
training:	Epoch: [1][46/204]	Loss 0.5700 (0.6758)	
training:	Epoch: [1][47/204]	Loss 0.5617 (0.6734)	
training:	Epoch: [1][48/204]	Loss 0.6234 (0.6723)	
training:	Epoch: [1][49/204]	Loss 0.5880 (0.6706)	
training:	Epoch: [1][50/204]	Loss 0.6938 (0.6711)	
training:	Epoch: [1][51/204]	Loss 0.6318 (0.6703)	
training:	Epoch: [1][52/204]	Loss 0.5804 (0.6686)	
training:	Epoch: [1][53/204]	Loss 0.5313 (0.6660)	
training:	Epoch: [1][54/204]	Loss 0.5691 (0.6642)	
training:	Epoch: [1][55/204]	Loss 0.6565 (0.6640)	
training:	Epoch: [1][56/204]	Loss 0.6302 (0.6634)	
training:	Epoch: [1][57/204]	Loss 0.7601 (0.6651)	
training:	Epoch: [1][58/204]	Loss 0.6083 (0.6642)	
training:	Epoch: [1][59/204]	Loss 0.5862 (0.6628)	
training:	Epoch: [1][60/204]	Loss 0.6783 (0.6631)	
training:	Epoch: [1][61/204]	Loss 0.6597 (0.6630)	
training:	Epoch: [1][62/204]	Loss 0.6313 (0.6625)	
training:	Epoch: [1][63/204]	Loss 0.5514 (0.6608)	
training:	Epoch: [1][64/204]	Loss 0.6397 (0.6604)	
training:	Epoch: [1][65/204]	Loss 0.6688 (0.6606)	
training:	Epoch: [1][66/204]	Loss 0.6425 (0.6603)	
training:	Epoch: [1][67/204]	Loss 0.6557 (0.6602)	
training:	Epoch: [1][68/204]	Loss 0.6096 (0.6595)	
training:	Epoch: [1][69/204]	Loss 0.5881 (0.6584)	
training:	Epoch: [1][70/204]	Loss 0.6682 (0.6586)	
training:	Epoch: [1][71/204]	Loss 0.5919 (0.6576)	
training:	Epoch: [1][72/204]	Loss 0.5431 (0.6560)	
training:	Epoch: [1][73/204]	Loss 0.6349 (0.6558)	
training:	Epoch: [1][74/204]	Loss 0.5739 (0.6547)	
training:	Epoch: [1][75/204]	Loss 0.5700 (0.6535)	
training:	Epoch: [1][76/204]	Loss 0.5730 (0.6525)	
training:	Epoch: [1][77/204]	Loss 0.6037 (0.6518)	
training:	Epoch: [1][78/204]	Loss 0.5876 (0.6510)	
training:	Epoch: [1][79/204]	Loss 0.5254 (0.6494)	
training:	Epoch: [1][80/204]	Loss 0.7382 (0.6505)	
training:	Epoch: [1][81/204]	Loss 0.6192 (0.6501)	
training:	Epoch: [1][82/204]	Loss 0.5939 (0.6495)	
training:	Epoch: [1][83/204]	Loss 0.5854 (0.6487)	
training:	Epoch: [1][84/204]	Loss 0.6048 (0.6482)	
training:	Epoch: [1][85/204]	Loss 0.6466 (0.6481)	
training:	Epoch: [1][86/204]	Loss 0.6877 (0.6486)	
training:	Epoch: [1][87/204]	Loss 0.5135 (0.6470)	
training:	Epoch: [1][88/204]	Loss 0.5782 (0.6463)	
training:	Epoch: [1][89/204]	Loss 0.7052 (0.6469)	
training:	Epoch: [1][90/204]	Loss 0.6034 (0.6464)	
training:	Epoch: [1][91/204]	Loss 0.5398 (0.6453)	
training:	Epoch: [1][92/204]	Loss 0.5691 (0.6444)	
training:	Epoch: [1][93/204]	Loss 0.6505 (0.6445)	
training:	Epoch: [1][94/204]	Loss 0.5108 (0.6431)	
training:	Epoch: [1][95/204]	Loss 0.7590 (0.6443)	
training:	Epoch: [1][96/204]	Loss 0.6482 (0.6443)	
training:	Epoch: [1][97/204]	Loss 0.5194 (0.6431)	
training:	Epoch: [1][98/204]	Loss 0.6780 (0.6434)	
training:	Epoch: [1][99/204]	Loss 0.5524 (0.6425)	
training:	Epoch: [1][100/204]	Loss 0.5404 (0.6415)	
training:	Epoch: [1][101/204]	Loss 0.6920 (0.6420)	
training:	Epoch: [1][102/204]	Loss 0.5784 (0.6414)	
training:	Epoch: [1][103/204]	Loss 0.5212 (0.6402)	
training:	Epoch: [1][104/204]	Loss 0.5916 (0.6397)	
training:	Epoch: [1][105/204]	Loss 0.6036 (0.6394)	
training:	Epoch: [1][106/204]	Loss 0.6094 (0.6391)	
training:	Epoch: [1][107/204]	Loss 0.7215 (0.6399)	
training:	Epoch: [1][108/204]	Loss 0.6498 (0.6400)	
training:	Epoch: [1][109/204]	Loss 0.6050 (0.6396)	
training:	Epoch: [1][110/204]	Loss 0.6918 (0.6401)	
training:	Epoch: [1][111/204]	Loss 0.6560 (0.6403)	
training:	Epoch: [1][112/204]	Loss 0.6228 (0.6401)	
training:	Epoch: [1][113/204]	Loss 0.5231 (0.6391)	
training:	Epoch: [1][114/204]	Loss 0.5449 (0.6382)	
training:	Epoch: [1][115/204]	Loss 0.6145 (0.6380)	
training:	Epoch: [1][116/204]	Loss 0.5202 (0.6370)	
training:	Epoch: [1][117/204]	Loss 0.5441 (0.6362)	
training:	Epoch: [1][118/204]	Loss 0.5474 (0.6355)	
training:	Epoch: [1][119/204]	Loss 0.5735 (0.6349)	
training:	Epoch: [1][120/204]	Loss 0.5586 (0.6343)	
training:	Epoch: [1][121/204]	Loss 0.6824 (0.6347)	
training:	Epoch: [1][122/204]	Loss 0.5356 (0.6339)	
training:	Epoch: [1][123/204]	Loss 0.5922 (0.6336)	
training:	Epoch: [1][124/204]	Loss 0.5974 (0.6333)	
training:	Epoch: [1][125/204]	Loss 0.5576 (0.6327)	
training:	Epoch: [1][126/204]	Loss 0.5031 (0.6316)	
training:	Epoch: [1][127/204]	Loss 0.5579 (0.6310)	
training:	Epoch: [1][128/204]	Loss 0.5840 (0.6307)	
training:	Epoch: [1][129/204]	Loss 0.6479 (0.6308)	
training:	Epoch: [1][130/204]	Loss 0.6497 (0.6310)	
training:	Epoch: [1][131/204]	Loss 0.5888 (0.6306)	
training:	Epoch: [1][132/204]	Loss 0.6080 (0.6305)	
training:	Epoch: [1][133/204]	Loss 0.7277 (0.6312)	
training:	Epoch: [1][134/204]	Loss 0.6925 (0.6317)	
training:	Epoch: [1][135/204]	Loss 0.5670 (0.6312)	
training:	Epoch: [1][136/204]	Loss 0.6357 (0.6312)	
training:	Epoch: [1][137/204]	Loss 0.6365 (0.6312)	
training:	Epoch: [1][138/204]	Loss 0.6655 (0.6315)	
training:	Epoch: [1][139/204]	Loss 0.6140 (0.6314)	
training:	Epoch: [1][140/204]	Loss 0.5802 (0.6310)	
training:	Epoch: [1][141/204]	Loss 0.5472 (0.6304)	
training:	Epoch: [1][142/204]	Loss 0.6024 (0.6302)	
training:	Epoch: [1][143/204]	Loss 0.5553 (0.6297)	
training:	Epoch: [1][144/204]	Loss 0.5080 (0.6288)	
training:	Epoch: [1][145/204]	Loss 0.5968 (0.6286)	
training:	Epoch: [1][146/204]	Loss 0.5834 (0.6283)	
training:	Epoch: [1][147/204]	Loss 0.6101 (0.6282)	
training:	Epoch: [1][148/204]	Loss 0.6055 (0.6280)	
training:	Epoch: [1][149/204]	Loss 0.4598 (0.6269)	
training:	Epoch: [1][150/204]	Loss 0.5319 (0.6263)	
training:	Epoch: [1][151/204]	Loss 0.5168 (0.6255)	
training:	Epoch: [1][152/204]	Loss 0.6977 (0.6260)	
training:	Epoch: [1][153/204]	Loss 0.6133 (0.6259)	
training:	Epoch: [1][154/204]	Loss 0.5574 (0.6255)	
training:	Epoch: [1][155/204]	Loss 0.5616 (0.6251)	
training:	Epoch: [1][156/204]	Loss 0.5832 (0.6248)	
training:	Epoch: [1][157/204]	Loss 0.6905 (0.6252)	
training:	Epoch: [1][158/204]	Loss 0.6915 (0.6257)	
training:	Epoch: [1][159/204]	Loss 0.7044 (0.6261)	
training:	Epoch: [1][160/204]	Loss 0.6286 (0.6262)	
training:	Epoch: [1][161/204]	Loss 0.7273 (0.6268)	
training:	Epoch: [1][162/204]	Loss 0.5558 (0.6264)	
training:	Epoch: [1][163/204]	Loss 0.7329 (0.6270)	
training:	Epoch: [1][164/204]	Loss 0.6291 (0.6270)	
training:	Epoch: [1][165/204]	Loss 0.5608 (0.6266)	
training:	Epoch: [1][166/204]	Loss 0.5471 (0.6261)	
training:	Epoch: [1][167/204]	Loss 0.4384 (0.6250)	
training:	Epoch: [1][168/204]	Loss 0.6379 (0.6251)	
training:	Epoch: [1][169/204]	Loss 0.5992 (0.6249)	
training:	Epoch: [1][170/204]	Loss 0.5600 (0.6246)	
training:	Epoch: [1][171/204]	Loss 0.5267 (0.6240)	
training:	Epoch: [1][172/204]	Loss 0.6023 (0.6239)	
training:	Epoch: [1][173/204]	Loss 0.5659 (0.6235)	
training:	Epoch: [1][174/204]	Loss 0.6977 (0.6239)	
training:	Epoch: [1][175/204]	Loss 0.4685 (0.6231)	
training:	Epoch: [1][176/204]	Loss 0.5414 (0.6226)	
training:	Epoch: [1][177/204]	Loss 0.6295 (0.6226)	
training:	Epoch: [1][178/204]	Loss 0.5531 (0.6222)	
training:	Epoch: [1][179/204]	Loss 0.5600 (0.6219)	
training:	Epoch: [1][180/204]	Loss 0.5265 (0.6214)	
training:	Epoch: [1][181/204]	Loss 0.5077 (0.6207)	
training:	Epoch: [1][182/204]	Loss 0.6459 (0.6209)	
training:	Epoch: [1][183/204]	Loss 0.7183 (0.6214)	
training:	Epoch: [1][184/204]	Loss 0.5959 (0.6213)	
training:	Epoch: [1][185/204]	Loss 0.7320 (0.6219)	
training:	Epoch: [1][186/204]	Loss 0.5322 (0.6214)	
training:	Epoch: [1][187/204]	Loss 0.5570 (0.6210)	
training:	Epoch: [1][188/204]	Loss 0.5309 (0.6206)	
training:	Epoch: [1][189/204]	Loss 0.5993 (0.6205)	
training:	Epoch: [1][190/204]	Loss 0.5504 (0.6201)	
training:	Epoch: [1][191/204]	Loss 0.5621 (0.6198)	
training:	Epoch: [1][192/204]	Loss 0.4736 (0.6190)	
training:	Epoch: [1][193/204]	Loss 0.5186 (0.6185)	
training:	Epoch: [1][194/204]	Loss 0.6631 (0.6187)	
training:	Epoch: [1][195/204]	Loss 0.5192 (0.6182)	
training:	Epoch: [1][196/204]	Loss 0.5628 (0.6179)	
training:	Epoch: [1][197/204]	Loss 0.5733 (0.6177)	
training:	Epoch: [1][198/204]	Loss 0.5782 (0.6175)	
training:	Epoch: [1][199/204]	Loss 0.6408 (0.6176)	
training:	Epoch: [1][200/204]	Loss 0.5667 (0.6174)	
training:	Epoch: [1][201/204]	Loss 0.5195 (0.6169)	
training:	Epoch: [1][202/204]	Loss 0.4534 (0.6161)	
training:	Epoch: [1][203/204]	Loss 0.5668 (0.6158)	
training:	Epoch: [1][204/204]	Loss 0.4530 (0.6150)	
Training:	 Loss: 0.6141

Training:	 ACC: 0.7372 0.7415 0.8234 0.6511
Validation:	 ACC: 0.6982 0.7047 0.8089 0.5875
Validation:	 Best_BACC: 0.6982 0.7047 0.8089 0.5875
Validation:	 Loss: 0.5715
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/204]	Loss 0.6001 (0.6001)	
training:	Epoch: [2][2/204]	Loss 0.5443 (0.5722)	
training:	Epoch: [2][3/204]	Loss 0.5678 (0.5708)	
training:	Epoch: [2][4/204]	Loss 0.4787 (0.5478)	
training:	Epoch: [2][5/204]	Loss 0.5475 (0.5477)	
training:	Epoch: [2][6/204]	Loss 0.5562 (0.5491)	
training:	Epoch: [2][7/204]	Loss 0.5478 (0.5489)	
training:	Epoch: [2][8/204]	Loss 0.6653 (0.5635)	
training:	Epoch: [2][9/204]	Loss 0.5583 (0.5629)	
training:	Epoch: [2][10/204]	Loss 0.6530 (0.5719)	
training:	Epoch: [2][11/204]	Loss 0.5068 (0.5660)	
training:	Epoch: [2][12/204]	Loss 0.4559 (0.5568)	
training:	Epoch: [2][13/204]	Loss 0.5503 (0.5563)	
training:	Epoch: [2][14/204]	Loss 0.6202 (0.5609)	
training:	Epoch: [2][15/204]	Loss 0.5453 (0.5598)	
training:	Epoch: [2][16/204]	Loss 0.4637 (0.5538)	
training:	Epoch: [2][17/204]	Loss 0.5331 (0.5526)	
training:	Epoch: [2][18/204]	Loss 0.4573 (0.5473)	
training:	Epoch: [2][19/204]	Loss 0.4429 (0.5418)	
training:	Epoch: [2][20/204]	Loss 0.5232 (0.5409)	
training:	Epoch: [2][21/204]	Loss 0.5119 (0.5395)	
training:	Epoch: [2][22/204]	Loss 0.4997 (0.5377)	
training:	Epoch: [2][23/204]	Loss 0.5487 (0.5382)	
training:	Epoch: [2][24/204]	Loss 0.5722 (0.5396)	
training:	Epoch: [2][25/204]	Loss 0.4880 (0.5375)	
training:	Epoch: [2][26/204]	Loss 0.4982 (0.5360)	
training:	Epoch: [2][27/204]	Loss 0.5246 (0.5356)	
training:	Epoch: [2][28/204]	Loss 0.5722 (0.5369)	
training:	Epoch: [2][29/204]	Loss 0.5124 (0.5361)	
training:	Epoch: [2][30/204]	Loss 0.4831 (0.5343)	
training:	Epoch: [2][31/204]	Loss 0.5669 (0.5353)	
training:	Epoch: [2][32/204]	Loss 0.6316 (0.5383)	
training:	Epoch: [2][33/204]	Loss 0.5741 (0.5394)	
training:	Epoch: [2][34/204]	Loss 0.5050 (0.5384)	
training:	Epoch: [2][35/204]	Loss 0.4932 (0.5371)	
training:	Epoch: [2][36/204]	Loss 0.4561 (0.5349)	
training:	Epoch: [2][37/204]	Loss 0.4729 (0.5332)	
training:	Epoch: [2][38/204]	Loss 0.5767 (0.5343)	
training:	Epoch: [2][39/204]	Loss 0.4629 (0.5325)	
training:	Epoch: [2][40/204]	Loss 0.4125 (0.5295)	
training:	Epoch: [2][41/204]	Loss 0.5358 (0.5297)	
training:	Epoch: [2][42/204]	Loss 0.4841 (0.5286)	
training:	Epoch: [2][43/204]	Loss 0.5947 (0.5301)	
training:	Epoch: [2][44/204]	Loss 0.4892 (0.5292)	
training:	Epoch: [2][45/204]	Loss 0.5269 (0.5291)	
training:	Epoch: [2][46/204]	Loss 0.5010 (0.5285)	
training:	Epoch: [2][47/204]	Loss 0.4826 (0.5275)	
training:	Epoch: [2][48/204]	Loss 0.5697 (0.5284)	
training:	Epoch: [2][49/204]	Loss 0.4243 (0.5263)	
training:	Epoch: [2][50/204]	Loss 0.5128 (0.5260)	
training:	Epoch: [2][51/204]	Loss 0.5670 (0.5268)	
training:	Epoch: [2][52/204]	Loss 0.4840 (0.5260)	
training:	Epoch: [2][53/204]	Loss 0.4773 (0.5251)	
training:	Epoch: [2][54/204]	Loss 0.5614 (0.5258)	
training:	Epoch: [2][55/204]	Loss 0.5289 (0.5258)	
training:	Epoch: [2][56/204]	Loss 0.4887 (0.5252)	
training:	Epoch: [2][57/204]	Loss 0.4262 (0.5234)	
training:	Epoch: [2][58/204]	Loss 0.5193 (0.5234)	
training:	Epoch: [2][59/204]	Loss 0.4355 (0.5219)	
training:	Epoch: [2][60/204]	Loss 0.4586 (0.5208)	
training:	Epoch: [2][61/204]	Loss 0.5223 (0.5208)	
training:	Epoch: [2][62/204]	Loss 0.4961 (0.5204)	
training:	Epoch: [2][63/204]	Loss 0.5072 (0.5202)	
training:	Epoch: [2][64/204]	Loss 0.5928 (0.5214)	
training:	Epoch: [2][65/204]	Loss 0.4671 (0.5205)	
training:	Epoch: [2][66/204]	Loss 0.5564 (0.5211)	
training:	Epoch: [2][67/204]	Loss 0.4283 (0.5197)	
training:	Epoch: [2][68/204]	Loss 0.4788 (0.5191)	
training:	Epoch: [2][69/204]	Loss 0.5656 (0.5198)	
training:	Epoch: [2][70/204]	Loss 0.5400 (0.5200)	
training:	Epoch: [2][71/204]	Loss 0.4988 (0.5197)	
training:	Epoch: [2][72/204]	Loss 0.4292 (0.5185)	
training:	Epoch: [2][73/204]	Loss 0.5178 (0.5185)	
training:	Epoch: [2][74/204]	Loss 0.6642 (0.5204)	
training:	Epoch: [2][75/204]	Loss 0.5268 (0.5205)	
training:	Epoch: [2][76/204]	Loss 0.4280 (0.5193)	
training:	Epoch: [2][77/204]	Loss 0.4503 (0.5184)	
training:	Epoch: [2][78/204]	Loss 0.5088 (0.5183)	
training:	Epoch: [2][79/204]	Loss 0.4505 (0.5174)	
training:	Epoch: [2][80/204]	Loss 0.4723 (0.5169)	
training:	Epoch: [2][81/204]	Loss 0.5616 (0.5174)	
training:	Epoch: [2][82/204]	Loss 0.4945 (0.5171)	
training:	Epoch: [2][83/204]	Loss 0.4930 (0.5169)	
training:	Epoch: [2][84/204]	Loss 0.6004 (0.5178)	
training:	Epoch: [2][85/204]	Loss 0.5883 (0.5187)	
training:	Epoch: [2][86/204]	Loss 0.5711 (0.5193)	
training:	Epoch: [2][87/204]	Loss 0.5935 (0.5201)	
training:	Epoch: [2][88/204]	Loss 0.5237 (0.5202)	
training:	Epoch: [2][89/204]	Loss 0.5205 (0.5202)	
training:	Epoch: [2][90/204]	Loss 0.5246 (0.5202)	
training:	Epoch: [2][91/204]	Loss 0.6384 (0.5215)	
