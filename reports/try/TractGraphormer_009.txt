Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, epochs=200, tensorboard=True, net_architecture='TractGraphormer', batch_size=32, rate=1e-05, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20, fl=64, nh=1)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	1e-05
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	2

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/204]	Loss 0.7216 (0.7216)	
training:	Epoch: [1][2/204]	Loss 0.7526 (0.7371)	
training:	Epoch: [1][3/204]	Loss 0.7225 (0.7322)	
training:	Epoch: [1][4/204]	Loss 0.6724 (0.7173)	
training:	Epoch: [1][5/204]	Loss 0.6658 (0.7070)	
training:	Epoch: [1][6/204]	Loss 0.6720 (0.7011)	
training:	Epoch: [1][7/204]	Loss 0.6974 (0.7006)	
training:	Epoch: [1][8/204]	Loss 0.6812 (0.6982)	
training:	Epoch: [1][9/204]	Loss 0.6607 (0.6940)	
training:	Epoch: [1][10/204]	Loss 0.7063 (0.6952)	
training:	Epoch: [1][11/204]	Loss 0.6922 (0.6950)	
training:	Epoch: [1][12/204]	Loss 0.7397 (0.6987)	
training:	Epoch: [1][13/204]	Loss 0.6775 (0.6971)	
training:	Epoch: [1][14/204]	Loss 0.6936 (0.6968)	
training:	Epoch: [1][15/204]	Loss 0.6913 (0.6964)	
training:	Epoch: [1][16/204]	Loss 0.7036 (0.6969)	
training:	Epoch: [1][17/204]	Loss 0.7150 (0.6980)	
training:	Epoch: [1][18/204]	Loss 0.6585 (0.6958)	
training:	Epoch: [1][19/204]	Loss 0.7176 (0.6969)	
training:	Epoch: [1][20/204]	Loss 0.7121 (0.6977)	
training:	Epoch: [1][21/204]	Loss 0.6936 (0.6975)	
training:	Epoch: [1][22/204]	Loss 0.7177 (0.6984)	
training:	Epoch: [1][23/204]	Loss 0.7230 (0.6995)	
training:	Epoch: [1][24/204]	Loss 0.7065 (0.6998)	
training:	Epoch: [1][25/204]	Loss 0.6901 (0.6994)	
training:	Epoch: [1][26/204]	Loss 0.7146 (0.7000)	
training:	Epoch: [1][27/204]	Loss 0.7051 (0.7001)	
training:	Epoch: [1][28/204]	Loss 0.6694 (0.6991)	
training:	Epoch: [1][29/204]	Loss 0.6971 (0.6990)	
training:	Epoch: [1][30/204]	Loss 0.7047 (0.6992)	
training:	Epoch: [1][31/204]	Loss 0.7175 (0.6998)	
training:	Epoch: [1][32/204]	Loss 0.7225 (0.7005)	
training:	Epoch: [1][33/204]	Loss 0.7090 (0.7007)	
training:	Epoch: [1][34/204]	Loss 0.6522 (0.6993)	
training:	Epoch: [1][35/204]	Loss 0.7151 (0.6998)	
training:	Epoch: [1][36/204]	Loss 0.6990 (0.6997)	
training:	Epoch: [1][37/204]	Loss 0.6984 (0.6997)	
training:	Epoch: [1][38/204]	Loss 0.6792 (0.6992)	
training:	Epoch: [1][39/204]	Loss 0.6752 (0.6986)	
training:	Epoch: [1][40/204]	Loss 0.6853 (0.6982)	
training:	Epoch: [1][41/204]	Loss 0.6961 (0.6982)	
training:	Epoch: [1][42/204]	Loss 0.6791 (0.6977)	
training:	Epoch: [1][43/204]	Loss 0.6801 (0.6973)	
training:	Epoch: [1][44/204]	Loss 0.6721 (0.6967)	
training:	Epoch: [1][45/204]	Loss 0.7075 (0.6970)	
training:	Epoch: [1][46/204]	Loss 0.6988 (0.6970)	
training:	Epoch: [1][47/204]	Loss 0.6792 (0.6966)	
training:	Epoch: [1][48/204]	Loss 0.6796 (0.6963)	
training:	Epoch: [1][49/204]	Loss 0.7261 (0.6969)	
training:	Epoch: [1][50/204]	Loss 0.7084 (0.6971)	
training:	Epoch: [1][51/204]	Loss 0.6809 (0.6968)	
training:	Epoch: [1][52/204]	Loss 0.7121 (0.6971)	
training:	Epoch: [1][53/204]	Loss 0.6828 (0.6968)	
training:	Epoch: [1][54/204]	Loss 0.6690 (0.6963)	
training:	Epoch: [1][55/204]	Loss 0.6836 (0.6961)	
training:	Epoch: [1][56/204]	Loss 0.6724 (0.6957)	
training:	Epoch: [1][57/204]	Loss 0.7097 (0.6959)	
training:	Epoch: [1][58/204]	Loss 0.6945 (0.6959)	
training:	Epoch: [1][59/204]	Loss 0.7214 (0.6963)	
training:	Epoch: [1][60/204]	Loss 0.6558 (0.6956)	
training:	Epoch: [1][61/204]	Loss 0.6859 (0.6955)	
training:	Epoch: [1][62/204]	Loss 0.6827 (0.6953)	
training:	Epoch: [1][63/204]	Loss 0.7063 (0.6954)	
training:	Epoch: [1][64/204]	Loss 0.7176 (0.6958)	
training:	Epoch: [1][65/204]	Loss 0.6696 (0.6954)	
training:	Epoch: [1][66/204]	Loss 0.7293 (0.6959)	
training:	Epoch: [1][67/204]	Loss 0.6798 (0.6957)	
training:	Epoch: [1][68/204]	Loss 0.6666 (0.6952)	
training:	Epoch: [1][69/204]	Loss 0.6719 (0.6949)	
training:	Epoch: [1][70/204]	Loss 0.7090 (0.6951)	
training:	Epoch: [1][71/204]	Loss 0.6887 (0.6950)	
training:	Epoch: [1][72/204]	Loss 0.7020 (0.6951)	
training:	Epoch: [1][73/204]	Loss 0.6772 (0.6949)	
training:	Epoch: [1][74/204]	Loss 0.7112 (0.6951)	
training:	Epoch: [1][75/204]	Loss 0.7032 (0.6952)	
training:	Epoch: [1][76/204]	Loss 0.7134 (0.6954)	
training:	Epoch: [1][77/204]	Loss 0.6683 (0.6951)	
training:	Epoch: [1][78/204]	Loss 0.6656 (0.6947)	
training:	Epoch: [1][79/204]	Loss 0.7298 (0.6951)	
training:	Epoch: [1][80/204]	Loss 0.7040 (0.6952)	
training:	Epoch: [1][81/204]	Loss 0.7038 (0.6954)	
training:	Epoch: [1][82/204]	Loss 0.6742 (0.6951)	
training:	Epoch: [1][83/204]	Loss 0.6731 (0.6948)	
training:	Epoch: [1][84/204]	Loss 0.6899 (0.6948)	
training:	Epoch: [1][85/204]	Loss 0.7014 (0.6948)	
training:	Epoch: [1][86/204]	Loss 0.6894 (0.6948)	
training:	Epoch: [1][87/204]	Loss 0.6937 (0.6948)	
training:	Epoch: [1][88/204]	Loss 0.7004 (0.6948)	
training:	Epoch: [1][89/204]	Loss 0.7076 (0.6950)	
training:	Epoch: [1][90/204]	Loss 0.7027 (0.6951)	
training:	Epoch: [1][91/204]	Loss 0.6872 (0.6950)	
training:	Epoch: [1][92/204]	Loss 0.7000 (0.6950)	
training:	Epoch: [1][93/204]	Loss 0.7032 (0.6951)	
training:	Epoch: [1][94/204]	Loss 0.7224 (0.6954)	
training:	Epoch: [1][95/204]	Loss 0.7260 (0.6957)	
training:	Epoch: [1][96/204]	Loss 0.6685 (0.6955)	
training:	Epoch: [1][97/204]	Loss 0.6710 (0.6952)	
training:	Epoch: [1][98/204]	Loss 0.6950 (0.6952)	
training:	Epoch: [1][99/204]	Loss 0.7048 (0.6953)	
training:	Epoch: [1][100/204]	Loss 0.6926 (0.6953)	
training:	Epoch: [1][101/204]	Loss 0.6989 (0.6953)	
training:	Epoch: [1][102/204]	Loss 0.7063 (0.6954)	
training:	Epoch: [1][103/204]	Loss 0.7253 (0.6957)	
training:	Epoch: [1][104/204]	Loss 0.6893 (0.6956)	
training:	Epoch: [1][105/204]	Loss 0.6859 (0.6955)	
training:	Epoch: [1][106/204]	Loss 0.7367 (0.6959)	
training:	Epoch: [1][107/204]	Loss 0.6728 (0.6957)	
training:	Epoch: [1][108/204]	Loss 0.7164 (0.6959)	
training:	Epoch: [1][109/204]	Loss 0.7103 (0.6960)	
training:	Epoch: [1][110/204]	Loss 0.7103 (0.6962)	
training:	Epoch: [1][111/204]	Loss 0.7193 (0.6964)	
training:	Epoch: [1][112/204]	Loss 0.6977 (0.6964)	
training:	Epoch: [1][113/204]	Loss 0.6922 (0.6964)	
training:	Epoch: [1][114/204]	Loss 0.6983 (0.6964)	
training:	Epoch: [1][115/204]	Loss 0.7148 (0.6965)	
training:	Epoch: [1][116/204]	Loss 0.6856 (0.6964)	
training:	Epoch: [1][117/204]	Loss 0.7281 (0.6967)	
training:	Epoch: [1][118/204]	Loss 0.7159 (0.6969)	
training:	Epoch: [1][119/204]	Loss 0.7206 (0.6971)	
training:	Epoch: [1][120/204]	Loss 0.6795 (0.6969)	
training:	Epoch: [1][121/204]	Loss 0.6943 (0.6969)	
training:	Epoch: [1][122/204]	Loss 0.7094 (0.6970)	
training:	Epoch: [1][123/204]	Loss 0.6869 (0.6969)	
training:	Epoch: [1][124/204]	Loss 0.6896 (0.6969)	
training:	Epoch: [1][125/204]	Loss 0.6971 (0.6969)	
training:	Epoch: [1][126/204]	Loss 0.6836 (0.6968)	
training:	Epoch: [1][127/204]	Loss 0.6798 (0.6966)	
training:	Epoch: [1][128/204]	Loss 0.6782 (0.6965)	
training:	Epoch: [1][129/204]	Loss 0.7137 (0.6966)	
training:	Epoch: [1][130/204]	Loss 0.7162 (0.6968)	
training:	Epoch: [1][131/204]	Loss 0.6935 (0.6967)	
training:	Epoch: [1][132/204]	Loss 0.6876 (0.6967)	
training:	Epoch: [1][133/204]	Loss 0.6795 (0.6965)	
training:	Epoch: [1][134/204]	Loss 0.6948 (0.6965)	
training:	Epoch: [1][135/204]	Loss 0.6716 (0.6963)	
training:	Epoch: [1][136/204]	Loss 0.6819 (0.6962)	
training:	Epoch: [1][137/204]	Loss 0.6967 (0.6962)	
training:	Epoch: [1][138/204]	Loss 0.6866 (0.6962)	
training:	Epoch: [1][139/204]	Loss 0.7077 (0.6963)	
training:	Epoch: [1][140/204]	Loss 0.6874 (0.6962)	
training:	Epoch: [1][141/204]	Loss 0.7080 (0.6963)	
training:	Epoch: [1][142/204]	Loss 0.6822 (0.6962)	
training:	Epoch: [1][143/204]	Loss 0.7150 (0.6963)	
training:	Epoch: [1][144/204]	Loss 0.6891 (0.6963)	
training:	Epoch: [1][145/204]	Loss 0.7050 (0.6963)	
training:	Epoch: [1][146/204]	Loss 0.6803 (0.6962)	
training:	Epoch: [1][147/204]	Loss 0.7053 (0.6963)	
training:	Epoch: [1][148/204]	Loss 0.6650 (0.6961)	
training:	Epoch: [1][149/204]	Loss 0.6828 (0.6960)	
training:	Epoch: [1][150/204]	Loss 0.6744 (0.6958)	
training:	Epoch: [1][151/204]	Loss 0.6890 (0.6958)	
training:	Epoch: [1][152/204]	Loss 0.6769 (0.6957)	
training:	Epoch: [1][153/204]	Loss 0.7029 (0.6957)	
training:	Epoch: [1][154/204]	Loss 0.6836 (0.6956)	
training:	Epoch: [1][155/204]	Loss 0.7053 (0.6957)	
training:	Epoch: [1][156/204]	Loss 0.6894 (0.6956)	
training:	Epoch: [1][157/204]	Loss 0.7015 (0.6957)	
training:	Epoch: [1][158/204]	Loss 0.6667 (0.6955)	
training:	Epoch: [1][159/204]	Loss 0.6978 (0.6955)	
training:	Epoch: [1][160/204]	Loss 0.6614 (0.6953)	
training:	Epoch: [1][161/204]	Loss 0.6692 (0.6951)	
training:	Epoch: [1][162/204]	Loss 0.7022 (0.6952)	
training:	Epoch: [1][163/204]	Loss 0.6795 (0.6951)	
training:	Epoch: [1][164/204]	Loss 0.7054 (0.6952)	
training:	Epoch: [1][165/204]	Loss 0.6745 (0.6950)	
training:	Epoch: [1][166/204]	Loss 0.7167 (0.6952)	
training:	Epoch: [1][167/204]	Loss 0.6620 (0.6950)	
training:	Epoch: [1][168/204]	Loss 0.6764 (0.6948)	
training:	Epoch: [1][169/204]	Loss 0.6687 (0.6947)	
training:	Epoch: [1][170/204]	Loss 0.7022 (0.6947)	
training:	Epoch: [1][171/204]	Loss 0.7017 (0.6948)	
training:	Epoch: [1][172/204]	Loss 0.6881 (0.6947)	
training:	Epoch: [1][173/204]	Loss 0.6826 (0.6947)	
training:	Epoch: [1][174/204]	Loss 0.7241 (0.6948)	
training:	Epoch: [1][175/204]	Loss 0.6867 (0.6948)	
training:	Epoch: [1][176/204]	Loss 0.6894 (0.6948)	
training:	Epoch: [1][177/204]	Loss 0.6738 (0.6946)	
training:	Epoch: [1][178/204]	Loss 0.6879 (0.6946)	
training:	Epoch: [1][179/204]	Loss 0.6843 (0.6945)	
training:	Epoch: [1][180/204]	Loss 0.7133 (0.6947)	
training:	Epoch: [1][181/204]	Loss 0.6743 (0.6945)	
training:	Epoch: [1][182/204]	Loss 0.6824 (0.6945)	
training:	Epoch: [1][183/204]	Loss 0.7080 (0.6945)	
training:	Epoch: [1][184/204]	Loss 0.7068 (0.6946)	
training:	Epoch: [1][185/204]	Loss 0.6682 (0.6945)	
training:	Epoch: [1][186/204]	Loss 0.6679 (0.6943)	
training:	Epoch: [1][187/204]	Loss 0.6624 (0.6942)	
training:	Epoch: [1][188/204]	Loss 0.7002 (0.6942)	
training:	Epoch: [1][189/204]	Loss 0.6886 (0.6942)	
training:	Epoch: [1][190/204]	Loss 0.6808 (0.6941)	
training:	Epoch: [1][191/204]	Loss 0.6737 (0.6940)	
training:	Epoch: [1][192/204]	Loss 0.7038 (0.6940)	
training:	Epoch: [1][193/204]	Loss 0.6787 (0.6940)	
training:	Epoch: [1][194/204]	Loss 0.6896 (0.6939)	
training:	Epoch: [1][195/204]	Loss 0.6857 (0.6939)	
training:	Epoch: [1][196/204]	Loss 0.6985 (0.6939)	
training:	Epoch: [1][197/204]	Loss 0.6885 (0.6939)	
training:	Epoch: [1][198/204]	Loss 0.6974 (0.6939)	
training:	Epoch: [1][199/204]	Loss 0.6907 (0.6939)	
training:	Epoch: [1][200/204]	Loss 0.6836 (0.6938)	
training:	Epoch: [1][201/204]	Loss 0.6941 (0.6938)	
training:	Epoch: [1][202/204]	Loss 0.6918 (0.6938)	
training:	Epoch: [1][203/204]	Loss 0.6804 (0.6938)	
training:	Epoch: [1][204/204]	Loss 0.6846 (0.6937)	
Training:	 Loss: 0.6927

Training:	 ACC: 0.5284 0.5427 0.8795 0.1773
Validation:	 ACC: 0.5359 0.5522 0.8936 0.1783
Validation:	 Best_BACC: 0.5359 0.5522 0.8936 0.1783
Validation:	 Loss: 0.6859
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/204]	Loss 0.7246 (0.7246)	
training:	Epoch: [2][2/204]	Loss 0.7224 (0.7235)	
training:	Epoch: [2][3/204]	Loss 0.7049 (0.7173)	
training:	Epoch: [2][4/204]	Loss 0.6928 (0.7112)	
training:	Epoch: [2][5/204]	Loss 0.6887 (0.7067)	
training:	Epoch: [2][6/204]	Loss 0.6988 (0.7054)	
training:	Epoch: [2][7/204]	Loss 0.6841 (0.7023)	
training:	Epoch: [2][8/204]	Loss 0.6883 (0.7006)	
training:	Epoch: [2][9/204]	Loss 0.6883 (0.6992)	
training:	Epoch: [2][10/204]	Loss 0.6973 (0.6990)	
training:	Epoch: [2][11/204]	Loss 0.6617 (0.6956)	
training:	Epoch: [2][12/204]	Loss 0.6892 (0.6951)	
training:	Epoch: [2][13/204]	Loss 0.6629 (0.6926)	
training:	Epoch: [2][14/204]	Loss 0.6716 (0.6911)	
training:	Epoch: [2][15/204]	Loss 0.6911 (0.6911)	
training:	Epoch: [2][16/204]	Loss 0.6687 (0.6897)	
training:	Epoch: [2][17/204]	Loss 0.6974 (0.6902)	
training:	Epoch: [2][18/204]	Loss 0.6529 (0.6881)	
training:	Epoch: [2][19/204]	Loss 0.7086 (0.6892)	
training:	Epoch: [2][20/204]	Loss 0.6643 (0.6879)	
training:	Epoch: [2][21/204]	Loss 0.7047 (0.6887)	
training:	Epoch: [2][22/204]	Loss 0.6530 (0.6871)	
training:	Epoch: [2][23/204]	Loss 0.6906 (0.6873)	
training:	Epoch: [2][24/204]	Loss 0.7169 (0.6885)	
training:	Epoch: [2][25/204]	Loss 0.6823 (0.6882)	
training:	Epoch: [2][26/204]	Loss 0.6558 (0.6870)	
training:	Epoch: [2][27/204]	Loss 0.6912 (0.6871)	
training:	Epoch: [2][28/204]	Loss 0.6932 (0.6874)	
training:	Epoch: [2][29/204]	Loss 0.6835 (0.6872)	
training:	Epoch: [2][30/204]	Loss 0.6977 (0.6876)	
training:	Epoch: [2][31/204]	Loss 0.6827 (0.6874)	
training:	Epoch: [2][32/204]	Loss 0.7076 (0.6881)	
training:	Epoch: [2][33/204]	Loss 0.6491 (0.6869)	
training:	Epoch: [2][34/204]	Loss 0.6707 (0.6864)	
training:	Epoch: [2][35/204]	Loss 0.7017 (0.6868)	
training:	Epoch: [2][36/204]	Loss 0.6691 (0.6863)	
training:	Epoch: [2][37/204]	Loss 0.6924 (0.6865)	
training:	Epoch: [2][38/204]	Loss 0.6839 (0.6864)	
training:	Epoch: [2][39/204]	Loss 0.6945 (0.6866)	
training:	Epoch: [2][40/204]	Loss 0.6805 (0.6865)	
training:	Epoch: [2][41/204]	Loss 0.6844 (0.6864)	
training:	Epoch: [2][42/204]	Loss 0.6925 (0.6866)	
training:	Epoch: [2][43/204]	Loss 0.6796 (0.6864)	
training:	Epoch: [2][44/204]	Loss 0.6872 (0.6864)	
training:	Epoch: [2][45/204]	Loss 0.6806 (0.6863)	
training:	Epoch: [2][46/204]	Loss 0.6746 (0.6861)	
training:	Epoch: [2][47/204]	Loss 0.6874 (0.6861)	
training:	Epoch: [2][48/204]	Loss 0.6623 (0.6856)	
training:	Epoch: [2][49/204]	Loss 0.6956 (0.6858)	
training:	Epoch: [2][50/204]	Loss 0.6586 (0.6852)	
training:	Epoch: [2][51/204]	Loss 0.7243 (0.6860)	
training:	Epoch: [2][52/204]	Loss 0.6451 (0.6852)	
training:	Epoch: [2][53/204]	Loss 0.6465 (0.6845)	
training:	Epoch: [2][54/204]	Loss 0.6621 (0.6841)	
training:	Epoch: [2][55/204]	Loss 0.6810 (0.6840)	
training:	Epoch: [2][56/204]	Loss 0.6768 (0.6839)	
training:	Epoch: [2][57/204]	Loss 0.6995 (0.6842)	
training:	Epoch: [2][58/204]	Loss 0.6782 (0.6841)	
training:	Epoch: [2][59/204]	Loss 0.6949 (0.6843)	
training:	Epoch: [2][60/204]	Loss 0.7031 (0.6846)	
training:	Epoch: [2][61/204]	Loss 0.6726 (0.6844)	
training:	Epoch: [2][62/204]	Loss 0.6833 (0.6844)	
training:	Epoch: [2][63/204]	Loss 0.6727 (0.6842)	
training:	Epoch: [2][64/204]	Loss 0.6685 (0.6839)	
training:	Epoch: [2][65/204]	Loss 0.6779 (0.6838)	
training:	Epoch: [2][66/204]	Loss 0.6729 (0.6837)	
training:	Epoch: [2][67/204]	Loss 0.7265 (0.6843)	
training:	Epoch: [2][68/204]	Loss 0.7128 (0.6847)	
training:	Epoch: [2][69/204]	Loss 0.6066 (0.6836)	
training:	Epoch: [2][70/204]	Loss 0.7080 (0.6839)	
training:	Epoch: [2][71/204]	Loss 0.6475 (0.6834)	
training:	Epoch: [2][72/204]	Loss 0.6768 (0.6833)	
training:	Epoch: [2][73/204]	Loss 0.6965 (0.6835)	
training:	Epoch: [2][74/204]	Loss 0.6720 (0.6834)	
training:	Epoch: [2][75/204]	Loss 0.6833 (0.6834)	
training:	Epoch: [2][76/204]	Loss 0.6995 (0.6836)	
training:	Epoch: [2][77/204]	Loss 0.6632 (0.6833)	
training:	Epoch: [2][78/204]	Loss 0.7094 (0.6836)	
training:	Epoch: [2][79/204]	Loss 0.7073 (0.6839)	
training:	Epoch: [2][80/204]	Loss 0.6912 (0.6840)	
training:	Epoch: [2][81/204]	Loss 0.6661 (0.6838)	
training:	Epoch: [2][82/204]	Loss 0.6775 (0.6837)	
training:	Epoch: [2][83/204]	Loss 0.6722 (0.6836)	
training:	Epoch: [2][84/204]	Loss 0.6802 (0.6836)	
training:	Epoch: [2][85/204]	Loss 0.6592 (0.6833)	
training:	Epoch: [2][86/204]	Loss 0.7183 (0.6837)	
training:	Epoch: [2][87/204]	Loss 0.6901 (0.6837)	
training:	Epoch: [2][88/204]	Loss 0.6652 (0.6835)	
training:	Epoch: [2][89/204]	Loss 0.7140 (0.6839)	
training:	Epoch: [2][90/204]	Loss 0.7026 (0.6841)	
training:	Epoch: [2][91/204]	Loss 0.6920 (0.6842)	
training:	Epoch: [2][92/204]	Loss 0.6924 (0.6843)	
training:	Epoch: [2][93/204]	Loss 0.6752 (0.6842)	
training:	Epoch: [2][94/204]	Loss 0.6894 (0.6842)	
training:	Epoch: [2][95/204]	Loss 0.6922 (0.6843)	
training:	Epoch: [2][96/204]	Loss 0.6895 (0.6844)	
training:	Epoch: [2][97/204]	Loss 0.6707 (0.6842)	
training:	Epoch: [2][98/204]	Loss 0.6700 (0.6841)	
training:	Epoch: [2][99/204]	Loss 0.6760 (0.6840)	
training:	Epoch: [2][100/204]	Loss 0.7051 (0.6842)	
training:	Epoch: [2][101/204]	Loss 0.6708 (0.6841)	
training:	Epoch: [2][102/204]	Loss 0.6764 (0.6840)	
training:	Epoch: [2][103/204]	Loss 0.7048 (0.6842)	
training:	Epoch: [2][104/204]	Loss 0.6741 (0.6841)	
training:	Epoch: [2][105/204]	Loss 0.6860 (0.6841)	
training:	Epoch: [2][106/204]	Loss 0.6511 (0.6838)	
training:	Epoch: [2][107/204]	Loss 0.6985 (0.6839)	
training:	Epoch: [2][108/204]	Loss 0.6682 (0.6838)	
training:	Epoch: [2][109/204]	Loss 0.7067 (0.6840)	
